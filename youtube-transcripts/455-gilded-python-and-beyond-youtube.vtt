WEBVTT

00:00:00.819 --> 00:00:05.940
<v Speaker 1>Hey, everybody. Welcome to another wonderful recording of Python Bytes.

00:00:07.700 --> 00:00:10.300
<v Speaker 1>Well, let's just kick it off. I'm good.

00:00:14.399 --> 00:00:18.200
<v Speaker 2>That's not kicking it off, Brian. No, let's do it.

00:00:18.720 --> 00:00:23.400
<v Speaker 1>Okay. Hello and welcome to Python Bytes,

00:00:23.400 --> 00:00:26.420
<v Speaker 1>where we deliver Python news and headlines directly to your earbuds.

00:00:27.200 --> 00:00:31.880
<v Speaker 1>This is episode 455, recorded October 27th, 2025.

00:00:32.700 --> 00:00:33.520
<v Speaker 1>And I'm Brian Okken.

00:00:34.040 --> 00:00:34.720
<v Speaker 2>I'm Michael Kennedy.

00:00:35.960 --> 00:00:39.620
<v Speaker 1>And this episode is sponsored by the wonderful people at Python Bytes.

00:00:40.160 --> 00:00:43.180
<v Speaker 1>Actually, both and everybody here.

00:00:43.740 --> 00:00:49.160
<v Speaker 1>So please check out the work we do for a little bit of money.

00:00:49.280 --> 00:00:52.920
<v Speaker 1>We've got Talk Python Training with tons of wonderful courses

00:00:54.120 --> 00:00:56.060
<v Speaker 1>and his new book, of course, Michael's new book.

00:00:56.800 --> 00:01:05.199
<v Speaker 1>And then also, if you want to check out pytest, there's the complete pytest course, or you can take the pytest course at Talk PythonTraining.

00:01:05.920 --> 00:01:08.560
<v Speaker 1>And as always, thank you to our Patreon supporters.

00:01:09.230 --> 00:01:09.860
<v Speaker 1>You guys rock.

00:01:11.200 --> 00:01:14.320
<v Speaker 1>We don't shout you out all the time, but we always appreciate you.

00:01:15.320 --> 00:01:25.300
<v Speaker 1>If you'd like to submit a topic idea or some feedback for us or just say hi, check out the show notes at pythonbytes.fm.

00:01:25.380 --> 00:01:28.600
<v Speaker 1>There's links to our socials.

00:01:28.760 --> 00:01:36.360
<v Speaker 1>We've got the show and both Michael and I are on Bluesky and Fosstodon, but that's really

00:01:36.520 --> 00:01:36.800
<v Speaker 1>massive.

00:01:37.300 --> 00:01:42.120
<v Speaker 1>So wherever you're at there, if you're listening to this and you're like, what sounds like

00:01:42.200 --> 00:01:44.620
<v Speaker 1>they're recording live some somewhere we can see it.

00:01:44.780 --> 00:01:50.740
<v Speaker 1>Yes, we are recording and streaming, but you can watch all the back episodes or participate

00:01:51.160 --> 00:01:52.560
<v Speaker 1>in the chat.

00:01:52.620 --> 00:01:59.920
<v Speaker 1>If you want to, check out pythonbytes.fm/live with all the details there and a link to when we're going to record next.

00:02:03.560 --> 00:02:05.680
<v Speaker 1>Before we get started, one last thing.

00:02:06.480 --> 00:02:14.020
<v Speaker 1>We do send out a newsletter that's getting better and better as we go along, thanks to both Michael and I working on it a little bit.

00:02:14.280 --> 00:02:19.500
<v Speaker 1>So there's a newsletter we send out with all the links so you don't have to take notes.

00:02:20.100 --> 00:02:21.380
<v Speaker 1>We'll send you out an email.

00:02:22.120 --> 00:02:27.520
<v Speaker 1>So sign up for the newsletter and we'll send you those links and extra details too.

00:02:27.740 --> 00:02:28.720
<v Speaker 2>It's a great resource.

00:02:29.720 --> 00:02:31.360
<v Speaker 2>Yeah, I think people really appreciate it, Brian.

00:02:32.120 --> 00:02:37.840
<v Speaker 2>You know, if you look at when you send out the email and then book back at the campaign,

00:02:38.040 --> 00:02:41.720
<v Speaker 2>how many people actually open to the thing that we send out or click on it?

00:02:41.810 --> 00:02:44.340
<v Speaker 2>Those are really close to 100%, which is ridiculous.

00:02:45.780 --> 00:02:48.700
<v Speaker 2>So that's a really good sign that other people might also like it.

00:02:49.820 --> 00:02:52.520
<v Speaker 1>Yeah, I think that people are using it in place of taking notes.

00:02:52.780 --> 00:02:54.660
<v Speaker 1>And also, if you're in a car or something,

00:02:54.700 --> 00:02:56.220
<v Speaker 1>you don't want to try to remember.

00:02:56.400 --> 00:02:57.000
<v Speaker 1>You don't have to.

00:02:57.300 --> 00:02:58.920
<v Speaker 1>So awesome.

00:02:59.880 --> 00:03:03.620
<v Speaker 1>All right, Michael, what do you got for us right away?

00:03:04.040 --> 00:03:06.740
<v Speaker 2>Well, let me see what I can do here.

00:03:08.900 --> 00:03:10.760
<v Speaker 2>Let's see what the Cyclops says.

00:03:12.300 --> 00:03:13.000
<v Speaker 2>The Cyclops.

00:03:13.980 --> 00:03:16.920
<v Speaker 2>It's like a little programming kitty, but it's a Cyclops.

00:03:17.220 --> 00:03:17.739
<v Speaker 2>It's a cute--

00:03:17.760 --> 00:03:20.500
<v Speaker 1>Like OPT, operations.

00:03:20.890 --> 00:03:23.020
<v Speaker 1>Like, I get it.

00:03:23.050 --> 00:03:24.600
<v Speaker 2>So we've heard of Click.

00:03:25.280 --> 00:03:28.020
<v Speaker 2>You and I were both just actually singing the praises

00:03:28.240 --> 00:03:30.520
<v Speaker 2>of ArgPars, which is pretty interesting.

00:03:31.480 --> 00:03:33.880
<v Speaker 2>I am such a fan of ArgPars these days

00:03:34.200 --> 00:03:36.240
<v Speaker 2>because it's just built in, and it's simple,

00:03:36.310 --> 00:03:37.080
<v Speaker 2>and it's good enough.

00:03:38.780 --> 00:03:45.260
<v Speaker 2>But if you were building something where the CLI API,

00:03:45.680 --> 00:03:48.920
<v Speaker 2>or interface or dev user experience was super important,

00:03:49.460 --> 00:03:51.500
<v Speaker 2>you might choose something other than arg parse.

00:03:52.240 --> 00:03:53.300
<v Speaker 2>So what are your options?

00:03:54.540 --> 00:03:55.800
<v Speaker 2>You could choose Click.

00:03:56.440 --> 00:03:57.740
<v Speaker 2>You could choose Typer.

00:03:58.560 --> 00:03:59.660
<v Speaker 2>Typer is actually built on Click.

00:03:59.720 --> 00:04:03.060
<v Speaker 2>So in a sense, it's like Starlette and FastAPI.

00:04:03.180 --> 00:04:05.100
<v Speaker 2>You choose one, you get the other, in a sense.

00:04:05.620 --> 00:04:06.720
<v Speaker 2>So there's a blend there.

00:04:07.360 --> 00:04:08.220
<v Speaker 2>And there's others.

00:04:08.320 --> 00:04:09.000
<v Speaker 2>I know I'm not--

00:04:09.700 --> 00:04:12.760
<v Speaker 2>there's a bunch of different CLI building options.

00:04:13.480 --> 00:04:19.359
<v Speaker 2>But the Cyclops is one that sort of has the second mover advantage over typer.

00:04:22.380 --> 00:04:25.440
<v Speaker 2>And their sort of tagline, I don't remember where I saw it, but

00:04:25.540 --> 00:04:28.400
<v Speaker 2>somewhere in here, it's not actually right here on this page.

00:04:29.320 --> 00:04:33.520
<v Speaker 2>But no shade at typer, but it says what you thought typer was going to be.

00:04:33.920 --> 00:04:36.140
<v Speaker 2>And by second mover advantage, it's like, all right,

00:04:36.600 --> 00:04:41.680
<v Speaker 2>we've seen different interface things built based around type information.

00:04:43.140 --> 00:04:46.300
<v Speaker 2>And now that we've seen the pluses and the minuses,

00:04:46.300 --> 00:04:49.060
<v Speaker 2>could we do a slightly different version that

00:04:49.060 --> 00:04:50.780
<v Speaker 2>has more pluses and fewer minuses?

00:04:52.080 --> 00:04:54.640
<v Speaker 2>And so there's a whole page of the typer comparison here.

00:04:56.560 --> 00:05:00.100
<v Speaker 2>And I'm not a super expert on typer.

00:05:00.880 --> 00:05:04.600
<v Speaker 2>My CLI foo is not that great.

00:05:05.800 --> 00:05:10.179
<v Speaker 2>So there's 13 different things, 13 different components

00:05:10.200 --> 00:05:19.040
<v Speaker 2>features or aspects of the API that they chose to address to say type is great. We're basically

00:05:19.280 --> 00:05:25.720
<v Speaker 2>inspired by typer. That's what we're doing. However, there's a couple of these 13 areas,

00:05:25.790 --> 00:05:29.400
<v Speaker 2>I think we should make better, right? So you could actually go through and look at all

00:05:29.520 --> 00:05:36.680
<v Speaker 2>of them. Probably the biggest one is that typer was made with Python, three, seven, three,

00:05:36.760 --> 00:05:41.320
<v Speaker 2>where Python's types were at an earlier stage in their life.

00:05:43.440 --> 00:05:48.260
<v Speaker 2>You had to say things like, this is an argument of int,

00:05:48.420 --> 00:05:50.540
<v Speaker 2>or it is an int when it's actually an argument of int

00:05:50.540 --> 00:05:51.340
<v Speaker 2>or something like that.

00:05:52.080 --> 00:05:57.780
<v Speaker 2>And this Cyclops thing was built once annotated was introduced,

00:05:58.420 --> 00:06:00.300
<v Speaker 2>which allows you to have sort of a dual role.

00:06:01.420 --> 00:06:05.479
<v Speaker 2>It really is one of these, but its type could also be

00:06:06.040 --> 00:06:13.720
<v Speaker 2>like its native value right allowing the removal of proxy defaults why does that matter because if

00:06:13.720 --> 00:06:20.820
<v Speaker 2>you build a CLI with Cyclops you can still and you use types you can still use it as regular

00:06:21.120 --> 00:06:26.040
<v Speaker 2>functions as if they just had an integer where it said it took an integer and so on so you know

00:06:26.060 --> 00:06:31.759
<v Speaker 2>things like that it's got about I think a thousand github stars or so it's not crazy crazy popular

00:06:31.820 --> 00:06:37.840
<v Speaker 2>But if you're really loving that concept of having types for your CLI,

00:06:38.100 --> 00:06:38.640
<v Speaker 2>give us a look.

00:06:40.500 --> 00:06:47.340
<v Speaker 1>Yeah, actually, I got a shout out to him for doing things like comparing to Typer.

00:06:47.720 --> 00:06:51.840
<v Speaker 1>Because like you said, a lot of us, I use Typer as well.

00:06:52.080 --> 00:06:55.260
<v Speaker 1>And I've used Typer, I've used Click and ArgParse.

00:06:56.260 --> 00:07:05.720
<v Speaker 1>And so you're probably not going to grab too many of the ArgParse people because they're just using it because it's built in, maybe.

00:07:06.880 --> 00:07:08.680
<v Speaker 2>You're like the unit test people, Brian.

00:07:08.750 --> 00:07:09.340
<v Speaker 2>What can you do?

00:07:10.440 --> 00:07:12.020
<v Speaker 2>You talk pytest for 10 years.

00:07:12.260 --> 00:07:13.120
<v Speaker 1>I yell louder.

00:07:13.380 --> 00:07:13.880
<v Speaker 1>That's what I do.

00:07:16.639 --> 00:07:26.180
<v Speaker 1>But I think it's a fair thing because I'd be thinking, well, if I'm not going to use ArgParse, I'm probably going to reach for TyperClick, lately Typer.

00:07:27.800 --> 00:07:29.040
<v Speaker 1>but having this

00:07:29.370 --> 00:07:29.600
<v Speaker 1>you know

00:07:30.620 --> 00:07:31.280
<v Speaker 1>comparison here

00:07:31.450 --> 00:07:32.460
<v Speaker 1>and then even a migration

00:07:32.700 --> 00:07:33.120
<v Speaker 1>from Typer

00:07:33.240 --> 00:07:33.460
<v Speaker 1>if you're

00:07:34.240 --> 00:07:35.360
<v Speaker 1>I think that's cool

00:07:35.560 --> 00:07:36.920
<v Speaker 1>to show that

00:07:37.300 --> 00:07:37.400
<v Speaker 1>so

00:07:38.060 --> 00:07:38.500
<v Speaker 1>and

00:07:39.020 --> 00:07:39.900
<v Speaker 1>and there's always

00:07:40.310 --> 00:07:41.120
<v Speaker 1>room for making

00:07:41.640 --> 00:07:42.520
<v Speaker 1>CLI tools better

00:07:42.960 --> 00:07:43.920
<v Speaker 1>we love CLI tools

00:07:44.480 --> 00:07:44.700
<v Speaker 2>yeah

00:07:45.480 --> 00:07:46.240
<v Speaker 1>yeah there definitely are

00:07:46.680 --> 00:07:47.400
<v Speaker 2>but yeah so cool

00:07:47.500 --> 00:07:48.120
<v Speaker 2>I ran across this

00:07:48.170 --> 00:07:48.300
<v Speaker 2>and

00:07:48.760 --> 00:07:49.980
<v Speaker 2>I built something with it

00:07:50.160 --> 00:07:50.440
<v Speaker 2>recently

00:07:50.680 --> 00:07:51.200
<v Speaker 2>and it was

00:07:51.300 --> 00:07:51.900
<v Speaker 2>it was nice

00:07:52.360 --> 00:07:52.520
<v Speaker 2>so

00:07:53.120 --> 00:07:53.540
<v Speaker 3>oh did you

00:07:53.590 --> 00:07:53.900
<v Speaker 1>checking out

00:07:55.320 --> 00:07:56.200
<v Speaker 2>a little

00:07:56.220 --> 00:07:58.200
<v Speaker 2>I'm starting to add, I've been kind of,

00:07:58.960 --> 00:08:03.840
<v Speaker 2>I've drunk the Kool-Aid now that maybe having an actual CLI,

00:08:03.970 --> 00:08:08.040
<v Speaker 2>even from my own tools, might be a little bit better than just,

00:08:08.480 --> 00:08:12.220
<v Speaker 2>here's a script I run, and maybe I'll pass it something

00:08:12.330 --> 00:08:15.280
<v Speaker 2>and just look at sys.argv and see if it's in there.

00:08:15.720 --> 00:08:16.260
<v Speaker 3>Oh, man.

00:08:16.420 --> 00:08:17.580
<v Speaker 2>Let's do a little bit better.

00:08:17.820 --> 00:08:18.980
<v Speaker 2>And now that I've started to do that,

00:08:19.020 --> 00:08:20.060
<v Speaker 2>I've been playing with these different ones,

00:08:20.260 --> 00:08:22.400
<v Speaker 2>which is, I think, where I kind of got inspired

00:08:22.600 --> 00:08:24.260
<v Speaker 2>to go down this path and talk about it today.

00:08:25.340 --> 00:08:25.820
<v Speaker 1>Yeah, awesome.

00:08:26.160 --> 00:08:26.340
<v Speaker 1>Cool.

00:08:27.920 --> 00:08:33.599
<v Speaker 1>I want to talk about the present, the future, a little bit of both.

00:08:34.320 --> 00:08:39.300
<v Speaker 1>So there is this interesting article, of course, in Python 3.14.

00:08:39.599 --> 00:08:49.400
<v Speaker 1>Well, we had it in 3.13, is that the GIL was optional with what they call the free threading version, right?

00:08:50.260 --> 00:08:51.200
<v Speaker 1>Free threaded Python.

00:08:52.020 --> 00:08:59.140
<v Speaker 2>Not the gill-less, not the galectomy, but free-threaded, I think is where we landed, from the pep.

00:09:00.980 --> 00:09:03.040
<v Speaker 1>Okay, but is it kind of the same thing?

00:09:03.480 --> 00:09:03.720
<v Speaker 2>Yeah.

00:09:04.420 --> 00:09:04.600
<v Speaker 1>Okay.

00:09:07.840 --> 00:09:08.100
<v Speaker 1>Oh, right.

00:09:08.160 --> 00:09:09.440
<v Speaker 1>The galectomy was a different project.

00:09:10.060 --> 00:09:10.980
<v Speaker 2>Yeah, it was...

00:09:11.220 --> 00:09:11.380
<v Speaker 2>Gosh.

00:09:12.900 --> 00:09:13.200
<v Speaker 2>Anyway.

00:09:14.160 --> 00:09:15.500
<v Speaker 2>Larry Hastings was doing that.

00:09:16.020 --> 00:09:17.300
<v Speaker 2>But I believe...

00:09:18.640 --> 00:09:21.480
<v Speaker 2>I mean, the free-threaded stuff, there's a lot of things that had to happen.

00:09:21.680 --> 00:09:24.160
<v Speaker 2>And actually, my next topic is also on free-threaded Python.

00:09:24.740 --> 00:09:30.840
<v Speaker 2>And it's not just let's take out the gil, but they had to rework memory management, the structure of objects.

00:09:30.940 --> 00:09:33.880
<v Speaker 2>There's like a lot of moving parts in order to make this possible.

00:09:34.759 --> 00:09:35.560
<v Speaker 1>Okay, cool.

00:09:37.080 --> 00:09:44.560
<v Speaker 1>Well, okay, so with the free-threaded Python, so now we have in 3.13, we had two versions of Python released.

00:09:44.680 --> 00:09:46.880
<v Speaker 1>So if you go to download Python, you get a choice.

00:09:47.640 --> 00:09:53.480
<v Speaker 1>And with uv too, you can add a T to it, 3.14 T, and you get the free threaded version.

00:09:56.639 --> 00:10:03.640
<v Speaker 1>And also definitely encourage everybody to test and publish that you've tested.

00:10:03.670 --> 00:10:08.700
<v Speaker 1>In CI, you go ahead and test it because the CI tools support both.

00:10:09.540 --> 00:10:10.720
<v Speaker 1>Why do we care about this?

00:10:10.840 --> 00:10:19.960
<v Speaker 1>Well, because some people are migrating to it because as with 3.14, it's no longer just an experimental thing.

00:10:20.030 --> 00:10:25.960
<v Speaker 1>So here, I'm going to cover an article by Giovanni Barili.

00:10:26.460 --> 00:10:27.540
<v Speaker 1>Sorry, Giovanni.

00:10:27.750 --> 00:10:28.620
<v Speaker 1>I can pronounce that one.

00:10:28.740 --> 00:10:31.480
<v Speaker 2>I'm pretty sure this is the same Giovanni that makes Granian.

00:10:32.879 --> 00:10:33.920
<v Speaker 1>Oh, okay.

00:10:35.320 --> 00:10:35.880
<v Speaker 1>I think so.

00:10:36.500 --> 00:10:39.580
<v Speaker 1>The future of Python web services looks gill-free.

00:10:39.920 --> 00:10:48.460
<v Speaker 1>So it starts out with two major changes when comparing free-threaded variant of 3.14 versus 3.13.

00:10:50.220 --> 00:10:55.820
<v Speaker 1>First, free-threaded support now reaches phase two, meaning it is no longer considered experimental.

00:10:56.280 --> 00:11:00.380
<v Speaker 1>So everybody's on board with this is what we're going to at least try to do for a while.

00:11:02.120 --> 00:11:07.820
<v Speaker 1>Not just try to do, but the Python core team is supporting this, and we're going to do it moving forward.

00:11:08.440 --> 00:11:08.900
<v Speaker 1>What does that mean?

00:11:09.060 --> 00:11:12.640
<v Speaker 1>it means that people can depend on a free-threaded version

00:11:12.730 --> 00:11:15.180
<v Speaker 1>and change their code accordingly if necessary,

00:11:16.100 --> 00:11:17.020
<v Speaker 1>which is great,

00:11:18.300 --> 00:11:21.900
<v Speaker 1>especially for a lot of ASIC stuff like web services.

00:11:22.620 --> 00:11:26.060
<v Speaker 1>So secondly, the implementation is now completed,

00:11:26.240 --> 00:11:29.740
<v Speaker 1>meaning that the workarounds introduced in 3.13

00:11:30.570 --> 00:11:32.000
<v Speaker 1>make code sound.

00:11:33.800 --> 00:11:36.319
<v Speaker 1>To make code sound without the gil are now gone

00:11:36.340 --> 00:11:41.820
<v Speaker 1>and free-threaded implementation now uses the adaptive interpreter as the gill-enabled variant

00:11:42.960 --> 00:11:49.820
<v Speaker 1>what does that mean uh it means that the option the additional optimizations oh okay also those

00:11:49.980 --> 00:11:57.380
<v Speaker 1>facts plus additional optimizations make the performance penalty that we had in 313 way better

00:11:57.720 --> 00:12:05.120
<v Speaker 1>or way less annoying so in 313 the free thread if you're doing non-async code just straight code

00:12:05.700 --> 00:12:09.460
<v Speaker 1>you had to face like a 35% time penalty, which sucks.

00:12:10.940 --> 00:12:12.820
<v Speaker 1>Now it's like a 5% to 10% difference,

00:12:13.000 --> 00:12:14.840
<v Speaker 1>and I think it's going to shrink even more.

00:12:16.660 --> 00:12:20.660
<v Speaker 1>So if you don't need the free-threaded, don't use it,

00:12:21.700 --> 00:12:23.140
<v Speaker 1>or use it, whatever.

00:12:23.540 --> 00:12:26.620
<v Speaker 1>But if it's time-critical, don't right now.

00:12:26.740 --> 00:12:27.900
<v Speaker 1>But I think it's going to get better.

00:12:29.480 --> 00:12:32.500
<v Speaker 1>And especially if you are using async and stuff,

00:12:32.800 --> 00:12:35.220
<v Speaker 1>it's way faster to use the free-threaded, of course.

00:12:35.640 --> 00:12:41.320
<v Speaker 1>So this article does a shout out to Miguel Grimberg's article about performance.

00:12:41.560 --> 00:12:43.420
<v Speaker 1>I think we covered it last week.

00:12:43.420 --> 00:12:43.640
<v Speaker 2>You did.

00:12:44.460 --> 00:12:44.740
<v Speaker 1>Okay.

00:12:45.100 --> 00:12:45.780
<v Speaker 2>No, maybe I did.

00:12:46.100 --> 00:12:46.460
<v Speaker 2>We did.

00:12:46.460 --> 00:12:47.900
<v Speaker 2>I can't remember who started it.

00:12:49.900 --> 00:12:53.020
<v Speaker 2>So this article has a whole bunch more benchmarks,

00:12:53.240 --> 00:12:54.820
<v Speaker 1>and I'm not going to walk through all of them,

00:12:54.980 --> 00:13:00.879
<v Speaker 1>but talks about doing a service with JSON responses

00:13:01.700 --> 00:13:08.300
<v Speaker 1>and both measuring with WSGI and ASCII implementations.

00:13:08.550 --> 00:13:12.780
<v Speaker 1>And yes, talking about using Granian.

00:13:13.080 --> 00:13:14.000
<v Speaker 1>So that makes sense.

00:13:15.320 --> 00:13:16.440
<v Speaker 1>So I'm going to jump down.

00:13:16.440 --> 00:13:17.340
<v Speaker 1>So there's a bunch of metrics,

00:13:17.410 --> 00:13:23.980
<v Speaker 1>but basically the end result is if you're using ASCII or ASGI,

00:13:25.120 --> 00:13:27.220
<v Speaker 1>the free-threaded one is obviously the way to go.

00:13:28.660 --> 00:13:35.360
<v Speaker 1>And you don't get really much of a hit because of free-threaded implementation at all.

00:13:35.680 --> 00:13:38.700
<v Speaker 1>You get speedups and memory usage is reasonable.

00:13:41.380 --> 00:13:48.320
<v Speaker 1>So what I want people to do, if you're going to check this out, is to jump down to the final thoughts because there's some great stuff here.

00:13:49.480 --> 00:13:53.560
<v Speaker 1>Essentially, so what do I want to pick out?

00:13:55.840 --> 00:14:07.600
<v Speaker 1>On asynchronous protocols like ASGI, despite the fact that concurrency model doesn't change that much, it's a shift from one event loop per process to one event loop per thread.

00:14:08.120 --> 00:14:09.680
<v Speaker 1>I think that's a big change, actually.

00:14:10.560 --> 00:14:16.060
<v Speaker 1>Just the fact that we no longer need to scale memory allocations just to use more CPU is a massive improvement.

00:14:16.500 --> 00:14:16.820
<v Speaker 1>That's cool.

00:14:16.980 --> 00:14:17.580
<v Speaker 1>Didn't know that.

00:14:17.980 --> 00:14:18.400
<v Speaker 1>That's neat.

00:14:19.280 --> 00:14:22.780
<v Speaker 1>For everybody out there coding a web application in Python,

00:14:23.460 --> 00:14:25.300
<v Speaker 1>simplifying the concurrency paradigms

00:14:25.300 --> 00:14:27.460
<v Speaker 1>and the deployment process of such applications

00:14:27.730 --> 00:14:29.340
<v Speaker 1>is a good thing, obviously.

00:14:30.140 --> 00:14:31.240
<v Speaker 1>And the conclusion being,

00:14:33.779 --> 00:14:36.460
<v Speaker 1>for me, the future of Python web services

00:14:36.800 --> 00:14:37.740
<v Speaker 1>looks like the Guilfri.

00:14:37.980 --> 00:14:39.940
<v Speaker 1>So if you're doing web services,

00:14:40.800 --> 00:14:42.440
<v Speaker 1>try out the free-threaded version.

00:14:43.380 --> 00:14:45.500
<v Speaker 1>At least one developer there is completely happy

00:14:45.660 --> 00:14:46.740
<v Speaker 1>with what we have now.

00:14:47.320 --> 00:14:48.880
<v Speaker 2>Yeah, that's super nice.

00:14:49.000 --> 00:14:50.660
<v Speaker 2>A bit of real-time follow-up.

00:14:51.100 --> 00:14:51.460
<v Speaker 2>Yes, indeed.

00:14:51.800 --> 00:14:53.120
<v Speaker 2>It is the same Giovanni.

00:14:54.480 --> 00:14:54.880
<v Speaker 2>All right.

00:14:55.040 --> 00:14:55.980
<v Speaker 2>Who creates Granian.

00:14:56.720 --> 00:15:01.280
<v Speaker 2>So with any of these web production app servers

00:15:01.800 --> 00:15:03.760
<v Speaker 2>where you run your Python code when it's running,

00:15:04.140 --> 00:15:05.120
<v Speaker 2>whether it's Django, Flask, whatever,

00:15:05.360 --> 00:15:07.260
<v Speaker 2>there's a process that runs your code.

00:15:07.900 --> 00:15:12.100
<v Speaker 2>Very often, the way that we've got them to work around the GIL,

00:15:12.280 --> 00:15:14.940
<v Speaker 2>like in the web, the GIL is a much smaller problem.

00:15:16.080 --> 00:15:17.260
<v Speaker 2>It's still a problem, but it's less.

00:15:17.500 --> 00:15:23.060
<v Speaker 2>because what we do often is we create a web garden.

00:15:23.740 --> 00:15:25.140
<v Speaker 2>So when you set up Grand, you can say,

00:15:25.380 --> 00:15:27.140
<v Speaker 2>and start four copies of yourself.

00:15:28.780 --> 00:15:31.100
<v Speaker 2>So that, yes, they all have the gil,

00:15:31.560 --> 00:15:34.240
<v Speaker 2>but we'll round-robin request between the four.

00:15:34.260 --> 00:15:37.200
<v Speaker 2>So if you've got four cores, four CPUs,

00:15:37.760 --> 00:15:40.560
<v Speaker 2>you basically can say, well, each one of these is dedicated to a CPU,

00:15:40.720 --> 00:15:42.260
<v Speaker 2>and it can kind of match, right?

00:15:44.720 --> 00:15:46.660
<v Speaker 2>Yeah, so we're running on Grand, good stuff.

00:15:47.200 --> 00:15:52.460
<v Speaker 2>But the thing is, when you have this free-threaded option,

00:15:52.810 --> 00:15:55.440
<v Speaker 2>you can actually have true concurrency in your one worker.

00:15:55.830 --> 00:15:59.160
<v Speaker 2>So instead of scaling out four copies, you could have just one

00:15:59.740 --> 00:16:02.120
<v Speaker 2>and just say, let that one take 10 concurrent requests

00:16:02.780 --> 00:16:03.780
<v Speaker 2>or whatever it needs to take.

00:16:05.280 --> 00:16:07.180
<v Speaker 2>So that's how the free-threaded gets better.

00:16:07.210 --> 00:16:08.340
<v Speaker 2>And like, well, okay, why?

00:16:08.420 --> 00:16:10.760
<v Speaker 2>What's like six, one, half dozen, the other?

00:16:10.880 --> 00:16:11.120
<v Speaker 2>No.

00:16:12.160 --> 00:16:15.079
<v Speaker 2>The memory becomes a problem when you create these little web gardens

00:16:15.200 --> 00:16:18.980
<v Speaker 2>because if normally your server would use half a gig

00:16:19.340 --> 00:16:20.920
<v Speaker 2>and you create four, well, now it's two gigs,

00:16:21.320 --> 00:16:24.080
<v Speaker 2>and maybe that bumps you up in another tier and so on, right?

00:16:25.120 --> 00:16:30.140
<v Speaker 1>Yeah, and if you need that memory for data, you don't have it.

00:16:31.040 --> 00:16:33.120
<v Speaker 2>Yeah, so I think that's kind of one of the angles

00:16:33.320 --> 00:16:35.080
<v Speaker 2>of not just your writing concurrent code,

00:16:35.280 --> 00:16:38.380
<v Speaker 2>but the foundations of your code running can do so more efficiently

00:16:38.580 --> 00:16:42.140
<v Speaker 2>because they're no longer needing to work around processes,

00:16:43.080 --> 00:16:46.120
<v Speaker 2>limitations like, well, we've got to fan out these processes because of the kill.

00:16:47.120 --> 00:16:47.280
<v Speaker 2>Okay.

00:16:48.780 --> 00:16:51.080
<v Speaker 2>Back to the regular scheduled programming.

00:16:52.540 --> 00:16:53.220
<v Speaker 2>Free-threaded Python.

00:16:53.500 --> 00:16:54.060
<v Speaker 2>Let's keep going.

00:16:56.140 --> 00:17:02.540
<v Speaker 2>So here's an interesting article from a snake wearing a very fast,

00:17:02.600 --> 00:17:07.880
<v Speaker 2>it's a very fast snake with jet engines that is wearing a garbage collector.

00:17:08.500 --> 00:17:14.760
<v Speaker 2>So what I want to talk about is unlocking the performance in Python's free-threaded future GC optimization.

00:17:16.699 --> 00:17:23.400
<v Speaker 2>So garbage collection is not something that we frequently talk about a ton in Python.

00:17:25.380 --> 00:17:26.459
<v Speaker 2>We don't think a lot about GC.

00:17:26.630 --> 00:17:27.740
<v Speaker 2>We don't think a lot about memory.

00:17:29.280 --> 00:17:30.100
<v Speaker 2>I don't know why.

00:17:30.700 --> 00:17:34.140
<v Speaker 2>As you know, Brian, in C++, people are always about it.

00:17:34.240 --> 00:17:38.160
<v Speaker 2>Like, oh my gosh, what size of point and what size of integer

00:17:38.300 --> 00:17:39.320
<v Speaker 2>are we going to use for this?

00:17:39.720 --> 00:17:42.660
<v Speaker 2>And it's just way more in the forefront.

00:17:43.000 --> 00:17:45.340
<v Speaker 2>But it's nice every now and then to peel back the covers

00:17:45.600 --> 00:17:47.580
<v Speaker 2>and get a look at what's going on here.

00:17:48.679 --> 00:17:53.800
<v Speaker 2>So Neil Schemenauer wrote this article over here

00:17:53.940 --> 00:17:54.640
<v Speaker 2>from QuantSight.

00:17:55.480 --> 00:17:57.260
<v Speaker 2>They do a bunch of data science primarily.

00:17:57.880 --> 00:17:58.680
<v Speaker 2>But here, check this out.

00:17:59.480 --> 00:18:01.560
<v Speaker 2>This is news to me, actually, even though I'm still

00:18:01.720 --> 00:18:02.360
<v Speaker 2>interested in these things.

00:18:02.620 --> 00:18:04.300
<v Speaker 2>It says, first, the most important thing to know

00:18:04.460 --> 00:18:07.140
<v Speaker 2>is that free-threaded Python uses a different garbage

00:18:07.520 --> 00:18:10.380
<v Speaker 2>collector than the default Python, the gil-enabled.

00:18:11.660 --> 00:18:13.760
<v Speaker 2>The gilded-- I still want to call it the gilded Python.

00:18:14.200 --> 00:18:15.320
<v Speaker 2>The gilded Python.

00:18:16.720 --> 00:18:19.240
<v Speaker 2>So we have the default GC, which people probably

00:18:19.520 --> 00:18:21.180
<v Speaker 2>understand pretty well.

00:18:21.760 --> 00:18:25.020
<v Speaker 2>When you create a number, like 1,000,

00:18:25.640 --> 00:18:29.440
<v Speaker 2>when you create a list, when you create an object from a class,

00:18:30.340 --> 00:18:34.480
<v Speaker 2>All of those things have a data structure at the top of them

00:18:34.900 --> 00:18:37.740
<v Speaker 2>that allows them to manage and track the memory.

00:18:39.240 --> 00:18:41.660
<v Speaker 2>And people think of the GIL as being a threading thing.

00:18:42.240 --> 00:18:46.300
<v Speaker 2>The GIL is really a protection mechanism for memory management

00:18:46.390 --> 00:18:46.840
<v Speaker 2>in Python.

00:18:47.660 --> 00:18:51.520
<v Speaker 2>It's basically protection against a race condition

00:18:51.960 --> 00:18:52.960
<v Speaker 2>in the reference counting.

00:18:53.590 --> 00:18:56.120
<v Speaker 2>So basically, interacting with this structure

00:18:56.320 --> 00:18:59.800
<v Speaker 2>that has seven things reference me and I reference it.

00:18:59.940 --> 00:19:00.860
<v Speaker 2>You know what I mean?

00:19:00.920 --> 00:19:02.100
<v Speaker 2>Like that sort of deal.

00:19:02.660 --> 00:19:05.560
<v Speaker 2>And when the thing goes out of scope, it decrements that,

00:19:05.620 --> 00:19:09.040
<v Speaker 2>and that's where the gil comes in to make that decrement safe and so on.

00:19:09.600 --> 00:19:13.600
<v Speaker 2>So that's the regular one.

00:19:13.860 --> 00:19:18.960
<v Speaker 2>But we don't even have that structure anymore in the free-threaded one.

00:19:21.020 --> 00:19:21.320
<v Speaker 2>So what?

00:19:22.040 --> 00:19:24.380
<v Speaker 2>So instead, when you allocate stuff,

00:19:24.520 --> 00:19:30.040
<v Speaker 2>it goes into a special memory managed heap called memalloc,

00:19:30.220 --> 00:19:31.620
<v Speaker 2>or managed by memalloc.

00:19:33.080 --> 00:19:36.020
<v Speaker 2>And this allows the GC to loop over all these objects

00:19:36.580 --> 00:19:40.380
<v Speaker 2>and figure out which ones are junk and which ones aren't.

00:19:40.480 --> 00:19:44.340
<v Speaker 2>A little bit like a mark and sweep garbage collector.

00:19:45.560 --> 00:19:47.640
<v Speaker 2>So there's a couple of interesting things.

00:19:51.360 --> 00:19:54.500
<v Speaker 2>One is that most of the mark and sweep garbage collectors

00:19:54.760 --> 00:19:58.380
<v Speaker 2>and that type of thing, they have generations,

00:19:59.060 --> 00:20:00.180
<v Speaker 2>like a generational one.

00:20:00.880 --> 00:20:03.980
<v Speaker 2>So it has Gen 0s where most objects come.

00:20:03.980 --> 00:20:06.240
<v Speaker 2>Then it'll check that frequently and so on.

00:20:06.280 --> 00:20:08.220
<v Speaker 2>That's also how the garbage collector that

00:20:08.320 --> 00:20:11.240
<v Speaker 2>looks for cycles in the regular Python goes.

00:20:12.480 --> 00:20:16.780
<v Speaker 2>But it doesn't work here because of unmanaged C

00:20:17.120 --> 00:20:20.020
<v Speaker 2>extensions and a bunch of stuff like that.

00:20:20.680 --> 00:20:28.040
<v Speaker 2>So what it does is it can mark all the objects reachable

00:20:28.240 --> 00:20:29.880
<v Speaker 2>from what it calls known roots.

00:20:30.080 --> 00:20:31.960
<v Speaker 2>So instead of trying to scan the entire memory space,

00:20:32.080 --> 00:20:34.640
<v Speaker 2>it says, well, what are globals, what are locals, what are--

00:20:34.640 --> 00:20:36.740
<v Speaker 2>you know, it sort of follow those,

00:20:36.880 --> 00:20:38.480
<v Speaker 2>and it marks those as active.

00:20:39.540 --> 00:20:42.840
<v Speaker 2>And they can automatically be excluded from the search,

00:20:44.260 --> 00:20:46.380
<v Speaker 2>which is kind of like a generational optimization.

00:20:47.040 --> 00:20:48.060
<v Speaker 2>So pretty interesting.

00:20:48.160 --> 00:20:49.100
<v Speaker 2>But here's the takeaway.

00:20:49.800 --> 00:20:51.560
<v Speaker 2>We were talking about making things faster

00:20:51.570 --> 00:20:52.780
<v Speaker 2>by using free-threaded Python.

00:20:54.660 --> 00:20:58.060
<v Speaker 2>With this one, the free-threaded GC collection

00:20:58.090 --> 00:21:02.440
<v Speaker 2>is between 2 to 12 times faster than the 3.13 version.

00:21:03.300 --> 00:21:03.600
<v Speaker 1>Wow.

00:21:03.740 --> 00:21:04.880
<v Speaker 2>That's pretty wild, right?

00:21:05.940 --> 00:21:06.020
<v Speaker 1>Yeah.

00:21:06.600 --> 00:21:06.860
<v Speaker 2>Yeah.

00:21:07.580 --> 00:21:09.340
<v Speaker 2>So depending on how your algorithms work,

00:21:09.340 --> 00:21:13.320
<v Speaker 2>do you have a super pointer-heavy allocation-heavy type

00:21:13.350 --> 00:21:16.100
<v Speaker 2>of algorithm, then we'll probably see a bigger benefit

00:21:16.170 --> 00:21:18.199
<v Speaker 2>than if you just allocate a few things

00:21:18.220 --> 00:21:20.000
<v Speaker 2>and jam on those.

00:21:21.540 --> 00:21:23.980
<v Speaker 2>So anyway, if you want to peel back the covers

00:21:24.180 --> 00:21:25.900
<v Speaker 2>and see the GC, I didn't even know they were different,

00:21:26.200 --> 00:21:28.620
<v Speaker 2>but apparently they're different, and presumably

00:21:28.940 --> 00:21:32.420
<v Speaker 2>maybe they could even have multi-threaded GC stuff going on.

00:21:32.940 --> 00:21:35.260
<v Speaker 2>That might be cool, or background thread GC.

00:21:35.560 --> 00:21:36.820
<v Speaker 2>I know there's a lot of different things

00:21:36.960 --> 00:21:38.120
<v Speaker 2>that happen in some of these systems,

00:21:39.360 --> 00:21:40.340
<v Speaker 3>these garbage collector systems.

00:21:40.880 --> 00:21:43.600
<v Speaker 2>Yeah, and finally, a little bit of real time,

00:21:44.400 --> 00:21:47.100
<v Speaker 2>not quite real time, but real world follow-up, I'll say.

00:21:47.620 --> 00:21:50.540
<v Speaker 2>So for some reason, I can't remember what I was doing.

00:21:50.900 --> 00:21:52.860
<v Speaker 2>Something I had to do, I had to go back,

00:21:53.500 --> 00:21:56.380
<v Speaker 2>go through all the examples of my async programming course

00:21:56.510 --> 00:21:57.060
<v Speaker 2>at Talk Python.

00:21:58.140 --> 00:21:59.520
<v Speaker 2>So I went back in there.

00:21:59.990 --> 00:22:01.160
<v Speaker 2>And I said, all right, well, let me just

00:22:01.380 --> 00:22:03.900
<v Speaker 2>make sure everything's running, update all the dependencies.

00:22:05.380 --> 00:22:06.380
<v Speaker 2>I can't remember what it was.

00:22:06.550 --> 00:22:07.820
<v Speaker 2>But anyway, something had to be updated.

00:22:08.360 --> 00:22:09.960
<v Speaker 2>So I made sure all the dependencies got updated

00:22:10.200 --> 00:22:11.520
<v Speaker 2>and pinned them to newer ones.

00:22:11.630 --> 00:22:13.460
<v Speaker 2>And I'm like, well, there's a lot of changes here.

00:22:13.560 --> 00:22:14.400
<v Speaker 2>Let me make sure they're all running.

00:22:15.040 --> 00:22:21.300
<v Speaker 2>around and I ran every example from the course, which is probably 25 little apps.

00:22:22.520 --> 00:22:26.300
<v Speaker 2>And many of them are like, well, here's a web app, or here's just a way to do

00:22:27.020 --> 00:22:27.900
<v Speaker 2>multiprocessing, have a look at it.

00:22:27.930 --> 00:22:31.140
<v Speaker 2>But a lot of them were like, let's actually do this with threads.

00:22:31.490 --> 00:22:34.460
<v Speaker 2>And in this example with doing IO, you see it does make a difference.

00:22:34.570 --> 00:22:38.980
<v Speaker 2>In this example doing computational stuff, like no different or slower because

00:22:39.240 --> 00:22:42.400
<v Speaker 2>there's just overhead and it's still running one at a time, right?

00:22:43.400 --> 00:22:52.900
<v Speaker 2>Well, I decided to type uv-python 3.14t, run those examples,

00:22:54.120 --> 00:22:55.100
<v Speaker 2>and run them free-threaded.

00:22:55.330 --> 00:22:57.840
<v Speaker 2>The ones that used to have no benefit or a net negative

00:22:59.530 --> 00:23:01.820
<v Speaker 2>are now like seven times faster.

00:23:05.020 --> 00:23:11.280
<v Speaker 2>It would go like synchronous one, 10.2 seconds.

00:23:12.360 --> 00:23:16.600
<v Speaker 2>Async or threaded one, rather, 10.25 seconds.

00:23:17.760 --> 00:23:21.000
<v Speaker 2>And now just put the T on the end, three seconds.

00:23:23.420 --> 00:23:24.360
<v Speaker 2>That's pretty cool.

00:23:24.760 --> 00:23:25.960
<v Speaker 2>That is pretty cool, man.

00:23:26.480 --> 00:23:32.200
<v Speaker 2>So to me, it's really down to what are the framework--

00:23:32.320 --> 00:23:34.500
<v Speaker 2>what are the packages and libraries that you're using?

00:23:35.420 --> 00:23:38.100
<v Speaker 2>If it's green across the board for free threaded,

00:23:39.200 --> 00:23:39.940
<v Speaker 2>that's pretty interesting.

00:23:40.240 --> 00:23:49.520
<v Speaker 1>opens up some real possibilities. Yeah. Also, I think I'll be looking forward to the, I know,

00:23:49.860 --> 00:23:54.920
<v Speaker 1>I don't know if we have a deadline or timeline for this, but when we can go back to having one

00:23:55.620 --> 00:24:05.500
<v Speaker 1>version of Python and it's being the free threaded one, it'll be, you know, maybe we can switch it

00:24:05.540 --> 00:24:10.200
<v Speaker 1>to be the default is the free threaded one. And for a couple of versions, there's the other one

00:24:10.220 --> 00:24:11.920
<v Speaker 2>You got to say 314G.

00:24:12.640 --> 00:24:13.880
<v Speaker 2>Yeah, or something like that.

00:24:15.200 --> 00:24:27.720
<v Speaker 1>And I think that would be good because the default way we think about how to program and how to program quickly and the rules of thumb based on the GIL will be gone.

00:24:28.060 --> 00:24:29.180
<v Speaker 1>So we'll teach people different.

00:24:30.620 --> 00:24:30.700
<v Speaker 1>Yeah.

00:24:31.000 --> 00:24:32.440
<v Speaker 1>So anyway.

00:24:32.940 --> 00:24:33.040
<v Speaker 2>Yeah.

00:24:33.440 --> 00:24:37.120
<v Speaker 2>And before we move on real quick, a follow up to your topic from chart here.

00:24:37.540 --> 00:24:49.760
<v Speaker 2>Also, if you're on Kubernetes, it makes setting reasonable CPU and memory requests more difficult to have to scale out the WebGarden type stuff.

00:24:51.260 --> 00:24:56.240
<v Speaker 2>So if you can just have one process, like this is the one, I think it's easier.

00:24:56.990 --> 00:24:58.140
<v Speaker 2>I believe that's what he was getting at.

00:24:59.000 --> 00:24:59.240
<v Speaker 2>Yeah.

00:25:00.120 --> 00:25:06.760
<v Speaker 1>And also, from the C++ world, yes, processes and threads both can work.

00:25:08.680 --> 00:25:11.020
<v Speaker 1>but thinking in threads is easier than thinking about

00:25:11.300 --> 00:25:13.900
<v Speaker 2>multi-processing. 100%. Because you have shared memory.

00:25:14.980 --> 00:25:18.760
<v Speaker 2>You don't have to proxy stuff over and figure out how to sync it back up and copy it.

00:25:19.580 --> 00:25:21.460
<v Speaker 2>To queues and all sorts of...

00:25:24.220 --> 00:25:27.280
<v Speaker 2>I can't resist. I've got to say it. I've got to put this out there in the world

00:25:27.840 --> 00:25:30.760
<v Speaker 2>and see what you think. One thing right now, we have

00:25:31.640 --> 00:25:35.600
<v Speaker 2>the gilded Python and we have the free-threaded Python. And I can choose to run the free-threaded

00:25:35.620 --> 00:25:37.300
<v Speaker 2>as we've been talking about, if I want that.

00:25:38.320 --> 00:25:39.360
<v Speaker 2>But what about this, Brian?

00:25:39.440 --> 00:25:40.640
<v Speaker 2>This is what I want to put out in the world.

00:25:42.539 --> 00:25:44.820
<v Speaker 2>I want-- like right now, if I have a project,

00:25:45.800 --> 00:25:47.800
<v Speaker 2>and I've got some data, and I need to process this

00:25:47.800 --> 00:25:50.420
<v Speaker 2>in parallel on the gilded one, I need

00:25:50.580 --> 00:25:52.700
<v Speaker 2>to maybe do multiprocessing.

00:25:52.980 --> 00:25:56.080
<v Speaker 2>So that's going to spin up five little baby Python processes

00:25:56.820 --> 00:25:58.820
<v Speaker 2>all to go jam on that, right?

00:25:59.440 --> 00:26:00.660
<v Speaker 2>That's how you get around the gilded.

00:26:01.559 --> 00:26:02.500
<v Speaker 2>What about this?

00:26:02.680 --> 00:26:05.720
<v Speaker 2>What if you could pass a flag to multiprocessing,

00:26:05.920 --> 00:26:09.520
<v Speaker 2>and it would start up free-threaded Python with threads

00:26:10.250 --> 00:26:13.440
<v Speaker 2>instead of a bunch of different processes that can't coordinate

00:26:14.520 --> 00:26:15.680
<v Speaker 2>on whatever algorithm.

00:26:15.880 --> 00:26:18.400
<v Speaker 2>You just say, run multiprocessing, but this time

00:26:18.560 --> 00:26:23.340
<v Speaker 2>do it in free-threaded Python as your little multiprocess burst,

00:26:23.720 --> 00:26:24.980
<v Speaker 2>and then come back to me with the answers.

00:26:25.240 --> 00:26:25.740
<v Speaker 2>That would be cool.

00:26:30.000 --> 00:26:30.400
<v Speaker 1>Yeah.

00:26:31.340 --> 00:26:32.040
<v Speaker 2>You're not convinced?

00:26:32.620 --> 00:26:34.120
<v Speaker 2>I mean, you could that way, because you won't write.

00:26:34.120 --> 00:26:35.700
<v Speaker 1>I mean, your code has to be different, though.

00:26:36.660 --> 00:26:37.880
<v Speaker 2>Just that one function, though.

00:26:38.270 --> 00:26:39.120
<v Speaker 1>And you don't have to pay.

00:26:39.350 --> 00:26:39.740
<v Speaker 2>You know what I mean?

00:26:39.880 --> 00:26:42.240
<v Speaker 2>You're like, this one function that I'm going to call multiprocessing on,

00:26:42.920 --> 00:26:46.600
<v Speaker 2>I'm going to write it assuming that it will be an advantage to be threaded,

00:26:48.040 --> 00:26:49.960
<v Speaker 3>or it will be an advantage to be concurrent.

00:26:50.070 --> 00:26:51.900
<v Speaker 2>But the rest of my code, I don't have to rewrite.

00:26:52.260 --> 00:26:55.880
<v Speaker 2>Like, it'd be a cool way to sort of bring, kind of like Cython lets you bring

00:26:55.880 --> 00:26:58.400
<v Speaker 2>and say, this one function, if just this were way faster,

00:26:58.410 --> 00:26:59.340
<v Speaker 2>it would make all the difference.

00:27:00.240 --> 00:27:01.640
<v Speaker 2>You could do that, but for free threading.

00:27:02.240 --> 00:27:03.060
<v Speaker 2>I would like to see that.

00:27:03.720 --> 00:27:04.260
<v Speaker 2>It would be interesting.

00:27:04.440 --> 00:27:04.540
<v Speaker 1>Yeah.

00:27:05.840 --> 00:27:12.700
<v Speaker 1>Yeah, or just a chunk to say, like, yeah, this subsystem is a free thread.

00:27:12.860 --> 00:27:13.840
<v Speaker 2>With free threading.

00:27:16.080 --> 00:27:16.300
<v Speaker 1>No.

00:27:16.640 --> 00:27:16.780
<v Speaker 1>All right.

00:27:17.120 --> 00:27:17.340
<v Speaker 1>Go ahead.

00:27:17.940 --> 00:27:18.080
<v Speaker 1>Okay.

00:27:18.660 --> 00:27:23.800
<v Speaker 1>Completely different tangent is we're going to go back in time by a couple of one or two episodes.

00:27:25.600 --> 00:27:31.200
<v Speaker 1>We've been, the last few episodes, or two or three, talking about lazy imports.

00:27:31.280 --> 00:27:34.940
<v Speaker 1>because I thought they were coming in 315.

00:27:35.720 --> 00:27:38.320
<v Speaker 1>They're proposed in 315, but they're not here.

00:27:38.640 --> 00:27:40.640
<v Speaker 1>So, or they're not accepted yet.

00:27:40.730 --> 00:27:41.320
<v Speaker 1>I haven't checked.

00:27:41.350 --> 00:27:42.820
<v Speaker 1>I don't think that anything's changed.

00:27:45.500 --> 00:27:48.960
<v Speaker 1>So last episode, I did talk about lazy imports

00:27:48.960 --> 00:27:49.800
<v Speaker 1>that you can use today.

00:27:51.620 --> 00:27:55.720
<v Speaker 1>Responding to that, Bob Bilderbos wrote up,

00:27:56.880 --> 00:27:58.500
<v Speaker 1>had a discussion on LinkedIn.

00:27:58.560 --> 00:28:07.940
<v Speaker 1>and part of that discussion we had wheel mccugan hop in and said that he's excited about the pep

00:28:08.380 --> 00:28:15.300
<v Speaker 1>and that he uses lazy he has a lazy loading mechanism for textuals widgets which totally

00:28:15.540 --> 00:28:21.760
<v Speaker 1>makes sense like widgets um you might just need a button but you don't need like all of the

00:28:21.900 --> 00:28:27.100
<v Speaker 1>all of the other widgets be cool if you could just load what you needed and he's got that so i

00:28:27.040 --> 00:28:46.600
<v Speaker 1>Checked it out. I was checking this out. And I think this is sort of a small topic, but I think it's cool. So the idea that I want to highlight is I presented last week in a method for doing lazy loading now or lazy importing now on things that you depend on.

00:28:47.260 --> 00:28:49.580
<v Speaker 2>You're like, you don't have to wait. You can be lazy today.

00:28:50.500 --> 00:29:10.140
<v Speaker 1>But what about like, yeah, exactly. What about if you're the package? If you have a big package that you have a bunch of stuff that people might want to import, making it so that you're not the problem, that your package is going to import really fast because behind the scenes you're doing lazy importing for people.

00:29:10.680 --> 00:29:12.380
<v Speaker 1>And that's essentially what Textual does.

00:29:12.960 --> 00:29:15.620
<v Speaker 1>And the implementation is, like, really easy.

00:29:17.980 --> 00:29:27.600
<v Speaker 1>He's using a, basically, when you access anything, he's overriding the get adder function.

00:29:28.200 --> 00:29:33.400
<v Speaker 1>So if you access a widget, it's going to try to just grab it out of a dictionary.

00:29:34.220 --> 00:29:36.240
<v Speaker 1>And, of course, right away it's going to fail.

00:29:37.420 --> 00:29:45.800
<v Speaker 1>And then if it fails, he goes ahead and imports it and then stores it in.

00:29:45.850 --> 00:29:48.600
<v Speaker 1>So the next time somebody tries to grab it, it's going to be there.

00:29:50.420 --> 00:29:53.260
<v Speaker 1>So there's a little bit of misdirection here.

00:29:54.640 --> 00:30:01.300
<v Speaker 1>But in the end, you get lazy loading on every widget access.

00:30:02.020 --> 00:30:02.920
<v Speaker 1>So pretty cool.

00:30:05.300 --> 00:30:07.400
<v Speaker 1>So I thought, you know, this is pretty neat.

00:30:07.760 --> 00:30:09.220
<v Speaker 1>I don't want it just to be hidden.

00:30:09.820 --> 00:30:10.440
<v Speaker 1>It's not hidden.

00:30:10.640 --> 00:30:11.520
<v Speaker 1>It's an open source project.

00:30:12.060 --> 00:30:19.860
<v Speaker 1>So I went ahead and wrote all this up in a new post called Polite Lazy Imports for Python Package Maintainers.

00:30:20.380 --> 00:30:20.820
<v Speaker 3>Very nice.

00:30:20.900 --> 00:30:22.120
<v Speaker 1>So I'll link to that also.

00:30:22.800 --> 00:30:25.320
<v Speaker 1>But I just thought that the implementation is totally clever.

00:30:26.460 --> 00:30:31.180
<v Speaker 1>And maybe for other package maintainers, this is like, well, yeah, that's the way you do it.

00:30:31.799 --> 00:30:33.880
<v Speaker 1>But it was new to me, and I thought it was neat.

00:30:34.220 --> 00:30:35.020
<v Speaker 1>So there we go.

00:30:35.580 --> 00:30:38.580
<v Speaker 1>Also want to shout out, I didn't realize there was both get adder.

00:30:38.920 --> 00:30:45.460
<v Speaker 1>Get adder is used for if it's not found, for missing items.

00:30:46.180 --> 00:30:48.580
<v Speaker 1>But also there's another one called get attribute.

00:30:49.300 --> 00:30:50.780
<v Speaker 1>And I did look up.

00:30:50.780 --> 00:30:57.660
<v Speaker 1>I really appreciate that Trey Hunter had posted this every Dunder method article.

00:30:57.960 --> 00:30:59.720
<v Speaker 1>So pretty cool.

00:31:00.180 --> 00:31:02.120
<v Speaker 1>I totally have this bookmarked.

00:31:02.300 --> 00:31:05.840
<v Speaker 1>that I'm like, if I want to understand the Dunder method

00:31:05.940 --> 00:31:06.880
<v Speaker 1>in Python, I go here.

00:31:07.500 --> 00:31:08.220
<v Speaker 1>So yeah, very cool.

00:31:09.680 --> 00:31:13.000
<v Speaker 2>And I know it for classes, but for modules as well.

00:31:13.080 --> 00:31:13.200
<v Speaker 2>Interesting.

00:31:14.500 --> 00:31:16.880
<v Speaker 1>Yeah, I don't think-- and packages.

00:31:17.060 --> 00:31:18.880
<v Speaker 1>So this is-- you go ahead and throw it.

00:31:19.340 --> 00:31:21.160
<v Speaker 1>Well, I guess it is for modules.

00:31:21.500 --> 00:31:26.160
<v Speaker 1>But if you put that in a Dunder init file,

00:31:26.440 --> 00:31:27.540
<v Speaker 1>then it's for your entire package.

00:31:27.820 --> 00:31:28.180
<v Speaker 2>Yeah, exactly.

00:31:28.620 --> 00:31:28.960
<v Speaker 1>Very cool.

00:31:31.340 --> 00:31:31.800
<v Speaker 2>I love it.

00:31:32.380 --> 00:31:32.440
<v Speaker 2>Nice.

00:31:33.660 --> 00:31:33.840
<v Speaker 2>All right.

00:31:33.980 --> 00:31:34.840
<v Speaker 2>Is that all your extras?

00:31:35.680 --> 00:31:36.440
<v Speaker 2>Well, that's just.

00:31:36.960 --> 00:31:37.640
<v Speaker 2>Your main thing.

00:31:38.220 --> 00:31:38.800
<v Speaker 2>That's my main thing.

00:31:39.160 --> 00:31:40.240
<v Speaker 2>What do you got for extras then?

00:31:40.240 --> 00:31:40.420
<v Speaker 2>I do have one extra.

00:31:40.940 --> 00:31:41.240
<v Speaker 2>Do it.

00:31:41.300 --> 00:31:41.940
<v Speaker 1>The one extra.

00:31:42.820 --> 00:31:49.080
<v Speaker 1>So one of the things I've been doing is I've been, you know,

00:31:49.200 --> 00:31:51.840
<v Speaker 1>I paused Python people a long time ago,

00:31:52.540 --> 00:31:54.560
<v Speaker 1>and I totally stopped doing testing code.

00:31:55.240 --> 00:31:57.540
<v Speaker 1>I focused on this too and focused on work,

00:31:57.740 --> 00:31:58.920
<v Speaker 1>but I'm also writing more.

00:31:59.460 --> 00:32:02.200
<v Speaker 1>And one of the things I'm doing is writing a book on test-driven development.

00:32:03.160 --> 00:32:09.940
<v Speaker 1>And I don't have an announcement yet, but in the next couple of weeks, I think I might have an announcement.

00:32:10.280 --> 00:32:14.980
<v Speaker 1>But if you'd like to know, I guess what I'm going to do is I'm going to write the book.

00:32:15.530 --> 00:32:27.000
<v Speaker 1>But to motivate me and to make people not have to wait, I'm doing it as a  you know how Manning does the early access books and Pragmatic has beta books?

00:32:27.520 --> 00:32:28.580
<v Speaker 1>I'm kind of doing that.

00:32:28.820 --> 00:32:31.840
<v Speaker 1>I'm doing a lean, trying to do a lean startup applied to books.

00:32:32.440 --> 00:32:38.020
<v Speaker 1>And I'm going to release it after I've got, I'm like, you know, do a rough first draft,

00:32:38.320 --> 00:32:41.100
<v Speaker 1>clean it up a little bit and then release it for every chapter.

00:32:41.720 --> 00:32:45.160
<v Speaker 1>And then as I go along, incorporate feedback from people.

00:32:45.720 --> 00:32:49.800
<v Speaker 1>And then once the whole thing is good and polished, I might bring on an editor or something,

00:32:49.960 --> 00:32:50.980
<v Speaker 1>or maybe just release it.

00:32:52.360 --> 00:32:56.700
<v Speaker 1>But if you'd like to know more about that, I'll announce it here.

00:32:56.840 --> 00:32:59.060
<v Speaker 1>but also if you'd like to know it the minute it's available,

00:33:00.020 --> 00:33:03.160
<v Speaker 1>join the newsletter over at Python Test and I'll let you know.

00:33:03.420 --> 00:33:05.620
<v Speaker 2>Yeah, we really do appreciate when you join our newsletter.

00:33:07.460 --> 00:33:09.820
<v Speaker 2>You might think, oh, we can just announce these things on social media

00:33:10.200 --> 00:33:13.180
<v Speaker 2>or on the podcast, but it's not the same.

00:33:13.340 --> 00:33:15.280
<v Speaker 2>A lot of people miss it because they miss an episode

00:33:15.520 --> 00:33:18.400
<v Speaker 2>or social media is just a screaming feed of stuff.

00:33:18.840 --> 00:33:19.580
<v Speaker 2>It really makes a difference.

00:33:19.680 --> 00:33:20.440
<v Speaker 2>So join the newsletter.

00:33:21.480 --> 00:33:24.860
<v Speaker 2>Also, I want to talk about there's newsletters

00:33:24.960 --> 00:33:25.900
<v Speaker 1>and then there's newsletters.

00:33:26.300 --> 00:33:31.480
<v Speaker 1>So this isn't really, I don't do a weekly like eight tips on testing.

00:33:31.620 --> 00:33:32.220
<v Speaker 1>That would be cool.

00:33:32.340 --> 00:33:33.500
<v Speaker 1>I just don't have time to do that.

00:33:34.680 --> 00:33:38.840
<v Speaker 1>But so it's mostly announcements if there's something to keep track of.

00:33:39.200 --> 00:33:44.740
<v Speaker 1>But people like you and me and others, we're using things like, I'm using Kit, but there's others.

00:33:46.320 --> 00:33:47.900
<v Speaker 1>I've used other email things before.

00:33:48.540 --> 00:33:50.380
<v Speaker 1>I don't keep track of who's subscribed.

00:33:50.400 --> 00:33:52.540
<v Speaker 1>So I'm not going to sell this to anybody or anything.

00:33:53.080 --> 00:33:54.580
<v Speaker 1>It's just an announcement thing.

00:33:54.840 --> 00:33:56.180
<v Speaker 1>And you can unsubscribe anytime.

00:33:56.400 --> 00:33:57.280
<v Speaker 1>And I don't even know.

00:33:57.580 --> 00:33:59.480
<v Speaker 1>Like once you're unsubscribed, you're gone.

00:33:59.640 --> 00:34:01.000
<v Speaker 1>I don't have the access to that.

00:34:01.260 --> 00:34:07.820
<v Speaker 1>So there's people that abuse newsletters and people that I don't think there's a lot of people in the Python tech space that do, though.

00:34:08.240 --> 00:34:10.020
<v Speaker 1>So it's not spammy.

00:34:10.120 --> 00:34:11.520
<v Speaker 2>Anyway, moving on.

00:34:11.540 --> 00:34:12.659
<v Speaker 2>Do you have any extras for us?

00:34:12.879 --> 00:34:13.840
<v Speaker 2>I actually have a couple.

00:34:14.580 --> 00:34:16.179
<v Speaker 2>Let's jump over and hear about it.

00:34:16.560 --> 00:34:17.480
<v Speaker 2>Very exciting news.

00:34:17.760 --> 00:34:21.679
<v Speaker 2>I am really happy with this new course that I created called

00:34:21.980 --> 00:34:25.460
<v Speaker 2>Agentic AI Programming for Python Devs and Data Scientists.

00:34:26.220 --> 00:34:26.399
<v Speaker 3>Cool.

00:34:27.060 --> 00:34:32.860
<v Speaker 2>Yeah, and the idea is basically, so we have these agentic AI tools

00:34:33.659 --> 00:34:37.399
<v Speaker 2>like Claude Code and Cursor and Juni and all of them,

00:34:37.620 --> 00:34:41.040
<v Speaker 2>but how do you actually be successful with them, right?

00:34:42.020 --> 00:34:45.699
<v Speaker 2>I've talked a few times about just insane productivity

00:34:45.720 --> 00:34:47.639
<v Speaker 2>and really good outcomes that I've had.

00:34:47.659 --> 00:34:50.040
<v Speaker 2>And people are like, how are you doing that?

00:34:51.159 --> 00:34:56.139
<v Speaker 2>So this course is like a real-world set of guidelines

00:34:56.639 --> 00:35:00.060
<v Speaker 2>and examples and best practices for working with agentic AI.

00:35:01.040 --> 00:35:04.820
<v Speaker 2>So we'll spend an hour building a really detailed application,

00:35:05.720 --> 00:35:10.040
<v Speaker 2>but it also talks almost for an hour about guardrails and roadmaps

00:35:10.040 --> 00:35:12.740
<v Speaker 2>and how you get it to do exactly what you want.

00:35:12.860 --> 00:35:15.560
<v Speaker 2>When I say build an app, don't just build the app,

00:35:15.660 --> 00:35:18.660
<v Speaker 2>but build it with uv, write high test tests,

00:35:19.220 --> 00:35:21.100
<v Speaker 2>format it with rough, with this TAML,

00:35:21.460 --> 00:35:22.560
<v Speaker 3>and you don't have to do any--

00:35:22.940 --> 00:35:25.720
<v Speaker 2>but how do you get it to give you what you want,

00:35:26.120 --> 00:35:30.480
<v Speaker 2>not something in the vague general space of what you ask for?

00:35:30.520 --> 00:35:31.040
<v Speaker 2>You know what I mean?

00:35:31.720 --> 00:35:36.460
<v Speaker 2>So sort of a practical agentic AI programming course.

00:35:36.480 --> 00:35:39.660
<v Speaker 2>So I really-- I'm getting crazy amounts of good feedback

00:35:40.080 --> 00:35:40.460
<v Speaker 2>on this course.

00:35:40.660 --> 00:35:41.620
<v Speaker 2>So people check it out.

00:35:41.780 --> 00:35:44.740
<v Speaker 2>The link is in the show notes.

00:35:44.960 --> 00:35:47.900
<v Speaker 2>I think it's talkpython.fm/agentic-ai.

00:35:48.980 --> 00:35:49.680
<v Speaker 2>So that one's fun.

00:35:50.010 --> 00:35:50.300
<v Speaker 2>What else?

00:35:50.540 --> 00:35:54.640
<v Speaker 2>I'm also going to be on talking with Hugo Bowen-Anderson.

00:35:56.040 --> 00:35:59.200
<v Speaker 2>He's running the Vanishing Gradients podcast.

00:35:59.500 --> 00:36:02.220
<v Speaker 2>He was on Talk Python a while ago.

00:36:02.870 --> 00:36:06.580
<v Speaker 2>And now I'm going to be on his show, Data Science Meets Agentic AI.

00:36:08.200 --> 00:36:10.840
<v Speaker 2>Sort of a follow-up for my Talk Python in production book,

00:36:11.400 --> 00:36:13.060
<v Speaker 2>plus this course I just talked about.

00:36:13.240 --> 00:36:15.580
<v Speaker 2>Those kind of ideas, we're going to spend some time riffing on that.

00:36:15.660 --> 00:36:18.140
<v Speaker 2>So that's tomorrow night US time.

00:36:18.480 --> 00:36:19.800
<v Speaker 2>So check that out.

00:36:21.780 --> 00:36:22.260
<v Speaker 2>All right.

00:36:24.940 --> 00:36:31.420
<v Speaker 2>Also, OpenAI introduced an AI browser wrapping Google Chrome called Atlas.

00:36:32.820 --> 00:36:33.580
<v Speaker 2>This is interesting.

00:36:33.840 --> 00:36:34.900
<v Speaker 2>I played with it.

00:36:35.460 --> 00:36:36.060
<v Speaker 2>I'm not convinced.

00:36:36.260 --> 00:36:37.140
<v Speaker 2>I mean, it's kind of fine.

00:36:38.460 --> 00:36:38.900
<v Speaker 2>I don't know.

00:36:38.940 --> 00:36:40.180
<v Speaker 2>I don't even know what to think about these things.

00:36:40.560 --> 00:36:41.440
<v Speaker 2>That's not why I included it.

00:36:41.680 --> 00:36:43.220
<v Speaker 2>Maybe it's sort of interesting as a side note.

00:36:43.260 --> 00:36:45.340
<v Speaker 2>But I linked to the Ars Technica article.

00:36:46.160 --> 00:36:47.580
<v Speaker 2>Holy smokes, you guys.

00:36:47.880 --> 00:36:48.660
<v Speaker 2>Check out the comments.

00:36:49.520 --> 00:36:50.820
<v Speaker 2>They are not loving it.

00:36:52.080 --> 00:36:54.120
<v Speaker 2>And it's not OpenAI's fault exactly.

00:36:54.800 --> 00:37:01.260
<v Speaker 2>It's just this idea of AI taking over everything is not,

00:37:02.300 --> 00:37:04.480
<v Speaker 2>at least if there's not everyone hating on it,

00:37:04.980 --> 00:37:07.320
<v Speaker 2>there's a very vocal group of people who don't love it.

00:37:09.200 --> 00:37:11.680
<v Speaker 2>And I just noticed that Kyle Orland wrote this.

00:37:11.720 --> 00:37:14.980
<v Speaker 2>And he actually wrote a whole book on Minesweeper, which props to him for that.

00:37:15.520 --> 00:37:16.400
<v Speaker 1>Oh, that's awesome.

00:37:17.040 --> 00:37:17.940
<v Speaker 1>I got to check that out.

00:37:18.300 --> 00:37:21.340
<v Speaker 2>I'm like, geez, it's been a long time to thought about Minesweeper.

00:37:21.760 --> 00:37:25.860
<v Speaker 2>Anyway, this is actually just a really interesting cultural touchpoint, I think.

00:37:25.900 --> 00:37:26.860
<v Speaker 2>So people can check that out.

00:37:27.800 --> 00:37:28.100
<v Speaker 2>All right.

00:37:28.480 --> 00:37:35.700
<v Speaker 2>James Abel, I think he was behind Pi Bay this year, but yeah, down in the San Francisco area,

00:37:36.260 --> 00:37:40.860
<v Speaker 2>said, hey, somehow we were talking about dunder name,

00:37:41.200 --> 00:37:42.840
<v Speaker 2>if dunder name equals dunder main.

00:37:43.260 --> 00:37:43.960
<v Speaker 2>I said, hey, I have a package.

00:37:44.120 --> 00:37:44.900
<v Speaker 2>It's real, real simple.

00:37:45.560 --> 00:37:48.800
<v Speaker 2>But instead of writing this, if you want to get people started,

00:37:48.940 --> 00:37:55.200
<v Speaker 2>you can just import my thing and say if is main as a simpler, saner way.

00:37:56.220 --> 00:37:59.820
<v Speaker 2>That maybe should be a built-in, don't you think, Python?

00:38:00.860 --> 00:38:01.060
<v Speaker 2>Yeah.

00:38:01.380 --> 00:38:03.600
<v Speaker 2>Or what I would rather see is if is main.

00:38:04.180 --> 00:38:05.280
<v Speaker 2>Here's what I would like to see,

00:38:05.440 --> 00:38:08.340
<v Speaker 2>but I don't know that Python has a mechanism for it.

00:38:08.960 --> 00:38:12.460
<v Speaker 2>I would like to say at the top something

00:38:12.540 --> 00:38:16.140
<v Speaker 3>to the effect of register this function as me.

00:38:16.760 --> 00:38:20.840
<v Speaker 2>And then when it's done parsing the file, then it runs it.

00:38:21.440 --> 00:38:23.640
<v Speaker 2>Because even this, which is super helpful,

00:38:24.960 --> 00:38:28.140
<v Speaker 2>if you accidentally put code below it, it's a problem.

00:38:28.180 --> 00:38:28.560
<v Speaker 2>You know what I mean?

00:38:28.600 --> 00:38:31.160
<v Speaker 2>I would like Python to enforce, we load the whole file,

00:38:31.380 --> 00:38:33.120
<v Speaker 2>and then it runs, a cool mechanism for that.

00:38:33.240 --> 00:38:34.140
<v Speaker 2>But still, this is pretty nice.

00:38:34.740 --> 00:38:39.300
<v Speaker 2>And at first I thought I was just like wrapping if name is main, right?

00:38:39.920 --> 00:38:43.620
<v Speaker 2>But in fact, it's like doing something a little bit different here.

00:38:44.840 --> 00:38:50.200
<v Speaker 2>It's using the inspect stack to go back and then look at the value.

00:38:50.880 --> 00:38:54.520
<v Speaker 2>Because obviously it itself is not main, so it can't do that test.

00:38:54.720 --> 00:38:55.680
<v Speaker 2>So I don't know, kind of cute.

00:38:57.160 --> 00:38:57.420
<v Speaker 2>Yeah.

00:38:58.340 --> 00:39:03.260
<v Speaker 2>If you don't want to have a dependency on this library,

00:39:03.400 --> 00:39:05.740
<v Speaker 2>you could take that line of code and put it somewhere

00:39:05.810 --> 00:39:07.260
<v Speaker 2>as a utility class in yours.

00:39:08.280 --> 00:39:08.720
<v Speaker 1>OK.

00:39:08.890 --> 00:39:10.280
<v Speaker 1>I'm just going to throw this out there.

00:39:10.340 --> 00:39:12.140
<v Speaker 1>If you want to do something really fast,

00:39:13.220 --> 00:39:14.380
<v Speaker 1>make sure you time this.

00:39:15.590 --> 00:39:17.540
<v Speaker 1>Because every time I've added inspect,

00:39:17.950 --> 00:39:19.420
<v Speaker 1>I love the inspect library.

00:39:19.640 --> 00:39:22.140
<v Speaker 1>But whenever I add it, it slows things down.

00:39:22.800 --> 00:39:23.240
<v Speaker 1>OK.

00:39:25.240 --> 00:39:26.120
<v Speaker 1>Just benchmark.

00:39:27.640 --> 00:39:28.140
<v Speaker 1>Benchmark it.

00:39:28.500 --> 00:39:28.720
<v Speaker 1>All right.

00:39:28.840 --> 00:39:29.340
<v Speaker 2>Benchmark it.

00:39:30.440 --> 00:39:30.840
<v Speaker 2>All right.

00:39:30.880 --> 00:39:31.720
<v Speaker 2>Inline.

00:39:31.820 --> 00:39:32.100
<v Speaker 2>All right.

00:39:33.080 --> 00:39:37.720
<v Speaker 2>If you're an IntelliJ person like PyCharm, wouldn't it be nice, Brian?

00:39:38.100 --> 00:39:41.340
<v Speaker 2>Wouldn't it be incredible if, while you're working,

00:39:41.760 --> 00:39:45.420
<v Speaker 2>a little pet could walk around in the bottom?

00:39:45.600 --> 00:39:48.900
<v Speaker 2>Anthony Shaw, I believe, is the one who added this to VS Code.

00:39:50.900 --> 00:39:53.220
<v Speaker 2>But PyCharm people can love animals too.

00:39:54.600 --> 00:39:57.340
<v Speaker 2>So you could install the pet into your PyCharm now.

00:39:57.980 --> 00:39:58.520
<v Speaker 1>Oh, cool.

00:39:59.780 --> 00:39:59.940
<v Speaker 2>Nice.

00:40:00.380 --> 00:40:01.420
<v Speaker 2>I don't know that I'm going to be doing it.

00:40:01.940 --> 00:40:03.040
<v Speaker 2>This is a last minute edition.

00:40:03.320 --> 00:40:06.800
<v Speaker 2>I just got a brand new M5 iPad.

00:40:07.140 --> 00:40:07.800
<v Speaker 2>How insane is that?

00:40:08.740 --> 00:40:10.720
<v Speaker 2>The last one I had, I got five years ago.

00:40:10.900 --> 00:40:11.760
<v Speaker 2>I'm like, eh, it's probably time.

00:40:12.180 --> 00:40:17.180
<v Speaker 2>I got a 13-inch iPad Pro, which is an insanely large iPad.

00:40:19.300 --> 00:40:25.600
<v Speaker 2>But I've started using it as the second monitor for my laptop if I'm traveling or if I'm at a coffee shop or something.

00:40:26.560 --> 00:40:27.980
<v Speaker 2>And that's a super cool experience.

00:40:28.180 --> 00:40:31.100
<v Speaker 2>Just put them side by side if they're on the same network or with a cable.

00:40:32.040 --> 00:40:38.380
<v Speaker 2>and they just do real, real low latency dual monitor stuff.

00:40:38.960 --> 00:40:39.460
<v Speaker 2>Yeah, it's pretty neat.

00:40:39.460 --> 00:40:40.520
<v Speaker 2>So just a little shout out to that.

00:40:41.300 --> 00:40:42.100
<v Speaker 1>That's incredible.

00:40:42.640 --> 00:40:45.460
<v Speaker 1>It's like having a new laptop, a second laptop,

00:40:45.720 --> 00:40:47.100
<v Speaker 1>for the price of a second laptop.

00:40:47.440 --> 00:40:48.060
<v Speaker 2>Yes, it is.

00:40:48.060 --> 00:40:48.800
<v Speaker 2>But here's the thing.

00:40:50.280 --> 00:40:51.240
<v Speaker 2>Yes, I agree.

00:40:51.440 --> 00:40:53.040
<v Speaker 2>But it's also a really nice reading device,

00:40:53.320 --> 00:40:54.520
<v Speaker 2>which I do a lot of reading and stuff.

00:40:54.800 --> 00:40:54.940
<v Speaker 2>Okay.

00:40:55.000 --> 00:40:59.280
<v Speaker 2>And I was going to get one of the cheaper ones.

00:41:00.060 --> 00:41:04.120
<v Speaker 2>One of the things that drives me, I have a MacBook Air as my main laptop computer,

00:41:04.220 --> 00:41:05.460
<v Speaker 2>and the same chip for my main computer.

00:41:05.480 --> 00:41:10.220
<v Speaker 2>I don't have a high-end computer, and it's plenty good for Docker stuff,

00:41:11.040 --> 00:41:12.460
<v Speaker 2>for programming stuff, all these things.

00:41:12.600 --> 00:41:14.480
<v Speaker 2>It's totally fine, but here's the thing.

00:41:14.940 --> 00:41:21.800
<v Speaker 2>The iPad Air, while great, its peak brightness is something like 500 or 600 nits.

00:41:22.940 --> 00:41:26.080
<v Speaker 2>And if you're working, like I like to work in my back outside area in the summer.

00:41:26.240 --> 00:41:27.680
<v Speaker 2>Like that's kind of not relevant right now.

00:41:27.800 --> 00:41:31.920
<v Speaker 2>But in general, sit outside, enjoy the weather, get out of the office.

00:41:33.400 --> 00:41:36.820
<v Speaker 2>And if it's at all bright through a window or somewhere, it's really a pain.

00:41:37.920 --> 00:41:42.380
<v Speaker 2>This thing is like the best screen you can buy on a Mac, period, whatever.

00:41:43.440 --> 00:41:44.420
<v Speaker 2>And it's 1,000 nits.

00:41:44.560 --> 00:41:49.400
<v Speaker 2>So you could push the computer to the side and just put the laptop in front of you and type on it.

00:41:49.520 --> 00:41:50.180
<v Speaker 2>It's really nice.

00:41:51.500 --> 00:41:52.700
<v Speaker 2>Anyway, that's the main reason.

00:41:52.860 --> 00:41:55.080
<v Speaker 2>I want a brighter screen without having a MacBook Pro.

00:41:55.940 --> 00:42:00.700
<v Speaker 1>I feel so bad for you having to deal with, like, working outside in such bright light.

00:42:01.260 --> 00:42:01.900
<v Speaker 2>It's really horrible.

00:42:02.030 --> 00:42:02.360
<v Speaker 2>I know.

00:42:02.490 --> 00:42:02.960
<v Speaker 2>You should really.

00:42:04.340 --> 00:42:04.780
<v Speaker 2>It's hard.

00:42:05.020 --> 00:42:05.640
<v Speaker 2>It's hard to be me.

00:42:06.720 --> 00:42:06.960
<v Speaker 1>All right.

00:42:08.800 --> 00:42:09.640
<v Speaker 2>Carrying on with the jokes.

00:42:09.840 --> 00:42:10.140
<v Speaker 2>No, go ahead.

00:42:10.860 --> 00:42:17.180
<v Speaker 1>Well, before we get to the joke, I want to, I guess, highlight, going back to my announcement of possibly a book.

00:42:20.640 --> 00:42:23.360
<v Speaker 1>Jepinal7 says, a TDD book by Brian.

00:42:23.740 --> 00:42:24.060
<v Speaker 1>Can't wait.

00:42:24.420 --> 00:42:25.220
<v Speaker 1>Also, talk Python.

00:42:25.380 --> 00:42:27.280
<v Speaker 1>training courses are great. Kudos.

00:42:27.800 --> 00:42:31.480
<v Speaker 1>Yeah. I'm looking forward to getting that book out

00:42:31.480 --> 00:42:33.060
<v Speaker 1>and I'm looking forward to that AI course.

00:42:34.200 --> 00:42:35.320
<v Speaker 2>Yeah. Thanks. Yeah. Same.

00:42:36.540 --> 00:42:39.240
<v Speaker 2>All right. I'm not looking forward to surgery.

00:42:39.480 --> 00:42:41.640
<v Speaker 2>I'll tell you what. And it's getting to be weird, Brian.

00:42:42.110 --> 00:42:46.080
<v Speaker 2>I mean, like doctors using AI and stuff.

00:42:46.270 --> 00:42:48.600
<v Speaker 2>Actually, we probably are going to get better diagnoses

00:42:48.940 --> 00:42:50.100
<v Speaker 2>as for certain things with that.

00:42:50.290 --> 00:42:52.700
<v Speaker 2>But here's a surgeon situation.

00:42:53.960 --> 00:42:56.760
<v Speaker 2>There's a person who just, they're in post-op, okay?

00:42:56.960 --> 00:43:00.920
<v Speaker 2>They're laying there like, oh man, a little woozy from the anesthesia coming out of it.

00:43:01.280 --> 00:43:07.100
<v Speaker 2>And the doctor, which is a robot with a ChatGPT-like, I don't think it's, is it the same?

00:43:07.160 --> 00:43:07.420
<v Speaker 2>I don't know.

00:43:07.920 --> 00:43:10.380
<v Speaker 2>An AI logo for a robot face.

00:43:11.140 --> 00:43:17.040
<v Speaker 2>And the patient says, but why is the scar on the left if the appendix is on the right?

00:43:17.680 --> 00:43:19.700
<v Speaker 2>The AI surgeon says, you're absolutely right.

00:43:19.740 --> 00:43:20.880
<v Speaker 2>Let me try that one more time.

00:43:25.900 --> 00:43:32.100
<v Speaker 2>please don't try it one more time it's so bad it's pretty bad yeah but it's pretty funny

00:43:33.720 --> 00:43:38.400
<v Speaker 1>that actually drives me nuts when when i'm like this doesn't sound right is this you know

00:43:39.530 --> 00:43:46.279
<v Speaker 2>the user is pointing out i've made a mistake oh you're right yeah oh well it's yeah

00:43:48.700 --> 00:43:52.600
<v Speaker 2>So, yeah, just get a second opinion.

00:43:53.680 --> 00:43:57.020
<v Speaker 2>So if opening is going to operate on you,

00:43:57.840 --> 00:44:00.280
<v Speaker 2>have Anthropic be the backup, I guess, is the moral of the story.

00:44:00.290 --> 00:44:00.740
<v Speaker 2>I don't know.

00:44:01.840 --> 00:44:03.360
<v Speaker 1>I don't think that's the moral of the story.

00:44:03.360 --> 00:44:04.080
<v Speaker 1>You don't think so?

00:44:04.600 --> 00:44:04.860
<v Speaker 1>Okay.

00:44:05.320 --> 00:44:06.620
<v Speaker 1>Maybe somebody trained in medicine.

00:44:08.380 --> 00:44:09.140
<v Speaker 2>Trained on medicine?

00:44:09.440 --> 00:44:10.100
<v Speaker 2>I'm sure they are.

00:44:12.100 --> 00:44:13.680
<v Speaker 1>Trained with heavy medication.

00:44:14.220 --> 00:44:14.640
<v Speaker 3>Yeah, exactly.

00:44:15.640 --> 00:44:16.540
<v Speaker 3>All right, cool.

00:44:16.720 --> 00:44:18.660
<v Speaker 3>Well, fun as always.

00:44:19.420 --> 00:44:22.760
<v Speaker 1>Well, definitely fun as always, and we'll see everybody next week.

00:44:23.580 --> 00:44:24.140
<v Speaker 3>Yep, see ya.

