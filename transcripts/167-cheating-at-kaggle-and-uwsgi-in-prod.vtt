
00:00:00.000 --> 00:00:05.060
Hello and welcome to Python Bytes, where we deliver Python news and headlines directly to your earbuds.


00:00:05.060 --> 00:00:10.660
This is episode 167, recorded January 29th, 2020.


00:00:10.660 --> 00:00:13.860
I'm Michael Kennedy, and Brian Okken is away.


00:00:13.860 --> 00:00:16.860
We miss you, Brian, but I have a very special guest to join me,


00:00:16.860 --> 00:00:18.360
Vicki Boykus. Welcome, Vicki.


00:00:18.360 --> 00:00:19.160
Thanks for having me.


00:00:19.160 --> 00:00:20.600
Yeah, it's great to have you here.


00:00:20.600 --> 00:00:23.600
I'm excited to get your take on this week's Python news.


00:00:23.600 --> 00:00:28.660
I know you found some really interesting and controversial ones that we're going to jump into, and that will be great.


00:00:28.660 --> 00:00:32.060
Also great is Datadog, they're sponsoring this episode.


00:00:32.060 --> 00:00:35.060
So check them out at pythonbytes.fm/datadog.


00:00:35.060 --> 00:00:36.820
I'll tell you more about them later.


00:00:36.820 --> 00:00:41.860
Vicki, let me kick us off with command line interface libraries,


00:00:41.860 --> 00:00:43.260
for lack of a better word.


00:00:43.260 --> 00:00:47.740
So back on episode 164, so three episodes ago,


00:00:47.740 --> 00:00:49.540
I talked about this thing called typer.


00:00:49.540 --> 00:00:50.700
Have you heard of typer?


00:00:50.700 --> 00:00:51.900
T-Y-P-E-R?


00:00:51.900 --> 00:00:53.580
I have not, but I've heard of click.


00:00:53.580 --> 00:00:56.300
So I'm curious to see how this differs from that even.


00:00:56.300 --> 00:01:00.300
Yeah, so this is sort of a competitor to click.


00:01:00.300 --> 00:01:06.500
Typer is super cool because what it does is it uses native Python concepts


00:01:06.500 --> 00:01:11.200
to build out your CLI rather than attributes where you describe everything.


00:01:11.200 --> 00:01:13.900
So, for example, you can have a function


00:01:13.900 --> 00:01:19.900
and you just say this function takes a name colon str to give it a type or an int or whatever.


00:01:19.900 --> 00:01:23.400
And then Typer can automatically use the type


00:01:23.400 --> 00:01:29.320
and the name of the parameters and stuff to generate like your help and the inbound arguments and so on.


00:01:29.320 --> 00:01:30.320
So that's pretty cool, right?


00:01:30.320 --> 00:01:34.020
Yeah, seems like a great excuse to start using type annotations if you haven't yet.


00:01:34.020 --> 00:01:35.700
Yeah, exactly. Very, very nice.


00:01:35.700 --> 00:01:38.420
That it leverages type annotations, hence the name typer, right?


00:01:38.420 --> 00:01:43.060
So our listeners are great. They always send in stuff that we haven't heard about or


00:01:43.060 --> 00:01:46.460
you know, like, I can't believe you didn't talk about this other thing.


00:01:46.460 --> 00:01:52.060
So Marcelo sent in a message and says, hey, you should talk about CLIZ, C-L-I-Z-E,


00:01:52.060 --> 00:01:55.400
which turns functions into command line interface.


00:01:55.400 --> 00:01:57.900
So Klies is really cool,


00:01:57.900 --> 00:02:02.500
and it's very similar in regard to how it works to Typer.


00:02:02.500 --> 00:02:05.200
So what you do is you create functions,


00:02:05.200 --> 00:02:07.700
you give them variables, you don't have to use the types


00:02:07.700 --> 00:02:09.460
in the sense that Typer does,


00:02:09.460 --> 00:02:11.700
but you have positional arguments,


00:02:11.700 --> 00:02:13.700
and you have keyword only arguments.


00:02:13.700 --> 00:02:15.900
And you know, Python has that syntax that


00:02:15.900 --> 00:02:20.200
very few people use, but it's cool if you want to enforce it,


00:02:20.200 --> 00:02:24.840
where you can say, here are some parameters to a function, comma, star, comma,


00:02:24.840 --> 00:02:28.760
here are some more, and the stuff after the star has to be addressed


00:02:28.760 --> 00:02:30.280
as a keyword argument, right?


00:02:30.280 --> 00:02:33.560
Yeah, so it leverages that kind of stuff. So you can say,


00:02:33.560 --> 00:02:37.080
like their example says, here's a hello world function, and it takes a name,


00:02:37.080 --> 00:02:41.320
which has a default of none, and then star, comma, no capitalize


00:02:41.320 --> 00:02:43.480
is false, and it gives it a default value.


00:02:43.480 --> 00:02:46.920
So all you got to do to run it is basically


00:02:46.920 --> 00:02:49.800
import clies.run and call run on your function.


00:02:49.800 --> 00:02:53.240
And then what it does is it verifies those arguments


00:02:53.240 --> 00:02:55.720
about whether or not they're required, and then


00:02:55.720 --> 00:02:58.600
it'll convert the keyword arguments to like,


00:02:58.600 --> 00:03:01.160
dash dash, this or that. So like,


00:03:01.160 --> 00:03:03.240
--no capitalize


00:03:03.240 --> 00:03:05.720
will pass true to no capitalize.


00:03:05.720 --> 00:03:09.320
If you admit it, it'll pass, you know, whatever the default is, I guess, so false.


00:03:09.320 --> 00:03:11.880
So there's like positional ones where you don't say the name,


00:03:11.880 --> 00:03:16.280
but then also this cool way of adding on these --capitalize and so on.


00:03:16.280 --> 00:03:18.280
So it seems like a really cool


00:03:18.280 --> 00:03:21.560
and pretty extensive library for building command line interfaces.


00:03:21.560 --> 00:03:24.600
Yeah, so this seems like it would be good if you have a lot of parameters


00:03:24.600 --> 00:03:27.600
that you have to pass in. I'm thinking specifically of


00:03:27.600 --> 00:03:30.600
some of the work that you would do in the cloud, like in the AWS command line.


00:03:30.600 --> 00:03:31.240
Yeah, yeah.


00:03:31.240 --> 00:03:31.840
Or similar?


00:03:31.840 --> 00:03:32.840
Yeah, for sure.


00:03:32.840 --> 00:03:35.600
Another thing that's cool is it will take your doc strings


00:03:35.600 --> 00:03:37.720
and use those as help messages.


00:03:37.720 --> 00:03:38.560
Oh, that's neat.


00:03:38.560 --> 00:03:41.720
Yeah, so you know, in like some editors, you can type triple quote enter


00:03:41.720 --> 00:03:43.120
and it'll generate,


00:03:43.120 --> 00:03:45.760
you know, here's the summary of the method,


00:03:45.760 --> 00:03:49.320
And then here's the arguments, and you can put this, or you can just write them out, of course.


00:03:49.320 --> 00:03:53.440
And then here's the descriptions about each parameter.


00:03:53.440 --> 00:03:57.480
Those become help messages about each command in there.


00:03:57.480 --> 00:04:01.240
So it's really nice, and I like how it uses just pure Python,


00:04:01.240 --> 00:04:03.560
sort of similar to Typer in that regard that


00:04:03.560 --> 00:04:07.200
you don't put like three or four levels of decorators on top of things


00:04:07.200 --> 00:04:09.200
and then reference other parts of that.


00:04:09.200 --> 00:04:11.760
You just say, here's some Python code.


00:04:11.760 --> 00:04:14.080
I want to treat it as a command line interface,


00:04:14.080 --> 00:04:16.080
- That is pretty cool. - Yeah.


00:04:16.080 --> 00:04:20.080
So there's now a lot of choices if you want to do command line interfaces.


00:04:20.080 --> 00:04:25.080
Yeah, yeah. Definitely. And Click is good, and it's very popular in arg pars as well.


00:04:25.080 --> 00:04:30.080
But I'm kind of a fan of these pure Python ones that don't require me to go do a whole bunch of extra stuff.


00:04:30.080 --> 00:04:32.080
So, yeah, definitely loving that.


00:04:32.080 --> 00:04:36.080
You know what? I bet that Kaggle's not loving what you're talking about next.


00:04:36.080 --> 00:04:39.080
- Before we get into this. - Well, I think they might be, but...


00:04:39.080 --> 00:04:43.080
Yeah, we'll see. Okay. Tell us about Kaggle and what the big news here is.


00:04:43.080 --> 00:04:43.920
the big news here is.


00:04:43.920 --> 00:04:47.460
- Yeah, so there was a dust up at Kaggle a couple weeks ago.


00:04:47.460 --> 00:04:49.280
So just as a little bit of background,


00:04:49.280 --> 00:04:51.700
Kaggle's a platform that's now owned by Google


00:04:51.700 --> 00:04:54.460
that allows data scientists to find data sets,


00:04:54.460 --> 00:04:57.000
to learn data science, and most importantly,


00:04:57.000 --> 00:04:59.420
it's probably known for letting people participate


00:04:59.420 --> 00:05:01.400
in machine learning competitions.


00:05:01.400 --> 00:05:04.040
That's kind of how it gained its popularity and notoriety.


00:05:04.040 --> 00:05:04.880
- Yeah, that's how I know it.


00:05:04.880 --> 00:05:07.220
- Yep, and so people can sharpen their data science


00:05:07.220 --> 00:05:09.020
and modeling skills on it.


00:05:09.020 --> 00:05:11.260
So they recently, I wanna say last fall,


00:05:11.260 --> 00:05:16.500
hosted a competition that was about analyzing pet shelter data.


00:05:16.500 --> 00:05:19.360
And this resulted in enormous controversy.


00:05:19.360 --> 00:05:25.440
So what happened is there's this website that's called petfinder.my that helps people find


00:05:25.440 --> 00:05:28.760
pets to rescue in Malaysia from shelters.


00:05:28.760 --> 00:05:33.720
And in 2019, they announced a collaboration with Kaggle to create a machine learning predictor


00:05:33.720 --> 00:05:39.960
algorithm which pets would be most likely to be adopted based on the metadata descriptions


00:05:39.960 --> 00:05:40.960
on the site.


00:05:40.960 --> 00:05:46.360
go to petfinder.my, you'd see that they'll have a picture of the pet and then a description,


00:05:46.360 --> 00:05:49.320
how old they are, and some other attributes about them.


00:05:49.320 --> 00:05:53.280
- Right, were they vaccinated or things like that, right?


00:05:53.280 --> 00:05:57.560
Sort of, you might think, well, if they're vaccinated or they're neutered or spayed,


00:05:57.560 --> 00:06:00.520
they may be more likely to be adopted, but you don't necessarily know, right?


00:06:00.520 --> 00:06:04.040
So that was kind of some, like, what are the important factors was this whole competition,


00:06:04.040 --> 00:06:05.040
right?


00:06:05.040 --> 00:06:09.440
- Yeah, the goal was to help the shelters write better descriptions so that pets would


00:06:09.440 --> 00:06:14.800
be adopted more quickly. So after several months, they held the competition for several


00:06:14.800 --> 00:06:19.640
months and there was a contestant that won and he was previously what was called a Kaggle


00:06:19.640 --> 00:06:25.460
Grandmaster. So he'd won a lot of different stuff on Kaggle before and he won $10,000


00:06:25.460 --> 00:06:30.720
in prize money. But then what happened is they started to validate all of his data.


00:06:30.720 --> 00:06:35.320
Because when you do a Kaggle competition, you then submit all of your data and all of


00:06:35.320 --> 00:06:38.120
your results and your notebooks and your code.


00:06:38.120 --> 00:06:40.720
Like how you trained your models and stuff like that, right?


00:06:40.720 --> 00:06:42.240
Yeah, all of that stuff.


00:06:42.240 --> 00:06:47.120
And then what happened was Petfinder wanted to put this model into production.


00:06:47.120 --> 00:06:52.000
So you initially have something like a Jupyter or a Colab notebook in this case.


00:06:52.000 --> 00:06:56.880
And the idea is that now you want to be able to integrate it into the Petfinder website.


00:06:56.880 --> 00:07:03.120
So they can actually use these predictors to fine tune how they post the content.


00:07:03.120 --> 00:07:09.300
And so when a volunteer who was Benjamin Minnikhofer offered to put the algorithm into production


00:07:09.300 --> 00:07:13.280
and he started looking at it, he found that there was a huge discrepancy between the first


00:07:13.280 --> 00:07:16.840
and second place entrance in the contest.


00:07:16.840 --> 00:07:21.300
And so what happened was, so a little to get more into the technical aspect, the data they


00:07:21.300 --> 00:07:25.460
gave to the contestants asked them to predict the speed at which a pet would be adopted


00:07:25.460 --> 00:07:29.040
from one to five and included some of the features you talked about like animal, breed


00:07:29.040 --> 00:07:31.320
coloration, all that stuff.


00:07:31.320 --> 00:07:37.360
The initial training set had 15,000 animals and then after a couple months the contestants


00:07:37.360 --> 00:07:42.240
were given 4,000 animals that had not been seen before as a test of how accurate they


00:07:42.240 --> 00:07:43.240
were.


00:07:43.240 --> 00:07:49.240
So what the winner did was he actually scraped basically most of the website so that he got


00:07:49.240 --> 00:07:53.920
that 4,000 set, the validation set also.


00:07:53.920 --> 00:07:57.320
And he had the validation set in his notebook.


00:07:57.320 --> 00:08:04.560
So basically what he did was he used the MD5 library to create a hash for each unique pet


00:08:04.560 --> 00:08:09.320
and then he looked up the adoption score for each of those pets, basically when they were


00:08:09.320 --> 00:08:12.260
adopted from that external data set.


00:08:12.260 --> 00:08:16.920
And there were about 3,500 that had overlaps with the validation set.


00:08:16.920 --> 00:08:21.860
And then he did a column manipulation in pandas to get at the hidden prediction variable for


00:08:21.860 --> 00:08:23.080
every 10th pet.


00:08:23.080 --> 00:08:26.040
Not every single pet, but every 10th pet, so it didn't look too obvious.


00:08:26.040 --> 00:08:30.840
So he gave himself like a 10% head start or advantage or something like that.


00:08:30.840 --> 00:08:31.840
Exactly.


00:08:31.840 --> 00:08:36.120
And he replaced the prediction that should have been generated by the algorithm with


00:08:36.120 --> 00:08:38.000
the actual value.


00:08:38.000 --> 00:08:42.960
And then he did a dictionary lookup between the initial MD5 hash and the value of the


00:08:42.960 --> 00:08:43.960
hash.


00:08:43.960 --> 00:08:48.680
And this was all obfuscated in a separate function that happened in his data.


00:08:48.680 --> 00:08:53.120
And so they must have been looking at this going, "What does the MD5 hash of the pet


00:08:53.120 --> 00:08:55.640
attributes have to do with anything.


00:08:55.640 --> 00:08:56.320
You know what I mean?


00:08:56.320 --> 00:08:56.520
Right?


00:08:56.520 --> 00:08:58.960
It's the, the hashes are meant to obscure stuff.


00:08:58.960 --> 00:08:59.280
Right?


00:08:59.280 --> 00:08:59.760
Right.


00:08:59.760 --> 00:09:00.080
Yeah.


00:09:00.080 --> 00:09:01.400
So what was the fallout?


00:09:01.400 --> 00:09:04.640
So the fallout was this guy worked at h2o.ai.


00:09:04.640 --> 00:09:06.700
And so he was fired from there.


00:09:06.700 --> 00:09:11.720
And Kaggle also issued an apology where they explained exactly what happened.


00:09:11.720 --> 00:09:16.240
And they expressed the hope that this didn't mean that every contest going


00:09:16.240 --> 00:09:20.400
forward would be viewed with suspicion for more openness and for collaboration


00:09:20.400 --> 00:09:21.080
going forward.


00:09:21.080 --> 00:09:21.600
Wow.


00:09:21.680 --> 00:09:22.880
It was an amazing catch.


00:09:22.880 --> 00:09:24.280
Yeah, that's such a good catch.


00:09:24.280 --> 00:09:26.680
I'm so, so glad that Benjamin did that.


00:09:26.680 --> 00:09:28.880
I've got the whole deal here.


00:09:28.880 --> 00:09:32.120
Now, did Kaggle actually end up paying him the $10,000


00:09:32.120 --> 00:09:33.320
before they caught it?


00:09:33.320 --> 00:09:35.880
Is there some sort of waiting period?


00:09:35.880 --> 00:09:37.840
Unfortunately, I think the money had already


00:09:37.840 --> 00:09:40.480
been dispersed by that point.


00:09:40.480 --> 00:09:43.360
I can easily see something, well,


00:09:43.360 --> 00:09:48.680
the prize money will be sent out after a very deep--


00:09:48.680 --> 00:09:51.400
it may change the timing of that for sure in the future.


00:09:51.400 --> 00:09:57.340
Who knows but wow, that's crazy. Do you know why he was fired? I mean, they're just like we don't want


00:09:57.340 --> 00:10:02.760
You say I mean h2o AI they're kind of a will help you with your AI


00:10:02.760 --> 00:10:10.700
Story, so I guess you know, they're probably just like we don't want any of the negativity of that on our product


00:10:10.700 --> 00:10:16.140
Yeah, I think that's essentially it and it was a pretty big competition in the data science community


00:10:16.140 --> 00:10:19.800
And I think also once they'd started to look into it in other places


00:10:20.460 --> 00:10:25.460
Previously, he talked about just basically scraping data to gain competitions as well.


00:10:25.460 --> 00:10:27.860
So all of that stuff started to come out as well.


00:10:27.860 --> 00:10:29.560
I think they wanted to distance themselves.


00:10:29.560 --> 00:10:31.120
Yeah, I can imagine.


00:10:31.120 --> 00:10:33.060
Yikes. Okay, well, thank you for sharing that.


00:10:33.060 --> 00:10:36.960
Now, before we get to the next one, let me tell you about this week's sponsor, Datadog.


00:10:36.960 --> 00:10:41.860
They're a cloud-scale monitoring platform that unifies metrics, logs, and traces.


00:10:41.860 --> 00:10:44.820
Monitor your Python applications in real time,


00:10:44.820 --> 00:10:46.920
find bottlenecks with detailed flame graphs,


00:10:46.920 --> 00:10:49.720
trace requests as they travel across service boundaries,


00:10:49.800 --> 00:10:53.080
and their tracing client, auto instruments,


00:10:53.080 --> 00:10:56.320
popular frameworks like Django, AsyncIO, Flask,


00:10:56.320 --> 00:10:58.240
so you can quickly get started monitoring the health


00:10:58.240 --> 00:11:00.160
and performance of your Python apps.


00:11:00.160 --> 00:11:02.520
Do that with a 14-day free trial,


00:11:02.520 --> 00:11:05.520
and Datadog will send you a complimentary t-shirt,


00:11:05.520 --> 00:11:06.960
cool little Datadog t-shirt.


00:11:06.960 --> 00:11:10.280
So check them out at pythonbytes.fm/datadog.


00:11:10.280 --> 00:11:12.280
This next one kind of hits home for me


00:11:12.280 --> 00:11:15.520
because I have a ton of services and a lot of servers


00:11:15.520 --> 00:11:18.400
and websites and all these things working together,


00:11:18.400 --> 00:11:21.600
running on microWSGI, U-W-S-G-I.


00:11:21.600 --> 00:11:24.800
And I've had it running for quite a few years.


00:11:24.800 --> 00:11:26.200
It's got a lot of traffic.


00:11:26.200 --> 00:11:28.400
You know, we do like, I don't know,


00:11:28.400 --> 00:11:32.000
14 terabytes of traffic a month or maybe even more than that.


00:11:32.000 --> 00:11:35.700
So quite a bit of traffic going around these services and whatnot.


00:11:35.700 --> 00:11:37.700
So it's been working fine.


00:11:37.700 --> 00:11:41.000
But I ran across this article by the engineers at Bloomberg.


00:11:41.000 --> 00:11:43.200
So they talked about this thing called


00:11:43.200 --> 00:11:46.300
configuring microWSGI for production deployment.


00:11:46.800 --> 00:11:49.800
And I actually learned a lot from this article.


00:11:49.800 --> 00:11:52.300
So I don't feel like I was doing too many things wrong,


00:11:52.300 --> 00:11:55.500
but there was a couple of things I'm like, "Oh, yeah, I should probably do that."


00:11:55.500 --> 00:11:57.860
And other stuff just that is really nice.


00:11:57.860 --> 00:12:00.440
So I just want to run you through a couple of things that I learned.


00:12:00.440 --> 00:12:04.000
And if you want to hear more about how we're using MicroWizKey,


00:12:04.000 --> 00:12:07.500
you can check that out on Talk Python 215.


00:12:07.500 --> 00:12:12.560
Dan Bader and I swap stories about how we're running our various things,


00:12:12.560 --> 00:12:16.360
you know, Talk Python Training and realpython.com and whatnot.


00:12:16.360 --> 00:12:22.360
So this is guidance from Bloomberg's engineering structured products application group.


00:12:22.360 --> 00:12:23.860
Oh, that's quite the title.


00:12:23.860 --> 00:12:29.560
And they decided to use microWSGI because it's really, you know, good for performance, easy to work with.


00:12:29.560 --> 00:12:34.860
However, they said, microWSGI is, as it's maturing,


00:12:34.860 --> 00:12:40.560
some of the defaults that made sense when it was new, like in 2008, don't make sense anymore.


00:12:40.560 --> 00:12:46.160
The reason is partly just because the way people use these sites is different


00:12:46.160 --> 00:12:48.160
or these servers is different, for example,


00:12:48.160 --> 00:12:50.160
doing proxies up in front of


00:12:50.160 --> 00:12:52.160
microWSGI with say, Nginx,


00:12:52.160 --> 00:12:54.160
that used to not be so popular.


00:12:54.160 --> 00:12:56.160
So they made these defaults


00:12:56.160 --> 00:12:58.160
built into the system that maybe don't


00:12:58.160 --> 00:13:00.160
make sense anymore. And so what they did is


00:13:00.160 --> 00:13:02.160
we're going to go through and they said,


00:13:02.160 --> 00:13:04.160
we're going to go through and talk about all the things that we're


00:13:04.160 --> 00:13:06.160
going to override the defaults for and why


00:13:06.160 --> 00:13:08.160
unbit the developer microWSGI


00:13:08.160 --> 00:13:10.160
is going to fix all these bad


00:13:10.160 --> 00:13:12.160
defaults in the 2.1


00:13:12.160 --> 00:13:14.160
release. But right now it's 2.0


00:13:14.160 --> 00:13:20.760
as of this recording. So you're going to have to just, you know, hang in there or apply some of these changes.


00:13:20.760 --> 00:13:23.000
Now I do want to point out one thing.


00:13:23.000 --> 00:13:25.720
When I switched on a lot of these, I did them one at a time.


00:13:25.720 --> 00:13:31.120
And the way you get it to reload its config is you say relaunch the process, restart the process


00:13:31.120 --> 00:13:33.520
with system CTL, like a


00:13:33.520 --> 00:13:35.680
daemon management thing from Linux.


00:13:35.680 --> 00:13:40.680
And one of their recommendations is to use this flag die on term,


00:13:40.680 --> 00:13:44.680
which is for it to die on a different signal that it receives.


00:13:44.680 --> 00:13:47.040
And for whatever reason, maybe I'm doing it wrong,


00:13:47.040 --> 00:13:49.680
but whenever I turn that on, it would just lock up,


00:13:49.680 --> 00:13:52.240
and it would take about two minutes to restart the server,


00:13:52.240 --> 00:13:55.200
because it would just hang until it eventually timed out,


00:13:55.200 --> 00:13:56.240
it was like forcefully killed.


00:13:56.240 --> 00:13:58.080
So that seems bad, so I'm not using that.


00:13:58.080 --> 00:14:02.160
But I'll go quickly over the settings that I use that I thought were cool here.


00:14:02.160 --> 00:14:05.360
So you've got these complicated config files.


00:14:05.360 --> 00:14:07.360
If you want to make sure everything's validated,


00:14:07.360 --> 00:14:08.960
you can say strict equals true.


00:14:08.960 --> 00:14:12.840
that's cool, that will verify that everything that's typed in the file is accurate,


00:14:12.840 --> 00:14:15.460
and is valid, because it's kind of forgiven at the moment.


00:14:15.460 --> 00:14:20.800
Master is true is a good setting because this allows it to create worker processes


00:14:20.800 --> 00:14:23.300
and recycle them based on number of requests and so on.


00:14:23.300 --> 00:14:26.500
Something that's interesting, I didn't even realize you could do,


00:14:26.500 --> 00:14:29.900
maybe tell me if you knew this was possible in Python apps,


00:14:29.900 --> 00:14:31.900
you can disable the gill,


00:14:31.900 --> 00:14:35.200
the global interpreter lock, you can say, you know what, for this Python interpreter,


00:14:35.200 --> 00:14:36.200
let's not have a gill.


00:14:36.200 --> 00:14:37.200
Wow, how does that work?


00:14:37.200 --> 00:14:42.200
Yeah, well, it's I mean, people talk about having no GIL is like, Oh, you can do all this cool concurrency and whatnot.


00:14:42.200 --> 00:14:46.800
But what it really means is, you're basically guaranteeing you can only have one thread.


00:14:46.800 --> 00:14:53.400
So if you try to launch a background job on a micro WSGI server, and you don't pass enable thread, that's true.


00:14:53.400 --> 00:14:54.800
It's just going to not run.


00:14:54.800 --> 00:14:57.200
And because there's no gill, and there's no way to start it.


00:14:57.200 --> 00:15:03.000
So that's, that's something you want to have on vacuum equals true, this one I had off and I turned it on.


00:15:03.000 --> 00:15:05.400
Apparently, it cleans up like temporary files and so on.


00:15:05.400 --> 00:15:08.280
also a single interpreter. It used to be that


00:15:08.280 --> 00:15:11.720
microWSGI was more of an app server that might have different versions of Python


00:15:11.720 --> 00:15:14.680
and maybe Ruby as well. And this will just say, no, no, it's just


00:15:14.680 --> 00:15:18.840
the one version. A couple other ones, you can specify the name that shows up in


00:15:18.840 --> 00:15:23.160
like top or glances. So it'll say like, you can give it, say,


00:15:23.160 --> 00:15:25.640
your website name, and it'll say things like,


00:15:25.640 --> 00:15:30.040
that thing worker process one, or that thing master process, or whatnot.


00:15:30.040 --> 00:15:34.280
And so there's just a bunch of cool things in here with nice descriptions of


00:15:34.280 --> 00:15:35.720
why you want these features.


00:15:35.720 --> 00:15:38.120
So if you are out there and you're running microWSGI,


00:15:38.120 --> 00:15:40.360
give this a quick scan, it's really cool.


00:15:40.360 --> 00:15:43.480
Now, this next one also is pretty neat.


00:15:43.480 --> 00:15:46.760
So this one comes from the people who did spaCy, right?


00:15:46.760 --> 00:15:47.720
What do they got going on?


00:15:47.720 --> 00:15:48.560
- Yep, that's right.


00:15:48.560 --> 00:15:51.360
So this was just released a couple days ago,


00:15:51.360 --> 00:15:52.880
and it's called Think,


00:15:52.880 --> 00:15:56.240
and they bill it as a functional take on deep learning.


00:15:56.240 --> 00:15:59.800
And so basically, if you're familiar with deep learning,


00:15:59.800 --> 00:16:03.200
there's kind of two big competing frameworks right now,


00:16:03.200 --> 00:16:07.280
TensorFlow and PyTorch and MXNet is also in there.


00:16:07.280 --> 00:16:10.160
So the idea of this library is that it abstracts away


00:16:10.160 --> 00:16:12.320
some of the boilerplate that you have to write


00:16:12.320 --> 00:16:14.360
for both TensorFlow and PyTorch.


00:16:14.360 --> 00:16:16.680
PyTorch has a little bit less TensorFlow


00:16:16.680 --> 00:16:19.320
with Paris on top also has a little bit less,


00:16:19.320 --> 00:16:22.600
but you end up writing a lot of the same kind of stuff


00:16:22.600 --> 00:16:26.720
and there's also some stuff that's obfuscated away from you,


00:16:26.720 --> 00:16:29.140
specifically some of the matrix operations


00:16:29.140 --> 00:16:30.600
that go on under the hood.


00:16:30.600 --> 00:16:38.520
And so what Think does is, so it already runs on Spacey, which is an NLP library under the covers.


00:16:38.520 --> 00:16:45.120
So what the team did was they surfaced it so that other people could use it more generically in their projects.


00:16:45.120 --> 00:16:52.520
And so it has that favorite thing that we love, it has type checking, which is particularly helpful for tensors.


00:16:52.520 --> 00:16:56.800
When you're trying to get stuff and you're not sure why it's not returning things.


00:16:56.800 --> 00:17:01.080
It has classes for PyTorch wrappers and for TensorFlow,


00:17:01.080 --> 00:17:03.880
and you can intermingle the two if you want to,


00:17:03.880 --> 00:17:06.000
if you have two libraries that bridge things.


00:17:06.000 --> 00:17:09.560
It has deep support for NumPy structures,


00:17:09.560 --> 00:17:12.680
which are the kind of the underlying structures for deep learning.


00:17:12.680 --> 00:17:14.440
It operates in batches,


00:17:14.440 --> 00:17:17.920
which is also a common feature of deep learning projects,


00:17:17.920 --> 00:17:22.200
so they process features and data in batches.


00:17:22.200 --> 00:17:23.880
And then it also,


00:17:23.880 --> 00:17:25.840
sometimes a problem that you have with deep learning is


00:17:25.840 --> 00:17:31.280
you're constantly tuning hyperparameters or the variables that you put into your model


00:17:31.280 --> 00:17:36.160
to figure out how long you're going to run it for, how many training epochs you're going to have,


00:17:36.160 --> 00:17:40.880
what size your images are going to be. Usually those are those clustered in the beginning of


00:17:40.880 --> 00:17:46.160
your file is kind of like a dump or a dictionary or whatever. It has a special structure to handle


00:17:46.160 --> 00:17:51.520
those as well. So it basically hopes to make it easier and more flexible to do deep learning,


00:17:51.520 --> 00:17:54.120
especially if you're working with two different libraries,


00:17:54.120 --> 00:17:57.920
and it offers a nice higher level abstraction on top of that.


00:17:57.920 --> 00:17:59.320
And the other cool thing is,


00:17:59.320 --> 00:18:02.620
is they have already released all the examples and code


00:18:02.620 --> 00:18:05.920
that are available in Jupyter Notebooks on their GitHub repo.


00:18:05.920 --> 00:18:08.220
So I'm definitely going to be taking a closer look at that.


00:18:08.220 --> 00:18:09.020
Yeah, that's really cool.


00:18:09.020 --> 00:18:10.820
They have all these nice examples there,


00:18:10.820 --> 00:18:13.820
and even buttons to open them in Colab,


00:18:13.820 --> 00:18:15.920
which is, yeah, that's pretty awesome.


00:18:15.920 --> 00:18:16.920
This looks great.


00:18:16.920 --> 00:18:21.560
And it looks like it's doing some work with FastAPI as well.


00:18:21.560 --> 00:18:25.280
I know they hired a person who's maintaining FastAPI, which is cool.


00:18:25.280 --> 00:18:27.280
Also their Prodigy project.


00:18:27.280 --> 00:18:30.760
So yeah, this looks like a really nice library that they put together.


00:18:30.760 --> 00:18:33.880
Cool, and Ines has been on the show before.


00:18:33.880 --> 00:18:37.360
Ines from Explosion AI here as a guest co-host as well.


00:18:37.360 --> 00:18:37.860
Super cool.


00:18:37.860 --> 00:18:38.360
That's awesome.


00:18:38.360 --> 00:18:40.360
And this next one I want to talk about.


00:18:40.360 --> 00:18:43.680
I'd love to get your opinion because you're more on the data science side of things, right?


00:18:43.680 --> 00:18:44.180
Yeah.


00:18:44.180 --> 00:18:46.520
So this next one I want to tell folks about.


00:18:46.520 --> 00:18:50.060
This is another one from listeners, you know,


00:18:50.060 --> 00:18:52.260
we talked about something that validates pandas,


00:18:52.260 --> 00:18:54.260
and they're like, oh, you should also check out this thing.


00:18:54.260 --> 00:18:56.260
So this comes from Jacob Deppin,


00:18:56.260 --> 00:18:58.260
thank you, Jacob, for sending this in.


00:18:58.260 --> 00:19:00.920
And so it's pandas-vet.


00:19:00.920 --> 00:19:04.460
And what it is, is a plugin for Flake 8


00:19:04.460 --> 00:19:06.460
that checks pandas code.


00:19:06.460 --> 00:19:10.060
And it's this opinionated take on how you should use pandas.


00:19:10.060 --> 00:19:13.260
They say one of the challenges is that


00:19:13.260 --> 00:19:17.020
If you go and search on Stack Overflow or other tutorials,


00:19:17.020 --> 00:19:18.700
or even maybe video courses,


00:19:18.700 --> 00:19:20.540
they might show you how to do something with pandas,


00:19:20.540 --> 00:19:24.540
but maybe that's a deprecated way of working with pandas


00:19:24.540 --> 00:19:27.900
or some sort of old API, and there's a better way.


00:19:27.900 --> 00:19:31.100
So the idea is to make pandas more friendly for newcomers


00:19:31.100 --> 00:19:33.900
by trying to focus on best practices and saying,


00:19:33.900 --> 00:19:35.820
don't do it that way, do it this way,


00:19:35.820 --> 00:19:38.540
you know, read CSV, it has so many parameters, what are you doing?


00:19:38.540 --> 00:19:40.540
Here's how you use it, things like that.


00:19:41.100 --> 00:19:44.440
So this is based on a talk,


00:19:44.440 --> 00:19:50.680
or this linter was created, the idea was sparked by a talk by Ania Kupczynska.


00:19:50.680 --> 00:19:55.440
Sorry, I'm sure I blew that name bad, but at PyCascades 2019 in Seattle,


00:19:55.440 --> 00:19:57.780
I want your code responsibly, so I'll link to that as well.


00:19:57.780 --> 00:19:59.780
So it's kind of cool to see the evolution like,


00:19:59.780 --> 00:20:03.580
Ania gave a talk at PyCascades and then


00:20:03.580 --> 00:20:08.500
this person's like, oh, this is awesome, I'm going to actually turn this into a Flake 8 plugin and so on.


00:20:08.500 --> 00:20:09.940
What are your thoughts on this? Do you like this idea?


00:20:09.940 --> 00:20:13.700
Yeah, I'm a huge fan of it. I think in general there's been kind of like this


00:20:13.700 --> 00:20:19.800
I want to say culture war about whether notebooks are good or bad and there was recently a paper released


00:20:19.800 --> 00:20:25.460
I want to say not a paper but a blog post a couple days ago about how you should never use notebooks


00:20:25.460 --> 00:20:32.420
This there was a talk by Joel Grus last year about what the all the things that notebooks are bad with


00:20:32.420 --> 00:20:36.240
I think they have their place and I think this is one of the ways you can have


00:20:36.460 --> 00:20:40.460
I want to say guardrails around them and help people do things.


00:20:40.460 --> 00:20:45.460
I like the very opinionated warning that they have here, which is that DF is a bad variable name.


00:20:45.460 --> 00:20:49.460
Be kinder to yourself, because that's always true. You always start with the default of DF,


00:20:49.460 --> 00:20:54.460
and then you end up with 34 or 35 of them. I joke about this on Twitter all the time.


00:20:54.460 --> 00:21:01.460
But it's true. So that's a good one. The LOC and the .ix and loc/iloc is always a point of confusion,


00:21:01.460 --> 00:21:02.860
So it's good that they have that.


00:21:02.860 --> 00:21:06.760
And then the pivot table one is preferred to pivot or unstack.


00:21:06.760 --> 00:21:10.760
So there's a lot of places, so pandas is fantastic, but there's a lot of these


00:21:10.760 --> 00:21:15.500
places where you have old APIs, you have new APIs, you have people who usually are


00:21:15.500 --> 00:21:20.000
both new to Python and programming at the same time coming in and using these.


00:21:20.000 --> 00:21:23.380
So this is a good set of guardrails to help write better code if you're


00:21:23.380 --> 00:21:24.700
writing it in a notebook.


00:21:24.700 --> 00:21:25.060
Oh yeah.


00:21:25.060 --> 00:21:25.820
That's super cool.


00:21:25.820 --> 00:21:29.340
Do you know, is there a way to make Flake 8 run in the notebook automatically?


00:21:29.340 --> 00:21:30.060
I don't know.


00:21:30.340 --> 00:21:31.180
- You probably can, yeah.


00:21:31.180 --> 00:21:32.020
- It probably wouldn't be--


00:21:32.020 --> 00:21:33.500
- Yeah, yeah, but I don't know.


00:21:33.500 --> 00:21:34.780
- Yeah, but I'm thinking,


00:21:34.780 --> 00:21:35.960
it's interesting that you ask that


00:21:35.960 --> 00:21:37.500
because that's not generally,


00:21:37.500 --> 00:21:39.580
that's something you would do with notebooks,


00:21:39.580 --> 00:21:41.620
but maybe this kind of stuff will push it


00:21:41.620 --> 00:21:44.460
in the direction of being more like


00:21:44.460 --> 00:21:46.340
what we consider quote unquote mainstream


00:21:46.340 --> 00:21:49.260
or just web dev or backend programming.


00:21:49.260 --> 00:21:50.100
- Yeah, yeah, cool.


00:21:50.100 --> 00:21:51.120
Well, I definitely think it's nice.


00:21:51.120 --> 00:21:53.540
If I were getting started with Pandas,


00:21:53.540 --> 00:21:54.380
give this a check.


00:21:54.380 --> 00:21:56.100
You also, if you're getting started with Pandas,


00:21:56.100 --> 00:21:58.860
you may also be getting started with NumPy, right?


00:21:58.860 --> 00:22:01.420
- Yep, so NumPy is the backbone


00:22:01.420 --> 00:22:03.560
of numerical computing in Python.


00:22:03.560 --> 00:22:06.320
So I talked about TensorFlow, PyTorch,


00:22:06.320 --> 00:22:08.100
machine learning in the previous stories.


00:22:08.100 --> 00:22:10.780
All of that kind of rests on the work


00:22:10.780 --> 00:22:13.840
and the data structures that NumPy created.


00:22:13.840 --> 00:22:16.060
So Pandas, Scikit-learn, TensorFlow, PyTorch,


00:22:16.060 --> 00:22:18.040
they all lean heavily, if not directly,


00:22:18.040 --> 00:22:19.220
depend on the core concepts,


00:22:19.220 --> 00:22:21.300
which include matrix operation,


00:22:21.300 --> 00:22:25.020
through the NumPy array, also known as an NDRA.


00:22:25.020 --> 00:22:28.000
The problem was with NDRAs is they're fantastic,


00:22:28.000 --> 00:22:31.000
the documentation was a little bit hard for newcomers.


00:22:31.000 --> 00:22:34.320
So Anne Bonner wrote a whole new set of documentation


00:22:34.320 --> 00:22:37.440
for people that are both new to Python


00:22:37.440 --> 00:22:38.960
and scientific programming,


00:22:38.960 --> 00:22:42.640
and that's included in the NumPy docs themselves.


00:22:42.640 --> 00:22:45.320
Before, if you wanted to find out what arrays were,


00:22:45.320 --> 00:22:47.560
how they worked, you could go to the section


00:22:47.560 --> 00:22:50.080
and you could find out the parameters and attributes


00:22:50.080 --> 00:22:52.600
and all the methods of that class,


00:22:52.600 --> 00:22:54.920
but you wouldn't find out how or why you would use it.


00:22:54.920 --> 00:22:56.800
And so this documentation is fantastic


00:22:56.800 --> 00:22:59.300
because it has an explanation of what they are,


00:22:59.300 --> 00:23:01.240
it has visuals of what happens


00:23:01.240 --> 00:23:03.540
when you perform certain operations on arrays,


00:23:03.540 --> 00:23:05.920
and it has a lot of really great resources


00:23:05.920 --> 00:23:07.900
if you're just getting started with NumPy.


00:23:07.900 --> 00:23:09.100
The strong recommend for me,


00:23:09.100 --> 00:23:11.420
if you're doing any type of data work in Python,


00:23:11.420 --> 00:23:12.660
especially with pandas,


00:23:12.660 --> 00:23:14.580
that you become familiar with NumPy arrays,


00:23:14.580 --> 00:23:16.660
and this makes it really easy to do so.


00:23:16.660 --> 00:23:17.500
- Yeah, nice.


00:23:17.500 --> 00:23:18.780
It has things like,


00:23:18.780 --> 00:23:22.060
how do I convert a 1D array to a 2D array,


00:23:22.060 --> 00:23:25.100
or what's the difference between a Python list


00:23:25.100 --> 00:23:27.380
and a numpy array and whatnot.


00:23:27.380 --> 00:23:28.800
Yeah, it looks really helpful.


00:23:28.800 --> 00:23:31.060
I like the why, that's often missing.


00:23:31.060 --> 00:23:34.560
You'll see like, you do this, use this function for this,


00:23:34.560 --> 00:23:36.040
and here are the parameters.


00:23:36.040 --> 00:23:38.580
Sometimes they'll describe them, sometimes not.


00:23:38.580 --> 00:23:42.360
And then it's just like, well, maybe this is what I want.


00:23:42.360 --> 00:23:44.360
Stack Overflow seemed to indicate this is what I want.


00:23:44.360 --> 00:23:45.620
I'm not sure, I'll give it a try, right?


00:23:45.620 --> 00:23:47.940
So I like the little extra guidance behind it, that's great.


00:23:47.940 --> 00:23:49.820
- Yeah, it does a really good job of orienting you.


00:23:49.820 --> 00:23:51.700
- Cool, all right, well, Vicki,


00:23:51.700 --> 00:23:54.020
those are our main topics for the week,


00:23:54.020 --> 00:23:58.340
But we got a few extra quick items just to throw in here at the end.


00:23:58.340 --> 00:23:59.620
I'll let you go first with yours.


00:23:59.620 --> 00:23:59.860
Sure.


00:23:59.860 --> 00:24:03.580
This is just a bit of blatant self-promotion about who I am.


00:24:03.580 --> 00:24:06.340
So I am a data scientist on the side.


00:24:06.340 --> 00:24:09.340
I write a newsletter that's called Norm Core Tech.


00:24:09.340 --> 00:24:12.780
And it's about all the things that I'm not seeing covered in the mainstream media.


00:24:12.780 --> 00:24:15.180
And it's just a random hodgepodge of stuff.


00:24:15.180 --> 00:24:19.780
It ranges from anything like machine learning, how the data sets got created


00:24:19.780 --> 00:24:21.460
initially for NLP.


00:24:21.460 --> 00:24:23.580
I've written about Elon Musk memes.


00:24:23.580 --> 00:24:29.620
I wrote about the recent raid of the Nginx office in great detail and what happened there.


00:24:29.620 --> 00:24:33.800
So there's a free version that goes up once a week and paid subscribers get access to


00:24:33.800 --> 00:24:36.200
one more paid newsletter per week.


00:24:36.200 --> 00:24:38.920
But really it's more about the idea of supporting in-depth writing.


00:24:38.920 --> 00:24:40.400
So it's just vicki.substack.com.


00:24:40.400 --> 00:24:41.400
Cool.


00:24:41.400 --> 00:24:45.040
Well, that's a neat newsletter and I'm a subscriber.


00:24:45.040 --> 00:24:46.920
So very, very nice.


00:24:46.920 --> 00:24:49.640
I have a quick one for you all out there.


00:24:49.640 --> 00:24:51.360
And maybe two actually.


00:24:51.360 --> 00:24:54.560
PIP 20.0 was released.


00:24:54.560 --> 00:24:57.560
So not a huge change, obviously,


00:24:57.560 --> 00:25:01.760
PIP is compatible with the stuff that it did before and whatnot.


00:25:01.760 --> 00:25:04.360
But it does a couple of nice things,


00:25:04.360 --> 00:25:06.960
and I think this is going to be extra nice for beginners


00:25:06.960 --> 00:25:09.160
because it's so challenging.


00:25:09.160 --> 00:25:10.960
You go to a tutorial and it says,


00:25:10.960 --> 00:25:13.560
"All right, the first thing you got to do to run whatever,


00:25:13.560 --> 00:25:16.160
I want to run Flask, or I want to run Jupyter."


00:25:16.160 --> 00:25:19.160
As you say, "PIP install Flask" or "PIP install Jupyter."


00:25:19.160 --> 00:25:22.680
and it says you do not have permission to, you know,


00:25:22.680 --> 00:25:25.320
write to wherever those are going to install, right?


00:25:25.320 --> 00:25:26.880
Depending on your system.


00:25:26.880 --> 00:25:30.680
And so if that happens now in pip20,


00:25:30.680 --> 00:25:35.560
it will install as if --user was passed into the user profile.


00:25:35.560 --> 00:25:36.360
That's cool, huh?


00:25:36.360 --> 00:25:37.080
That's really neat.


00:25:37.080 --> 00:25:38.680
Yeah, so that's great.


00:25:38.680 --> 00:25:41.720
And cache wheels are built from GitHub requirements


00:25:41.720 --> 00:25:43.280
and a couple of other things.


00:25:43.280 --> 00:25:46.080
So yeah, nothing major, but nice to have that there.


00:25:46.080 --> 00:25:52.020
And then also I previously gone on a bit of a rant saying I was bugged that homebrew,


00:25:52.020 --> 00:25:55.420
which is how I put Python on my Mac,


00:25:55.420 --> 00:25:58.720
was great for installing Python 3 until 3.7.


00:25:58.720 --> 00:25:59.820
So if you just...


00:25:59.820 --> 00:26:03.680
It's even better because if you just say brew install Python,


00:26:03.680 --> 00:26:07.120
that means Python 3, which not legacy Python, which is great.


00:26:07.120 --> 00:26:08.620
But that sort of stopped working.


00:26:08.620 --> 00:26:11.580
It still works, but it installs Python 3.7.


00:26:11.580 --> 00:26:15.780
So that was kind of like, oh, sad face.


00:26:15.780 --> 00:26:20.340
But I'm sorry, I forget the person who sent this over on Twitter, but one of the listeners sent in a message said,


00:26:20.340 --> 00:26:24.260
"You can brew install python at 3.8 and that works."


00:26:24.260 --> 00:26:30.740
Why that's not... Is it safe to brew again? I've just started downloading directly from python.net. I know, exactly.


00:26:30.740 --> 00:26:35.380
Exactly. So I'm trying it today and so far it's going well.


00:26:35.380 --> 00:26:39.700
So I'm really excited that on macOS we can probably get the latest python.


00:26:39.700 --> 00:26:42.420
Even if you got to save the version, I just have an alias that


00:26:43.040 --> 00:26:46.840
re-aliases what Python means in my zshrc file,


00:26:46.840 --> 00:26:50.740
and it'll just say, "You know, if you type Python, that means Python 3.8 for now."


00:26:50.740 --> 00:26:51.880
Anyway, I'm pretty...


00:26:51.880 --> 00:26:52.840
- Fingers crossed. - Yeah, fingers crossed.


00:26:52.840 --> 00:26:54.780
So it looks like it's good, and that's nice.


00:26:54.780 --> 00:26:56.180
Hopefully, it just keeps updating itself.


00:26:56.180 --> 00:26:59.100
I suspect it will, at least within the 3.8 branch.


00:26:59.100 --> 00:27:00.940
All right, you ready to close this out with a joke?


00:27:00.940 --> 00:27:02.880
- Yeah. - Yeah, so...


00:27:02.880 --> 00:27:05.040
I'm sure you've heard the type of joke, you know,


00:27:05.040 --> 00:27:08.540
a mathematician and a physicist walk into a bar and...


00:27:08.600 --> 00:27:12.900
Right? Well, some weird thing about numbers and space ensues.


00:27:12.900 --> 00:27:17.300
So this one is kind of like that one. It's about search engine optimization.


00:27:17.300 --> 00:27:23.400
So an SEO expert walks into a bar, bars, pub, public house, Irish pub, tavern,


00:27:23.400 --> 00:27:27.200
bartender, beer, liquor, wine, alcohol, spirits, and so on.


00:27:27.200 --> 00:27:30.100
It's bad, huh?


00:27:30.100 --> 00:27:32.100
I like that. That's nice.


00:27:32.100 --> 00:27:33.900
Yeah, it's so true.


00:27:33.900 --> 00:27:37.200
Like, you remember how blatant websites used to be like 10 years ago,


00:27:37.200 --> 00:27:41.600
They would just have a massive bunch of just random keywords at the bottom.


00:27:41.600 --> 00:27:43.300
Just, you know, it seems like...


00:27:43.300 --> 00:27:45.360
Yeah, and sometimes they would be in white and white text.


00:27:45.360 --> 00:27:47.100
Yes, exactly, white and white.


00:27:47.100 --> 00:27:49.840
But then if you highlight it, it would be like whole three paragraphs.


00:27:49.840 --> 00:27:52.000
Here's where the SEO hacker went.


00:27:52.000 --> 00:27:54.240
I don't think that works so well anymore.


00:27:54.240 --> 00:27:56.600
But yeah, it's a good joke nonetheless.


00:27:56.600 --> 00:27:59.240
And Vicki, it's been great to have you here.


00:27:59.240 --> 00:28:01.300
Thanks so much for filling in for Brian


00:28:01.300 --> 00:28:04.300
and sharing the data science view of the world with us.


00:28:04.300 --> 00:28:05.740
- Thanks for having me. - You bet. Bye.


00:28:05.740 --> 00:28:06.400
Bye.


00:28:06.460 --> 00:28:08.160
Thank you for listening to Python Bytes.


00:28:08.160 --> 00:28:10.660
Follow the show on Twitter via @PythonBytes.


00:28:10.660 --> 00:28:13.660
That's Python Bytes as in B-Y-T-E-S.


00:28:13.660 --> 00:28:16.860
And get the full show notes at pythonbytes.fm.


00:28:16.860 --> 00:28:18.460
If you have a news item you want featured,


00:28:18.460 --> 00:28:20.960
just visit pythonbytes.fm and send it our way.


00:28:20.960 --> 00:28:23.760
We're always on the lookout for sharing something cool.


00:28:23.760 --> 00:28:25.660
On behalf of myself and Brian Auchin,


00:28:25.660 --> 00:28:26.960
this is Michael Kennedy.


00:28:26.960 --> 00:28:28.960
Thank you for listening and sharing this podcast


00:28:28.960 --> 00:28:30.460
with your friends and colleagues.

