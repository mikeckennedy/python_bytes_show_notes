
00:00:00.000 --> 00:00:07.080
Hello and welcome to Python bites where we deliver Python news and headlines directly to earbuds. This is episode 89


00:00:07.080 --> 00:00:13.440
Recorded August 2nd 2018. I'm Michael Kennedy and I'm Brian. Okken. Hey, Brian. How you doing? I'm doing great


00:00:13.440 --> 00:00:16.320
It's good to talk to you again. Yeah you as well and man


00:00:16.320 --> 00:00:21.120
We got some cool stuff lined up this week. We always say it but there's really so much happening in the Python space


00:00:21.120 --> 00:00:22.800
It's more exciting every week


00:00:22.800 --> 00:00:23.120
Yeah


00:00:23.120 --> 00:00:29.280
I think it's because we're here and then people are excited about Python and then they do more and then it builds on itself


00:00:29.280 --> 00:00:32.160
It's this awesome feedback loop. I can totally feel it.


00:00:32.160 --> 00:00:33.160
Yeah.


00:00:33.160 --> 00:00:36.160
What also is awesome is Datadog for sponsoring the show.


00:00:36.160 --> 00:00:39.640
So if you need infrastructure monitoring or monitoring of your apps,


00:00:39.640 --> 00:00:42.360
check them out at pythonbytes.fm/datadog.


00:00:42.360 --> 00:00:43.960
Tell you more about them later.


00:00:43.960 --> 00:00:46.920
Brian, you found some code that's pretty tenacious out there, didn't you?


00:00:46.920 --> 00:00:47.800
It just won't quit.


00:00:47.800 --> 00:00:49.600
Tenacious is a fun word.


00:00:49.600 --> 00:00:55.040
And there's a project called Tenacity that is pretty cool.


00:00:55.040 --> 00:00:57.600
We'll, of course, have a link in the show notes.


00:00:57.600 --> 00:01:02.600
But Tenacity is a general purpose retrying library


00:01:02.600 --> 00:01:05.600
to simplify the task of adding retry behavior


00:01:05.600 --> 00:01:07.180
to just about anything.


00:01:07.180 --> 00:01:08.820
And there's a little code snippet,


00:01:08.820 --> 00:01:12.780
but the gist of it is you import retry,


00:01:12.780 --> 00:01:14.880
and it can have a lot of options,


00:01:14.880 --> 00:01:18.300
but the defaults just sort of work also.


00:01:18.300 --> 00:01:20.880
You can just put this around a function,


00:01:20.880 --> 00:01:24.880
and if your function raises an exception,


00:01:24.880 --> 00:01:27.000
it just tries it again.


00:01:27.000 --> 00:01:29.480
and eats the exception.


00:01:29.480 --> 00:01:32.040
So for a lot of stuff, this is terrible.


00:01:32.040 --> 00:01:34.480
It's a terrible idea in a lot of places.


00:01:34.480 --> 00:01:38.320
But a few places, this might be reasonably good,


00:01:38.320 --> 00:01:40.920
especially like if, for instance, if you're gonna,


00:01:40.920 --> 00:01:43.360
like, I guess I'm not even gonna come with,


00:01:43.360 --> 00:01:45.280
for instance, you guys know,


00:01:45.280 --> 00:01:47.600
in places where retrying is a good idea,


00:01:47.600 --> 00:01:50.640
like maybe saving something to a file system


00:01:50.640 --> 00:01:54.280
or connecting to a service that sometimes has contention.


00:01:54.280 --> 00:01:55.480
- Yeah, or even a database,


00:01:55.480 --> 00:01:59.140
Like what if you have to like restart the database server


00:01:59.140 --> 00:02:00.960
or something to that effect, right?


00:02:00.960 --> 00:02:05.480
Or you need to really quickly apply a migration, right?


00:02:05.480 --> 00:02:08.900
You could say, just keep retrying the database


00:02:08.900 --> 00:02:09.740
until you get there.


00:02:09.740 --> 00:02:11.000
- Yeah, and then there's a whole bunch


00:02:11.000 --> 00:02:12.640
of extra conditions you can put on it.


00:02:12.640 --> 00:02:17.640
You can say, try so many times or a wait time for,


00:02:17.640 --> 00:02:21.080
if it doesn't work after a while, then give up.


00:02:21.080 --> 00:02:23.520
Then you can customize which exceptions


00:02:23.520 --> 00:02:25.560
it catches or doesn't catch.


00:02:25.560 --> 00:02:29.760
It even has a retry on coroutines, which is kind of fun.


00:02:29.760 --> 00:02:32.340
I haven't tried that, but I've tried the simple case.


00:02:32.340 --> 00:02:34.080
But I'm going to use it right away.


00:02:34.080 --> 00:02:37.860
We've got several conditions where we're in testing devices


00:02:37.860 --> 00:02:40.720
where sometimes it takes a device,


00:02:40.720 --> 00:02:43.520
we're doing like a little wifi devices, for example,


00:02:43.520 --> 00:02:44.400
it might be asleep.


00:02:44.400 --> 00:02:46.760
So it might take a while for the thing to wake up


00:02:46.760 --> 00:02:48.280
and respond.


00:02:48.280 --> 00:02:50.360
So a number of retries is good.


00:02:50.360 --> 00:02:54.000
And we have like retry code all over our code base.


00:02:54.000 --> 00:02:56.840
And so having decorator do that for us


00:02:56.840 --> 00:02:57.920
is just pretty slick.


00:02:57.920 --> 00:02:58.760
I like it.


00:02:58.760 --> 00:02:59.580
- Yeah, it's really cool.


00:02:59.580 --> 00:03:00.920
And you know, like I said, the database thing,


00:03:00.920 --> 00:03:02.240
like you wouldn't just put retry


00:03:02.240 --> 00:03:04.720
and say continue to hammer the database indefinitely


00:03:04.720 --> 00:03:07.040
if you can't, if you run into any problems.


00:03:07.040 --> 00:03:09.120
But you know, like the, like try five times


00:03:09.120 --> 00:03:10.720
with an exponential fall off,


00:03:10.720 --> 00:03:14.480
just so you could basically handle five second down times,


00:03:14.480 --> 00:03:15.400
things like that.


00:03:15.400 --> 00:03:16.240
Be nice.


00:03:16.240 --> 00:03:18.800
- Yeah, there's definitely be parts of, like you said,


00:03:18.800 --> 00:03:24.960
a distributed system where you got things that sometimes are not available for a little


00:03:24.960 --> 00:03:25.960
while.


00:03:25.960 --> 00:03:30.200
Yeah, especially stuff that you don't control everything like other services you depend


00:03:30.200 --> 00:03:31.200
upon.


00:03:31.200 --> 00:03:32.200
Yeah.


00:03:32.200 --> 00:03:33.200
Yeah, pretty cool.


00:03:33.200 --> 00:03:38.640
So I'm going to bring a theme into this show here for the next couple of topics that I'm


00:03:38.640 --> 00:03:39.640
going to bring in.


00:03:39.640 --> 00:03:43.120
And I think it's actually going to touch on every one of the things I'm bringing.


00:03:43.120 --> 00:03:45.480
So let's start with Anthony Shaw.


00:03:45.480 --> 00:03:48.680
Of course, we've got to cover an article by Anthony Shaw, right?


00:03:48.680 --> 00:03:50.440
Yeah, he writes a lot of good stuff.


00:03:50.440 --> 00:03:51.440
Yeah, he does.


00:03:51.440 --> 00:03:56.120
And one of the ones that I came across here is called, "Why is Python so slow?"


00:03:56.120 --> 00:04:01.000
So I think it's interesting to even ask the question, like, "Is Python slow?" and explain


00:04:01.000 --> 00:04:02.280
how that is.


00:04:02.280 --> 00:04:08.520
So Anthony looks at some benchmarks comparing, say, Python to Java, to C#, to C++, and things


00:04:08.520 --> 00:04:13.480
like that, and talks about, well, in some cases it is slower, and why might that be,


00:04:13.480 --> 00:04:14.480
right?


00:04:14.480 --> 00:04:20.160
answering the question like Python completes a comparable operation like


00:04:20.160 --> 00:04:24.760
two to ten times slower than say Java or C#. Why? And why can't we make it


00:04:24.760 --> 00:04:33.400
faster? So he has three main hypotheses which are pretty interesting. One is


00:04:33.400 --> 00:04:38.180
it's the GIL, the global interpreter lock. The other is it's because it's


00:04:38.180 --> 00:04:44.360
interpreted rather than compiled. And the final one is it's a dynamic language. So


00:04:44.360 --> 00:04:47.560
So those three ones are pretty interesting.


00:04:47.560 --> 00:04:49.080
What do you think?


00:04:49.080 --> 00:04:53.280
Okay, so he's not saying that these are the reasons, but these are theories.


00:04:53.280 --> 00:04:57.760
These are theories, and then he goes through each one of them and pretty deeply looks at


00:04:57.760 --> 00:05:00.440
how they work and then compares them.


00:05:00.440 --> 00:05:03.040
So for example, it's interpreted, not compiled.


00:05:03.040 --> 00:05:04.720
Well what does that mean in terms of trade-off?


00:05:04.720 --> 00:05:09.280
Let's compare that to say the way C# does things, the way C++ works.


00:05:09.280 --> 00:05:11.060
What are the trade-offs there?


00:05:11.060 --> 00:05:14.720
So let's go through some of them.


00:05:14.720 --> 00:05:18.380
One is, it's the gill.


00:05:18.380 --> 00:05:23.220
Now modern computers, modern processors have multiple cores.


00:05:23.220 --> 00:05:28.220
If you actually look at Moore's law, Moore's law is still alive and well, the number of


00:05:28.220 --> 00:05:30.980
transistors in a chip.


00:05:30.980 --> 00:05:36.260
But people sort of correlated that indirectly to, well, that means computers are getting


00:05:36.260 --> 00:05:38.300
faster and faster and faster.


00:05:38.300 --> 00:05:43.380
But it turns out that a number of years ago, four, five, I don't know how many years ago,


00:05:43.380 --> 00:05:49.980
not too long ago, the actual clock speed no longer kept doubling along with Moore's law.


00:05:49.980 --> 00:05:51.920
It sort of went flat more or less.


00:05:51.920 --> 00:05:55.420
And what started happening was we got two core machines and then four core machines.


00:05:55.420 --> 00:05:56.820
And I just got a new MacBook.


00:05:56.820 --> 00:05:59.060
It has six hyperthreaded cores.


00:05:59.060 --> 00:06:00.460
It's crazy, right?


00:06:00.460 --> 00:06:05.660
But on Python, if I want to take advantage of that computationally, it's super hard within


00:06:05.660 --> 00:06:07.900
one process because of the gill.


00:06:07.900 --> 00:06:12.240
So said, well, if you want to take advantage of modern hardware, maybe the GIL is the


00:06:12.240 --> 00:06:13.240
problem.


00:06:13.240 --> 00:06:17.140
So he talks about some of the trade offs there, when it matters when it doesn't matter.


00:06:17.140 --> 00:06:22.220
So for example, if what you're doing is IO bound, it basically doesn't much matter, right?


00:06:22.220 --> 00:06:25.740
The GIL is released when you're waiting on like network calls and stuff like that.


00:06:25.740 --> 00:06:29.580
So in some sense, like the GIL is not not the problem.


00:06:29.580 --> 00:06:32.940
If you created a bunch of threads, and they all started, you know, reading, writing files,


00:06:32.940 --> 00:06:37.020
talking over the network, it should just automatically handle that.


00:06:37.020 --> 00:06:40.660
But if on the other hand you try to create six threads and do computational stuff, you'll


00:06:40.660 --> 00:06:47.380
still probably get 12% CPU usage on my machine because it only really gets to run one at


00:06:47.380 --> 00:06:48.380
a time.


00:06:48.380 --> 00:06:50.620
So that's one of the theories.


00:06:50.620 --> 00:06:54.900
And I think this theory applies more in some places and less in others.


00:06:54.900 --> 00:06:57.380
And I kind of touched on that a little bit.


00:06:57.380 --> 00:07:02.620
If your goal is to do computational mathematical things, the GIL can really, really matter.


00:07:02.620 --> 00:07:05.220
It makes a big difference, right?


00:07:05.220 --> 00:07:08.500
you're trying to execute your Python code, it doesn't let go of the gill.


00:07:08.500 --> 00:07:12.180
But if say you're building a web app, it probably doesn't matter.


00:07:12.180 --> 00:07:16.780
There are some ways and you can do some things that would be better, but it doesn't really


00:07:16.780 --> 00:07:17.780
matter.


00:07:17.780 --> 00:07:25.340
So for example, if I looked at the various servers before we came on today, the training.talkpython.fm


00:07:25.340 --> 00:07:31.620
site has 16 worker processes all running parallel versions of the website handling requests.


00:07:31.620 --> 00:07:33.540
Talk Python itself has eight.


00:07:33.540 --> 00:07:35.900
I think maybe Python bytes has eight as well.


00:07:35.900 --> 00:07:40.160
So anyway, there's these eight processes and sure one of them may like lock up something


00:07:40.160 --> 00:07:44.460
with the gill, but there's a whole bunch of others that can leverage those other CPU cores


00:07:44.460 --> 00:07:45.620
and just keep on rocking.


00:07:45.620 --> 00:07:49.580
So if you're doing web stuff, it matters less in this sense, I think.


00:07:49.580 --> 00:07:53.060
And I mean, even if you are, there's ways to get around it.


00:07:53.060 --> 00:07:56.740
Yeah, if you can break up your algorithm and do sub process type parallelism.


00:07:56.740 --> 00:07:59.140
Okay, so that's the gill.


00:07:59.140 --> 00:08:02.660
The other is, could be it's an interpreted language.


00:08:02.660 --> 00:08:07.220
I think this one is the most interesting probably.


00:08:07.220 --> 00:08:12.580
It is an interpreted language, but actually it does compile code to bytecode, but it doesn't


00:08:12.580 --> 00:08:14.520
JIT compile it.


00:08:14.520 --> 00:08:21.940
One of the main considerations around JIT-compiled languages versus not is startup time.


00:08:21.940 --> 00:08:25.620
If our Python code is going to start and run for a while, then doing a whole bunch of JIT


00:08:25.620 --> 00:08:30.860
optimization would be maybe a little slower to start but then faster to run.


00:08:30.860 --> 00:08:35.420
But if we want to just do some CLI stuff that starts really quick, does a tiny thing and


00:08:35.420 --> 00:08:40.440
goes away, a whole bunch of JIT stuff might be, you know, sort of counterproductive.


00:08:40.440 --> 00:08:45.580
So there's a pretty interesting comparison against C# and Java and CPython here.


00:08:45.580 --> 00:08:49.860
The other thing I think that's worth throwing in here is because of C extensions and things


00:08:49.860 --> 00:08:52.500
like that, it's an interpreted language.


00:08:52.500 --> 00:08:54.980
I think that's a simplistic view.


00:08:54.980 --> 00:08:59.060
That's like, well, if you just take straight Python code and just run it on Python, you


00:08:59.060 --> 00:09:00.580
don't interact with any libraries.


00:09:00.580 --> 00:09:06.220
But if you work with NumPy, or if you work with SQLAlchemy, or a whole bunch of stuff


00:09:06.220 --> 00:09:11.180
that has C extensions to make certain parts fast, well, all of a sudden it's not interpreted.


00:09:11.180 --> 00:09:12.460
So there's these weird blends.


00:09:12.460 --> 00:09:14.080
All right, last one.


00:09:14.080 --> 00:09:16.540
It's because it's dynamically typed.


00:09:16.540 --> 00:09:19.180
So this is also, I think this is really interesting.


00:09:19.180 --> 00:09:23.260
I think actually this is probably why, and I'm going to throw in another one, unless


00:09:23.260 --> 00:09:25.340
I get distracted and forget it.


00:09:25.340 --> 00:09:30.800
I think this is really why it's probably the slowest, is that it's a dynamic language.


00:09:30.800 --> 00:09:36.460
It's not that you can't make a dynamic language fast, but because it's so flexible, it's hard


00:09:36.460 --> 00:09:41.540
to know how to optimize it.


00:09:41.540 --> 00:09:45.340
You might want to inline a function, but somebody could monkey patch that function, and then


00:09:45.340 --> 00:09:47.460
you wouldn't be inlining the right thing, for example.


00:09:47.460 --> 00:09:49.960
And then you monkey patch it only sometimes.


00:09:49.960 --> 00:09:50.960
What do you do then?


00:09:50.960 --> 00:09:51.960
Right?


00:09:51.960 --> 00:09:56.060
So things like method inlining, which can really make things faster, is super hard because


00:09:56.060 --> 00:09:57.060
that could actually change.


00:09:57.060 --> 00:10:01.300
Where say in C# or C++ or whatever, the method won't change.


00:10:01.300 --> 00:10:02.300
Yeah.


00:10:02.300 --> 00:10:03.300
Okay.


00:10:03.300 --> 00:10:04.300
All right.


00:10:04.300 --> 00:10:05.300
Final one.


00:10:05.300 --> 00:10:06.300
This is mine.


00:10:06.300 --> 00:10:07.300
I'm adding this to his.


00:10:07.300 --> 00:10:09.820
And it sort of has to do with this dynamic typing thing is everything is a reference


00:10:09.820 --> 00:10:12.900
type allocated on a heap in Python.


00:10:12.900 --> 00:10:13.900
Right?


00:10:13.900 --> 00:10:19.140
Some of the stuff that makes C++ and C# really, really fast is things like numbers and other


00:10:19.140 --> 00:10:21.700
stuff are allocated on the stack.


00:10:21.700 --> 00:10:25.660
And when you work with them, you never do pointer dereferencing, you never do reference


00:10:25.660 --> 00:10:29.940
counting, never do garbage collection or memory management, you just work with little bits


00:10:29.940 --> 00:10:30.940
on the stack.


00:10:30.940 --> 00:10:35.220
And because that little thing on the stack could become a full blown list or something


00:10:35.220 --> 00:10:39.860
just by changing what it points at, I think probably that also makes a big difference.


00:10:39.860 --> 00:10:43.240
- Okay, there's also like function calls


00:10:43.240 --> 00:10:45.700
are slower than they need to be.


00:10:45.700 --> 00:10:47.500
And that's, I think that's one of the things


00:10:47.500 --> 00:10:50.700
the Python core team is working on, is to try to--


00:10:50.700 --> 00:10:52.500
- And luckily there've been some advances there


00:10:52.500 --> 00:10:54.500
in the latest version of Python.


00:10:54.500 --> 00:10:57.220
I think 20% or something they got, was that three seven,


00:10:57.220 --> 00:10:58.580
I think that they got that much faster.


00:10:58.580 --> 00:11:00.740
So work is being done, but yeah,


00:11:00.740 --> 00:11:03.100
it could be more, I guess.


00:11:03.100 --> 00:11:05.540
But what's really interesting is the trade-offs


00:11:05.540 --> 00:11:07.900
or sort of comparisons of the article.


00:11:07.900 --> 00:11:11.820
- The thing that I want, there's a forest in the trees


00:11:11.820 --> 00:11:14.580
sort of issue I have here is that


00:11:14.580 --> 00:11:16.180
I don't think Python is slow.


00:11:16.180 --> 00:11:17.020
- Yeah, I'm with you.


00:11:17.020 --> 00:11:20.460
- And my people time is way more expensive


00:11:20.460 --> 00:11:25.100
than computer time for like 98% of the applications


00:11:25.100 --> 00:11:27.220
in the world as far as I probably.


00:11:27.220 --> 00:11:29.420
- Well, and somebody made that comment below


00:11:29.420 --> 00:11:30.700
in the comment section of this article.


00:11:30.700 --> 00:11:33.060
It said, "Well, if you're optimizing milliseconds


00:11:33.060 --> 00:11:35.180
"versus nanoseconds, yes, maybe.


00:11:35.180 --> 00:11:42.380
if you're optimizing weeks versus month from idea to shipped, you know, you Python's not slow at all. It's really fast.


00:11:42.380 --> 00:11:59.500
Yeah. And that I mean, that's what I see is the maintenance, development time, the maintenance time, all of those extra people time things. Python is way faster. And a lot of these things that we say is like the GIL is a problem or multi processing is difficult.


00:11:59.820 --> 00:12:01.820
Well, multi-processing isn't easy.


00:12:01.820 --> 00:12:03.820
Maybe it's easy in some languages.


00:12:03.820 --> 00:12:05.820
I mean, Go is sort of designed to do that


00:12:05.820 --> 00:12:07.820
from the start. But


00:12:07.820 --> 00:12:09.820
getting a complex algorithm in C++


00:12:09.820 --> 00:12:11.820
to utilize multi-cores,


00:12:11.820 --> 00:12:13.820
that's not a trivial task either.


00:12:13.820 --> 00:12:15.820
No, it's not. It's definitely not.


00:12:15.820 --> 00:12:17.820
And I would say on the


00:12:17.820 --> 00:12:19.820
web framework side, actually Python is really


00:12:19.820 --> 00:12:21.820
quite fast. I've compared it


00:12:21.820 --> 00:12:23.820
against other, like C#-based


00:12:23.820 --> 00:12:25.820
JIT compiled


00:12:25.820 --> 00:12:27.820
things like ASP.NET and stuff, or


00:12:27.820 --> 00:12:33.060
conference talk I did comparing Python to the .NET stuff. And actually Python was


00:12:33.060 --> 00:12:37.940
not just as fast but faster than the JIT compiled C# stuff. Yeah and also


00:12:37.940 --> 00:12:42.760
just for people that do think that maybe the Python speeds the problem really


00:12:42.760 --> 00:12:46.500
measure it and you can optimize there's ways in Python to optimize the parts


00:12:46.500 --> 00:12:51.040
that are slow. So yeah my next item will come back to this and show you some ways to make it


00:12:51.040 --> 00:12:57.420
faster. Okay. All right. So what's this Mew thing all about? Mew, you talked about Mew


00:12:57.420 --> 00:13:01.020
before, right? It's like a simplified IDE type thing. Is that right?


00:13:01.020 --> 00:13:06.300
Right. So I'm going to highlight Mew again, partly because I think it's a neat project


00:13:06.300 --> 00:13:10.580
that's going on and there's a lot of cool people working on it. But there was an article


00:13:10.580 --> 00:13:18.460
called Keynoting in Mew, and Mew being MU. And in the EuroPython 2018, David Beasley


00:13:18.460 --> 00:13:24.260
did a talk in a demo called Die Threads. And what amused me is my first thought was, is


00:13:24.260 --> 00:13:29.700
this a German joke?" And he actually addressed it early on. It's not a German joke, but it's


00:13:29.700 --> 00:13:36.860
a good demo. But he used Mew during his Python talk, and this article talks about, and also


00:13:36.860 --> 00:13:43.560
asked him about it, why he did that. And it's just a simple thing. It's the same experience


00:13:43.560 --> 00:13:49.080
for everybody. Like for instance, I use PyCharm, but my environment in PyCharm, all the different


00:13:49.080 --> 00:13:52.080
colors I like or the plugins I use.


00:13:52.080 --> 00:13:54.160
It's going to be different than everybody else.


00:13:54.160 --> 00:13:56.480
It's one of those customizable things.


00:13:56.480 --> 00:14:02.280
Having a very simple interface like Mew that works as a learning tool,


00:14:02.280 --> 00:14:05.880
but also shows people exactly what you're doing.


00:14:05.880 --> 00:14:08.900
One of the features of it is that if you've


00:14:08.900 --> 00:14:11.140
got a little script that you want to run,


00:14:11.140 --> 00:14:14.100
you can just push run at the top and then automatically


00:14:14.100 --> 00:14:17.240
a little window pops up at the bottom and shows you


00:14:17.240 --> 00:14:20.120
the output of the thing you're running.


00:14:20.120 --> 00:14:22.220
And that's just really handy.


00:14:22.220 --> 00:14:25.800
And so it's kind of fun to watch that in a talk


00:14:25.800 --> 00:14:27.480
and being used.


00:14:27.480 --> 00:14:28.960
And I think that would be, I use it,


00:14:28.960 --> 00:14:31.280
do little demos for people at work.


00:14:31.280 --> 00:14:34.120
And I think I might try this also just to not have


00:14:34.120 --> 00:14:36.880
to answer questions like, what plugin are you using?


00:14:36.880 --> 00:14:37.760
Or whatever.


00:14:37.760 --> 00:14:40.760
- Yeah, just sort of keep the distractions to a minimum, huh?


00:14:40.760 --> 00:14:43.060
- Yeah, and it's a real clean interface


00:14:43.060 --> 00:14:45.240
and looks like you can change the font size


00:14:45.240 --> 00:14:46.960
and it looks fun.


00:14:46.960 --> 00:14:50.460
Plus, Mew is, if you haven't played around with it yet,


00:14:50.460 --> 00:14:54.920
it's also something that it automatically has hooks


00:14:54.920 --> 00:14:57.660
into things like running micro bit


00:14:57.660 --> 00:15:00.860
and Raspberry Pi and stuff.


00:15:00.860 --> 00:15:03.420
- A micro Python and embedded IoT things?


00:15:03.420 --> 00:15:04.260
- Yeah.


00:15:04.260 --> 00:15:05.080
- Yeah, very cool.


00:15:05.080 --> 00:15:05.920
Yeah, I like it.


00:15:05.920 --> 00:15:06.740
That's a nice one.


00:15:06.740 --> 00:15:10.500
So before we get on to my next performance thing,


00:15:10.500 --> 00:15:11.860
let me tell you guys about Datadog.


00:15:11.860 --> 00:15:14.660
So Datadog is sponsoring this episode like they have many.


00:15:14.660 --> 00:15:16.860
Thank you to them for that, of course.


00:15:16.860 --> 00:15:21.260
So they have infrastructure monitoring, distributed tracing, and they've added logging.


00:15:21.260 --> 00:15:26.780
They provide end to end visibility for requests across different parts of your infrastructure,


00:15:26.780 --> 00:15:31.980
and as well as health and performance monitoring of your Python apps. So get started with them


00:15:31.980 --> 00:15:37.180
today for a 14 day free trial. And of course, they'll send you a cool data dog t shirt on it,


00:15:37.180 --> 00:15:42.140
just go to Python bytes.fm slash data dog and check it out. So yeah, very cool stuff from data


00:15:42.140 --> 00:15:47.500
dog. Brian, I told you about the performance thing and whether Python is slow. I agree


00:15:47.500 --> 00:15:52.940
with you. I generally find for what I do, it's like actually blazing. So not a problem,


00:15:52.940 --> 00:15:58.020
but it sort of depends on choosing the right infrastructure. Yeah. Yeah. So there's this


00:15:58.020 --> 00:16:05.860
interesting proof of concept done by this. I forgot the names, a European open source


00:16:05.860 --> 00:16:11.980
consortium. And the basic theme of this article is sort of exploring a response


00:16:11.980 --> 00:16:18.780
to the question of, "So I've heard Python is slow. Is it?" And I think it depends. So


00:16:18.780 --> 00:16:23.060
what they've done is they've created a multi-core, talked about how that can be


00:16:23.060 --> 00:16:28.020
hard to take advantage of, but here it is, multi-core Python HTTP server that is


00:16:28.020 --> 00:16:31.980
much faster than Go. So you've heard a lot of people say, "Well, we're gonna go


00:16:31.980 --> 00:16:35.740
to Go because its parallelism is so much better than Python and it's fast."


00:16:35.740 --> 00:16:40.460
so we can, you know, do things fast. And of course, there's that big trade off with sort


00:16:40.460 --> 00:16:46.300
of the functionality and speed to market or speed to idea completion and so on. But this thing is


00:16:46.300 --> 00:16:52.140
like, hey, and we can even go as fast or faster. So they compare it against actually to go web


00:16:52.140 --> 00:16:58.380
servers. And it's faster than both of them. So what it is, is these guys have gone and said,


00:16:58.380 --> 00:17:04.300
we've talked about this idea before they said, we want to go look at all the various C based,


00:17:04.300 --> 00:17:11.020
not Python C based sort of co-routine libraries that will let us write like low level HTTP


00:17:11.020 --> 00:17:15.420
servers and it turns out that there were not that many good options and the best option


00:17:15.420 --> 00:17:17.820
that they found was still slower than go.


00:17:17.820 --> 00:17:20.160
So they're like, well, maybe this isn't going to work.


00:17:20.160 --> 00:17:26.080
But then it turned out they found this thing called LWAN, L-W-A-N, which is a C library


00:17:26.080 --> 00:17:32.840
and they used Cython to create a Cython based web server that wraps it up and exposes it


00:17:32.840 --> 00:17:33.840
to Python.


00:17:33.840 --> 00:17:34.840
Cool, right?


00:17:34.840 --> 00:17:37.840
So basically, it's not really ready to ship or anything.


00:17:37.840 --> 00:17:42.320
It just validates the concept of creating a high-performance thing with Cython.


00:17:42.320 --> 00:17:43.880
And I think that's a big part of the article.


00:17:43.880 --> 00:17:46.760
So it says, "Here's some interesting things about Cython.


00:17:46.760 --> 00:17:52.440
It's both an optimizing static compiler and a hybrid language that gives you the ability


00:17:52.440 --> 00:17:57.280
to write Python code that can call back and forth with C and C++ really easy.


00:17:57.280 --> 00:18:03.080
It has static type declarations that make Python code faster because it can do like


00:18:03.080 --> 00:18:05.040
It doesn't have to treat everything like a reference type.


00:18:05.040 --> 00:18:08.960
It can put integers as integers on the stack and whatnot.


00:18:08.960 --> 00:18:13.680
The other thing is it releases the GIL by just having a keyword in Cython.


00:18:13.680 --> 00:18:16.680
You can say, "This part actually don't need the GIL.


00:18:16.680 --> 00:18:19.040
I'm doing this in C. Leave me alone."


00:18:19.040 --> 00:18:21.880
Isn't it interesting how that actually hits so many of the things that Anthony brought


00:18:21.880 --> 00:18:22.880
up?


00:18:22.880 --> 00:18:28.080
The typing, optimizations, and the GIL stuff is all right there.


00:18:28.080 --> 00:18:30.420
it generates super efficient C code


00:18:30.420 --> 00:18:33.240
that is then compiled into a Python module.


00:18:33.240 --> 00:18:36.420
So it's like really great for wrapping up C libraries


00:18:36.420 --> 00:18:37.580
and exposing them to Python.


00:18:37.580 --> 00:18:40.020
- I hope they continue on with work on this.


00:18:40.020 --> 00:18:41.860
- Yeah, it looks pretty awesome.


00:18:41.860 --> 00:18:44.000
Yeah, anyway, I think if you're interested


00:18:44.000 --> 00:18:45.540
in sort of checking out the performance,


00:18:45.540 --> 00:18:47.040
definitely have a look.


00:18:47.040 --> 00:18:48.180
It's pretty cool.


00:18:48.180 --> 00:18:50.980
So are you gonna cover something on testing in this episode?


00:18:50.980 --> 00:18:53.220
- Oh yeah, it's my turn again.


00:18:53.220 --> 00:18:54.900
- It's your turn for a theme.


00:18:54.900 --> 00:18:55.740
- Where am I at?


00:18:55.740 --> 00:18:56.660
I'm trying to get on here.


00:18:56.660 --> 00:19:01.820
Oh, yes, I am very excited about some news that came out last week.


00:19:01.820 --> 00:19:03.660
So I've been playing with--


00:19:03.660 --> 00:19:07.740
I started PyCharm, using PyCharm a while ago.


00:19:07.740 --> 00:19:09.300
You've been using it off and on.


00:19:09.300 --> 00:19:10.780
I think you use lots of stuff now.


00:19:10.780 --> 00:19:11.940
Yeah, yeah.


00:19:11.940 --> 00:19:12.940
For quite a while, yeah.


00:19:12.940 --> 00:19:18.100
The pytest support has been getting better and better in the last year or so.


00:19:18.100 --> 00:19:24.620
And the PyCharm released 2018.2, I think it was last week.


00:19:24.620 --> 00:19:29.620
And it totally beefs up support for pytest fixtures.


00:19:29.620 --> 00:19:32.040
And that's, I think, I just wanted to mention that


00:19:32.040 --> 00:19:35.960
because anybody that's using both PyCharm and pytest


00:19:35.960 --> 00:19:38.980
definitely needs to update to 2018.2


00:19:38.980 --> 00:19:42.780
because a few things that used to not work but do now,


00:19:42.780 --> 00:19:45.020
if you have a fixture that you're,


00:19:45.020 --> 00:19:46.900
so a fixture, if anybody's not familiar,


00:19:46.900 --> 00:19:49.460
a fixture is just a little piece of code,


00:19:49.460 --> 00:19:52.140
a little function that you declare as a fixture,


00:19:52.140 --> 00:19:55.180
that you put it as the parameter to your test,


00:19:55.180 --> 00:19:58.540
and it gets run before your test gets run at various levels.


00:19:58.540 --> 00:20:00.900
That's a simplification, but that works.


00:20:00.900 --> 00:20:03.020
But it shows up as a parameter,


00:20:03.020 --> 00:20:06.380
but you don't actually have to use it in your function.


00:20:06.380 --> 00:20:09.120
It just tells pytest to run that code.


00:20:09.120 --> 00:20:10.860
Well, PyCharm used to flag that


00:20:10.860 --> 00:20:13.520
as a variable that isn't referenced.


00:20:13.520 --> 00:20:15.300
So, it gives you a warning.


00:20:15.300 --> 00:20:18.260
And you also couldn't do code completion with it


00:20:18.260 --> 00:20:21.620
if it was an object that had methods on it.


00:20:21.620 --> 00:20:23.620
And you also couldn't use it to look up


00:20:23.620 --> 00:20:25.380
where is that fixture defined?


00:20:25.380 --> 00:20:27.420
Because it's often defined in a different file,


00:20:27.420 --> 00:20:29.240
in a conf test file or something.


00:20:29.240 --> 00:20:31.480
But now you can, all those things now work.


00:20:31.480 --> 00:20:34.180
So, big thank you to the PyCharm team


00:20:34.180 --> 00:20:35.580
for getting that out so quickly.


00:20:35.580 --> 00:20:39.500
And yeah, I just wanted to let people know about that.


00:20:39.500 --> 00:20:40.460
- Yeah, that's really awesome.


00:20:40.460 --> 00:20:42.460
PyCharm just keeps getting better and better.


00:20:42.460 --> 00:20:44.620
All right, so speaking of getting better,


00:20:44.620 --> 00:20:48.260
let's go back to Python performance in an indirect way.


00:20:48.260 --> 00:20:51.340
- You got like this bone that you won't let go of, man.


00:20:51.340 --> 00:20:52.340
No, it's good.


00:20:52.340 --> 00:20:57.900
So this one actually hits on two themes that I think we've touched a lot on.


00:20:57.900 --> 00:21:01.580
One is this performance kick that I'm on today, but the other is packaging.


00:21:01.580 --> 00:21:04.960
Like we've talked about how it's not super easy to package up Python.


00:21:04.960 --> 00:21:09.840
So for example, Go compiles to a single executable binary with zero dependencies.


00:21:09.840 --> 00:21:12.920
You take that, you give it to somebody, they run it.


00:21:12.920 --> 00:21:16.840
That's not how it works for complicated apps that have package dependencies and stuff in


00:21:16.840 --> 00:21:17.840
Python.


00:21:17.840 --> 00:21:20.160
You can't just easily go, "Here, double click this."


00:21:20.160 --> 00:21:22.480
You're like, yeah, good luck.


00:21:22.480 --> 00:21:24.040
Hold on first pip install requirements.


00:21:24.040 --> 00:21:25.040
Now what else?


00:21:25.040 --> 00:21:26.040
Oh, you got the wrong version of Python.


00:21:26.040 --> 00:21:27.040
Hold on.


00:21:27.040 --> 00:21:31.240
So the whole packaging thing, you know, there are some attempts to deal with it, like CX


00:21:31.240 --> 00:21:33.000
freeze and stuff like that.


00:21:33.000 --> 00:21:35.240
And it's somewhat working.


00:21:35.240 --> 00:21:41.440
But here's a interesting thing from Facebook called Azar.


00:21:41.440 --> 00:21:45.440
So Azar is an executable archive.


00:21:45.440 --> 00:21:52.180
And basically what it is, is you can package up some bunch of code into a single executable


00:21:52.180 --> 00:21:57.640
file that you can then mount as like a separate file system that's read only like, like a


00:21:57.640 --> 00:21:59.500
CD or something, I guess.


00:21:59.500 --> 00:22:01.580
And so you can mount this and then execute it.


00:22:01.580 --> 00:22:07.100
But because it came as a whole, like block, basically, it's sort of a native file system


00:22:07.100 --> 00:22:13.520
that you know, you can read files that are next to your Python files, and you just package


00:22:13.520 --> 00:22:15.100
up your dependencies and run it.


00:22:15.100 --> 00:22:20.340
Yeah, so it's this read only file system, which when you mount it, it looks like a regular


00:22:20.340 --> 00:22:24.700
directory to user space, the one drawback, like I was when I saw this, I'm like, Oh,


00:22:24.700 --> 00:22:25.700
my gosh, is this it?


00:22:25.700 --> 00:22:29.380
Is this the thing that is going to make it that we can just go here, double click this


00:22:29.380 --> 00:22:30.760
in Python.


00:22:30.760 --> 00:22:35.780
It turns out there's a minor little step here, this requires a one time installation of a


00:22:35.780 --> 00:22:38.740
system level device driver for the file system.


00:22:38.740 --> 00:22:41.660
So maybe not so much just double click.


00:22:41.660 --> 00:22:46.460
But if you're willing to install this, maybe as your organization you install it or on


00:22:46.460 --> 00:22:52.260
your servers for whatever reason, you're willing to install this thing called Squash FS, then


00:22:52.260 --> 00:22:55.460
stars become these things you can just pass around and run.


00:22:55.460 --> 00:22:56.460
So that's pretty cool.


00:22:56.460 --> 00:22:58.600
So that's a good caveat.


00:22:58.600 --> 00:23:02.760
But think of Facebook, their goal is to make it super easy to deploy these applications


00:23:02.760 --> 00:23:05.340
onto servers and run them.


00:23:05.340 --> 00:23:10.420
So they must pre-configure their servers with Squash FS and then just make this part of


00:23:10.420 --> 00:23:16.860
their deploy mechanism. So there's basically two primary use cases for these SARs. One is simply


00:23:16.860 --> 00:23:21.780
collecting a number of files for automatic atomic mounting somewhere in the file system. Cool. And


00:23:21.780 --> 00:23:30.060
said you can use this thing called SAR exec helper. It becomes a self-contained package of


00:23:30.060 --> 00:23:36.860
executable code and its data. So an example might be a Python app that archives all of its source


00:23:36.860 --> 00:23:40.780
code and its native libraries and configuration and all that kind of stuff.


00:23:40.780 --> 00:23:41.780
I still think that's pretty cool.


00:23:41.780 --> 00:23:45.100
Yeah, it's kind of a focused use but still pretty cool.


00:23:45.100 --> 00:23:49.380
Yeah, but then it's a rabbit hole now I got to go and read Squire about squash and what


00:23:49.380 --> 00:23:50.380
that is.


00:23:50.380 --> 00:23:51.580
Yes, it's true.


00:23:51.580 --> 00:23:59.380
So actually on this, it seems like it's generally, it could be a general mechanism for, I don't


00:23:59.380 --> 00:24:02.420
know, Ruby or JavaScript or Node or something like that.


00:24:02.420 --> 00:24:07.980
But they particularly call out Python on the GitHub page from the Facebook incubator.


00:24:07.980 --> 00:24:14.700
And it says some of the advantages for using it for Python are it looks like regular files


00:24:14.700 --> 00:24:15.700
on disk to Python.


00:24:15.700 --> 00:24:18.920
So that means you can just run CPython and it doesn't know any better.


00:24:18.920 --> 00:24:20.860
Same thing, it looks like regular files to use.


00:24:20.860 --> 00:24:24.780
You don't need to use like weird package import, package resource tricks or anything like that


00:24:24.780 --> 00:24:27.180
to pass stuff around.


00:24:27.180 --> 00:24:32.940
compression, it doesn't require of unpacking SO files, apparently, which like


00:24:32.940 --> 00:24:38.100
if you try to use one of the PEX mechanisms, that was a problem. So more


00:24:38.100 --> 00:24:42.220
or less it just like CPython just works basically. Your idea of like okay so


00:24:42.220 --> 00:24:46.100
there's this dependency but if you're using it within an organization that


00:24:46.100 --> 00:24:50.660
totally makes sense, it's fine. And I yeah that sort of use case or within your own


00:24:50.660 --> 00:24:54.740
servers or whatever, yeah there's a lot of use cases where I think this would be


00:24:54.740 --> 00:24:58.380
very useful for people. Yeah, I mean this like those places you described there's


00:24:58.380 --> 00:25:02.340
that's where a lot of Python is used. So it also has some interesting


00:25:02.340 --> 00:25:07.260
performance benefits which is coming back to that. It's because the way squash


00:25:07.260 --> 00:25:12.820
FS works you're actually reading off a disk a smaller set of binaries because


00:25:12.820 --> 00:25:16.580
it's compressed a smaller set of data so there's less disk activity and it's


00:25:16.580 --> 00:25:23.260
still really fast so the startup time can actually be faster for your app than


00:25:23.260 --> 00:25:26.160
and even native Python, which is pretty cool.


00:25:26.160 --> 00:25:28.580
And once it started, once it's pretty cool.


00:25:28.580 --> 00:25:30.220
There's some statistics in there


00:25:30.220 --> 00:25:32.880
that show it's either as fast or sometimes


00:25:32.880 --> 00:25:35.440
like the second time you've interacted with that,


00:25:35.440 --> 00:25:38.820
that SAR, because the file system actually decompresses


00:25:38.820 --> 00:25:40.320
and caches that stuff in memory,


00:25:40.320 --> 00:25:42.420
it can actually run a little bit quicker.


00:25:42.420 --> 00:25:43.820
So there's a lot of interesting things


00:25:43.820 --> 00:25:45.020
around performance there.


00:25:45.020 --> 00:25:49.940
And finally, these file system thing, these SARs, right?


00:25:49.940 --> 00:25:51.900
They're read-only, which means the integrity


00:25:51.900 --> 00:25:55.540
of your app is guaranteed as opposed to say a virtual environment or like a


00:25:55.540 --> 00:25:59.780
folder where people could mess with it or change the Python system like it's


00:25:59.780 --> 00:26:02.820
read-only so what you gave them is what they're running. That's a cool thing too.


00:26:02.820 --> 00:26:05.900
Yeah there's a lot of neat stuff here. I don't think it fits my use case but


00:26:05.900 --> 00:26:09.340
maybe some listeners it'll work well for them. Definitely. Yeah a lot of people


00:26:09.340 --> 00:26:13.180
seemed excited about it on the Twitter. On the Twitters. Exactly. All right well


00:26:13.180 --> 00:26:15.740
that's it for all of our items. You got some extras you want to throw out there?


00:26:15.740 --> 00:26:20.620
Oh just somebody mentioned to me on Twitter, speaking of Twitters, that NumPy


00:26:20.620 --> 00:26:25.620
1.15 or 1.15.0 was just released recently,


00:26:25.620 --> 00:26:28.860
and they completely overhauled their testing


00:26:28.860 --> 00:26:30.940
to use pytest, yay.


00:26:30.940 --> 00:26:33.180
- Yay, another win for pytest, that's awesome.


00:26:33.180 --> 00:26:35.340
- Yeah, and then you've got a couple lists


00:26:35.340 --> 00:26:37.260
of some videos that are out.


00:26:37.260 --> 00:26:38.740
- Yeah, I have a couple of events,


00:26:38.740 --> 00:26:41.540
but I have two in the future and two in the past.


00:26:41.540 --> 00:26:43.140
Maybe, depending on when you listen to this,


00:26:43.140 --> 00:26:44.300
actually all of them in the past,


00:26:44.300 --> 00:26:46.960
but right now, two in the future and two in the past.


00:26:46.960 --> 00:26:51.960
So SciPy 2018, the Data Science Python World Conference,


00:26:51.960 --> 00:26:54.920
the videos for that are now out on YouTube.


00:26:54.920 --> 00:26:56.720
So if you couldn't make it to SciPy


00:26:56.720 --> 00:26:59.400
and you wanna catch a bunch of the presentations there,


00:26:59.400 --> 00:27:00.720
here's a link to the videos.


00:27:00.720 --> 00:27:03.800
Also, PyOhio, we have a lot of these


00:27:03.800 --> 00:27:05.120
regional Python conferences,


00:27:05.120 --> 00:27:07.160
and somehow PyOhio has actually gained


00:27:07.160 --> 00:27:09.320
quite a bit of momentum, which is interesting


00:27:09.320 --> 00:27:12.540
with PyCon being there last year and next.


00:27:12.540 --> 00:27:14.280
Anyway, the videos for that are also out.


00:27:14.280 --> 00:27:17.960
So a bunch of good YouTube watching coming up here over the weekend.


00:27:17.960 --> 00:27:21.200
Yeah, I've already started a couple of these.


00:27:21.200 --> 00:27:23.080
And then in the future, we've got...


00:27:23.080 --> 00:27:26.220
In the future, PyCon Canada is coming.


00:27:26.220 --> 00:27:30.280
So the call for papers on that is open for about a month.


00:27:30.280 --> 00:27:40.280
And in the future, PyBay 2018, so the regional San Francisco Python conference is happening


00:27:40.280 --> 00:27:41.360
in a couple of weeks.


00:27:41.360 --> 00:27:42.960
And I would love to go to that, but I can't.


00:27:42.960 --> 00:27:44.120
Yeah, I still haven't decided.


00:27:44.120 --> 00:27:44.960
- Are you thinking about going?


00:27:44.960 --> 00:27:46.600
- I was thinking about going to Pi Bay,


00:27:46.600 --> 00:27:48.320
but there's a lot of stuff going on in the fall,


00:27:48.320 --> 00:27:49.160
so we'll see.


00:27:49.160 --> 00:27:51.120
- Yeah, I have daughters going to college right around then,


00:27:51.120 --> 00:27:53.400
so it'd probably be better if I helped them do that


00:27:53.400 --> 00:27:55.600
instead of just go hang out in San Francisco.


00:27:55.600 --> 00:27:59.560
All right, so the last thing is I just wanna let people know


00:27:59.560 --> 00:28:01.420
that I have another course out,


00:28:01.420 --> 00:28:04.360
building data-driven web apps with Pyramid and SQLAlchemy.


00:28:04.360 --> 00:28:07.080
It's super fun, nine hours of awesomeness.


00:28:07.080 --> 00:28:09.280
So there's a link in there, check that out as well,


00:28:09.280 --> 00:28:10.120
people are interested.


00:28:10.120 --> 00:28:11.120
- Oh, that looks fun.


00:28:11.120 --> 00:28:12.720
- Yeah, it's definitely a good one.


00:28:12.720 --> 00:28:16.640
I covered some things that I've wanted to cover for a while like ORM migrations and


00:28:16.640 --> 00:28:19.600
managing stuff over time with databases and stuff.


00:28:19.600 --> 00:28:20.600
Pretty cool.


00:28:20.600 --> 00:28:21.600
All right.


00:28:21.600 --> 00:28:22.600
Well, Brian, thanks as always.


00:28:22.600 --> 00:28:23.600
It's been fun.


00:28:23.600 --> 00:28:24.600
Thank you.


00:28:24.600 --> 00:28:25.600
Yep.


00:28:25.600 --> 00:28:26.600
Bye.


00:28:26.600 --> 00:28:27.600
Thank you for listening to Python Bytes.


00:28:27.600 --> 00:28:30.160
Follow the show on Twitter via @pythonbytes.


00:28:30.160 --> 00:28:33.280
That's Python Bytes as in B-Y-T-E-S.


00:28:33.280 --> 00:28:36.580
And get the full show notes at pythonbytes.fm.


00:28:36.580 --> 00:28:40.280
If you have a news item you want featured, just visit pythonbytes.fm and send it our


00:28:40.280 --> 00:28:41.280
way.


00:28:41.280 --> 00:28:43.780
the lookout for sharing something cool.


00:28:43.780 --> 00:28:47.260
On behalf of myself and Brian Aukin, this is Michael Kennedy.


00:28:47.260 --> 00:28:50.300
Thank you for listening and sharing this podcast with your friends and colleagues.

