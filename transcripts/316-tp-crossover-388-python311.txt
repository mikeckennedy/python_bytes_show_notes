00:00:00 Hey folks, for our final episode of 2022 here on Python bytes, we're crossing the streams with my other show talk Python to me, I present you one of the more important episodes over there for the year, the release of Python 311 with its new features and 40% performance improvements. Thank you for listening to Python bytes in 2022. Have a great holiday break. And Brian and I will see you next week. Here's that Python 311 episode. Python 311 is here. Keeping with the annual release cycle, the Python core Devs have released the latest version of Python and this one is a big one. It has more friendly error messages and is massively faster than 310 being between 10 to 60% faster in general, which is a big deal for a year over year release of a 30 year old platform. On this episode. We have Erie Cottrill, Pablo Galindo, sacado Mark Shannon and Brant Booker, all of whom participated in releasing Python this week, the here on talk Python to tell us all about that process and some of the highlight features. This is talk Python to me Episode 388, recorded October 28 2022. Welcome to Talk Python, a weekly podcast on Python. This is your host, Michael Kennedy. Follow me on Twitter where I'm at M Kennedy and keep up with the show and listen to past episodes at talk python.fm and follow the show on Twitter via at talk Python. We've started streaming most of our episodes live on YouTube, subscribe to our YouTube channel over at talk python.fm/youtube to get notified about upcoming shows and be part of that episode. Everyone. Welcome to Talk Python to me, it's great to have you all here. It reads Brant, Pablo and Mark. It's gonna be super fun to speak with all of you about Python 311. And before we get into it, I guess just real quickly. I know some of you been on the show before but not all of you. So let's just do a quick introduction about who you are and how you ended up here on the show. You're each one of us our first Yeah.

00:02:10 Hi. I'm a Python coder. Earlier in the week, we stream the release of Python 311. And on the back of that Michael just invited us all here for chat.

00:02:21 Fantastic. Yeah, that was a great live stream. And we'll we'll talk about that for sure in a second. But Brent, welcome back.

00:02:27 Hello. My name is Brent bucur. I have been a core dev for like two years now. And I work with Mark and here on the faster C Python team at Microsoft. I was on the show like a month

00:02:37 ago. Yeah, you were talking about the faster Python stuff which we'll touch on again.

00:02:41 Hello, I'm Paolo Galindo. And in famous release manager, I released Python 311. And you can really take all your complaints to my email address. No, please don't do that. So I'ma see Python code Dev. I'm also serving this year and the last year on the Python Student Council. And I also release I released my ear for Python 310 and 311, which is now the best version of Python downloaded today. Apart from that I do a bunch of parser stuff. But now we are not talking about that. Yeah, fantastic.

00:03:10 Well, welcome, Mark. Welcome back.

00:03:12 I'm Al Chan. I'm the tech lead of the fastest C Python team. I work with Eric Brant. I've been a quarter of some number of years, I don't recall, you've been

00:03:21 spending a couple of years working on this faster, C Python thing and very excited to see some of the fruits of those labors starting to show up and get in the hands of everyone with this release. Yeah, it's

00:03:32 good to have the stuff out actually in public and in people's hands, it's really rewarding to know that stuff you're working on is actually used and used by a lot of people. Yeah,

00:03:40 that is totally true. It's it's one thing to build software, I mean, just by itself, but it's fun. But all of you are working on code that touches dough many people think about there's layers, right? One layer is how many people use Python? Many millions, millions. Does anyone know a reasonable estimate of this number?

00:03:58 I think it's some I don't remember who came up with the number of I think they were estimating like 6 million Python developers, something like that. I mean, probably is between zero and 10 million, let's say

00:04:10 yeah, that's a massive impact, but and also maybe nervousness about pushing code out to that group. But then, you know, those people will build software for others, right? If using Instagram or using YouTube or other things, right. It's also having massive knock on effects there. So thanks for putting all this together. Thanks for improving the tools that we all get to use. So yeah, big news. The big news is that Python 311 is out and as erupts and said You all live streamed that release. Here we're all together we're having an awesome chat about the features and the what people can do to take advantage of it and why they might care about new features and want to learn them but they're you did a little bit of that but also Pablo, you actually step by step did the release of See Python mostly live, right? Yeah,

00:05:02 I did. It was, except the boring part. This is something that I started last year, because apparently I didn't have enough things to worry. And I decided to make my life even more difficult. I'm an expert on that. quite proficient.

00:05:17 I'm also an expert, I'm very bad at doing too many things and will be at

00:05:21 least manager is the only requirement. So yeah, they've used that kind of religion. Python is a process that is quite complicated. It's also quite boring. So it's not like you need you know, you need to be how a galaxy brain cannot think to do it. But it's just a lot of steps. And it's very easy to do wrong. And it's very unglamorous. So I said, Oh, wow, I'm sure people really like to see a very unglamorous process happening in life. And then I said, Let's do it. And I asked around, and I was surprised about how many people enjoy unglamorous processes. And then I did the release of Python 310 beta one, which turned out to be much funnier than I thought, because we just broke it up. That happened live. Yes.

00:06:03 Have you imported all the issues? And did that migration? Or was that separate?

00:06:08 Do you will think that that is a good candidate, but not that was not the thing that broke GitHub, we renamed the master domain on the C Python repo and the whole GitHub platform was down. What about that? Wow, yeah, you can see those Ruby workers really struggling with renaming all those forks. I think we were the I don't know, someone at GitHub may confirm this. But I think we were the first big project to the renaming. Something went wrong. I was very funny, because I literally said, how funny will it be if now I get a 500 that you owe 500? On the screen? Yeah, he's recorded there is someone actually recorded a clip? Yeah. So I said, Wow, man, this has been such an exciting thing that I can break such a big project. Let's do it more. So I decided also to stream the 310 release itself. And I said, Well, technically, the release, the final release is even more boring and longer. So that is actually probably not going to be even, like, you know, something that someone wants to see. So I said, Okay, let's not do it alone. So I invited a bunch of friends and CO developers, so they can actually talk about, you know, the things that they worked on the python three point 10 release, and brand new, were there. So that they can, they can probably tell you how they find out but like, apparently, was something that a lot of people enjoy. Because, you know, it's not only an opportunity to see how the sausage is made, because, you know, I was just explaining all the commands and all the faces and whatnot. But like when something became very boring, then you know, like brands are needed, we're there to save the day and explain the cool things they they work on to you know, which is a very good opportunity. Because you know, when is the last time you go here, the author of the future that you love, talk about the future, your love dies, fantastic. And it happened,

00:07:48 right? Not only did it happen, but as they were explaining the feature that they built, the action of it being delivered to the entire world was right, it was like all coming together in a pretty awesome way.

00:08:00 Exactly. And I could only do just to be fair also, and you know, credit where credit is due, I could only do that, because the first time I did the live thing, I was also doing all the, you know, pushing all the buttons, and at the same time doing all the video stuff with I don't know, what is this over to the stream or like whatever. And the second time, we use the help of the Python, the score team, which are fantastic. And they help us a lot. They they have this fantastic UI, where you know, all the questions that were asked on the chat or saw on the screen and we couldn't use it. You know why? Because Facebook, or now meta decided to break DNS globally. What an incredible feat. Just in time, I think wanna

00:08:44 so what I'm learning is if we need some sort of like big cloud global outage, you all just need to

00:08:53 just hire me today. So yeah, so that now we were two big outages on Python release the, you know, there is only a line that passes through two points, but I you know, it was a it was a good statistic already. So we sell what can what else can we break. So there you are, we decided to three level and release again, then mark was there as well, which increases the probability of things being broken by our sorry, Mike. He also fixes them so you know, it's fine, and nothing broke. So kudos to mark everything thanks to that, and we did a release. So we did the same thing. We explain the whole thing. So people will see from the authors themselves, like why lolis features are very cool, and I did the non boring parts of the release. And then we have a bit of some dramas in Backstage because my Dubey key that I used to sign release broke and I freak out quite a lot, but I thankfully have a backup doohickey. So no, I have crazy hair. Because if I didn't have the I will have to stop the whole thing. But we didn't have To that, it was just backstage. So yeah, quite, quite exciting. Nothing broke except my YubiKey. I suppose that the third thing that broke is not a global, you know, so well, but I still mourn it is here.

00:10:10 Yeah. It served you well, but now it's yeah, it's live for Python 311. Too much power, like three level was too powerful. dangerous job that you got? Yeah. You handed it off, right. This is your last time last main release.

00:10:29 Yeah, I need to security and bug fix releases. But I don't need to do the ones that you know, you need to chase people down. And as for like cherry picking, there was a range of things of the release that were quite boring. Like, normally, we release the previous version like that before the final version, there is something called the release candidate, which is, you know, like the last version that people need to try out before we do the final release. And ideally, that is the last version that we publish. Normally, it means that you polish from that commit. But it was not the case. This is the first release that had 130 Something comments on top of that, I have to painstakingly cherry pick. And it was not fun. But I did that before the release is like two hours, you need to fix conflicts and things like that. Yeah, very, very boring. But yeah, I started the stream with that already launched. So it was fine. Yeah.

00:11:18 Fantastic. Now before we get into all the features, and I want to maybe just talk a little bit about some of the tools for actually doing the release, and maybe start with you is just what, what is 311 mean, for you all I getting this out? What does that mean for the Python community, from your perspective,

00:11:36 what 311 is, is a huge release, there's a lot packed into it, compared to the last few releases. There are new features, there's the performance work, it's just massive changes internally, it's just a huge reducing. Personally, I've started working on, you know, exception groups about two years ago. So for me, this is almost like finishing another PhD or something. It's a massive kind of effort. And here it is, it's done. It was a big day, Monday, I had a bottle of champagne ready for the stream. It was a celebration.

00:12:09 Yeah, it was brand. How about you,

00:12:10 I'm really excited about 311. Because I think there's something for everyone. And I think you'd be hard pressed to find someone who doesn't want their code to run faster, and who doesn't want better error messages. And then you have all these other improvements. On top of that, it's really nice to see both these like new features, which are something that we get in most Python releases, but also just the stuff that's there for everyone else who just wants to upgrade Python and just have a better experience all around. Yeah,

00:12:35 I totally agree with that.

00:12:36 It's cool to see people's responses to that too, because the responses have been really, really positive, which is another thing that I liked about the live stream, because we did you know, live q&a and chat and everything going on. And when you're staring at the same codebase for like a year, you're like, Okay, I'm pretty sure that what we've done here is really, really cool. But you know, like, is it actually as awesome as I think it is, you know? Or have I just been staring at it for too long and released it to the world and people are even more stoked about who you are. And that's a really good feeling.

00:13:03 Yeah, it is. Awesome. Mark.

00:13:05 Yeah. Well, I guess I started on trying to get Python faster. 15 years ago, I guess early PhD. Right. Yeah. So that was a long, long. It's this has been a long time coming. So yeah, it's amazing to have it actually out and started to see the speed up. So obviously, we're keeping working on it. So it's pretty good.

00:13:24 Yeah, fantastic. i You must be really proud. Because like you said, you have been proposing this for a really long time, you've had a lot of ideas. And finally, you've got a group of people working on it and the written brand. You're all on the same thing with Mark and Guido. Yeah. And just making legitimate serious progress here. So it's, you must be really proud to just sort of see this actually go out the door.

00:13:46 Yes, definitely. Especially in

00:13:47 Maine, too. It's really nice that we're able to you know, deliver this

00:13:52 for everyone. Yeah, for me, I see is basically three things like kind of like you said, Brian, I see that obviously there's these new features like exception groups, which are lovely and make the language better, but it also gets friendlier for especially for beginners but for everyone of course with the better error messages and better reporting and trace backs and it gets faster and so it on all the axes that seem to matter. It's It's really fantastic. Okay, so let's dive in. I just Paulo let's go back just a little bit to the release process because people got to watch you do it but they didn't actually, you know, see exactly what you're typing on your screen the whole time. It was more of about a like an event of it. Sometimes the screen was up sometimes it wasn't. But there's an official PEP that talks about like here's the recipe for doing this right

00:14:39 that is correct is PEP one on one doing Python releases and is curious document is peculiar document talks about how it's done, but it's like it's kind of weird. So the document is up to date like you can actually you know, search PEP 101 and it will show you the thing. So the what is there is the actual process is also contains these weird sentences, like you search for it, there is a bunch of places that says Stop, stop, stop, stop, stop qualifying. And that is, if I recall correctly, Laurie Haskins. If you were all those things, and the years that he could search for those places, and he knows that the stage, he needs to wait for something to happen or something, and we left it there. So it is our job, like we're artifacts. And you know, it's full of bullet points, because some stages, you need to do some things and some others and things like that. And, you know, he says, Okay, if you're running a beater or listening to this bunch of things, and a uranium, alpha religion, a bunch of things, and I have done the, I have done a state machine that goes through the whole thing, because like, if you actually write this down is quiet is you know, the, the House does call the maintainability index of this process is insane. It is just rejects your thing is done. Right. And so like, Yeah, I'm not doing this breathing. So one thing I did, which is the thing that I was using in the stream, my first work as a release manager is say, I'm not going to do this by hand, and I is the vision. And then I did this a script that is on GitHub slash Python is last released tools. And it's a, it's my second attempt at automating this process as much as possible, which, unfortunately, you know, it still requires a bunch of manual steps, because like, that's life and things happen. But it's quite automatically, at least things that are not like Final releases. So I'll fast and release candidates. And now that we are in bug fix releases, it mostly runs automatically. Except that, you know, in the final release, everything fails, because there's the final release for you. And then you need to fix things manually. So you I think you saw me, you know, executing a bunch of those fixes. At some point, I added division by zero just to know that something was hit. And I was seen on the screen because people were like division by zero. Why do you need that to release Python? Oh, no. It's very complicated.

00:16:59 asserted false. Come on. Anything that I'm working

00:17:01 on? No, we do it with zero. I'm a physicist. So that's what I do. Computer scientists

00:17:06 study black holes, right? Yeah. You were looking for some sort of like infinite sort of thing. They are divided by zero?

00:17:13 Yeah, I would say, let's just collapse the universe divided by zero. Was too friendly. Instead of collapsing. The universe is only an exception, you know, quite nice. Only through alone. No, no, I'm joking. Anyway. So yes, yes, you can follow this pipe and you know, just enjoy the whole process on its glory. Or you can see the script. But yeah, it's quite verbose, you can see that it's very, it's lots of places when everything can go wrong, and you can panic. Now we know one more only your YubiKey can break. So that's something that can happen as well. But like, it's quite annoying. And that's the main job of the release manager goes through this annoying process. So

00:17:49 yeah, yeah, I see that there are some parts in here. You should have a few more stops, I should say, stop, stop, stop, make sure GitHub still works. Stop, stop, stop, make sure Azure still works.

00:17:59 Yeah, don't cry, don't cry at this stage. Everyone is looking at you. But yeah, the one thing that is new in this book is that you also are in charge in theory of this extreme abstract mandate, which is the during charge of the stability of the release, whatever that means. That translates modeling and chasing people because they broke things. Another unfortunate event that we are trying to also fix a bit for the for the releases is that most people turn to a release manager to solve problems. So they say, hey, this person says that we should do X, while this other person says we should do Y we need someone to the side, let's let's let's reach the release manager, but the release manager, this guy on the corner, like he doesn't know shit, so like, not the best person to fight it. But everybody was like, What do you think Paulo? So we merged this, like, I don't know, man, this is some enum things. Like I don't know about this. I have no context whatsoever.

00:18:51 Your only concern is will it still build and ship?

00:18:56 I like it. Yeah. What are these 2000 lines of code? So this tiny bug was like more, maybe less? Not more to that. But yeah, like we are trying to also like, you know, draw the deck all of these to the steering Council, which also I am in the steering council. So apparently, I'm not going to get rid of these questions. I'm joking, I enjoy all these questions. But as a lead manager, I don't so I like the key here is that the release manager should not take unilateral decisions on the evolution of these things, because like, he's just a release manager. So the reason this thing comes in is you are the

00:19:22 one who delivers the code you can kind of you could sneak a feature

00:19:26 on the site important things I just execute on chase people and I'm this annoying guy that says do work this fix it but like then if there is an important decisions to be taken, you know, does the steering Council job which is five people because you know, one person shouldn't decide these things? It's like, and this happens like sometimes I say, hey, there is this PR when people are asking lawyers to we do and then this is my opinion as the member of the steering Council and the other four members. Maybe they say why actually does no good opinion. So what about this? You know, we ended up in a much better place because he was five people. Five persons doing a decision is to Yep.

00:20:03 This portion of talk Python dummies brought to you by Microsoft for startups founders hub. Starting a business is hard. By some estimates, over 90% of startups will go out of business in just their first year. With that in mind, Microsoft for startups set out to understand what startups need to be successful and to create a digital platform to help them overcome those challenges. Microsoft for startups founders hub was born. Founders hub provides all founders at any stage with free resources to solve their startup challenges. The platform provides technology benefits, access to expert guidance and skilled resources, mentorship and networking connections, and much more. Unlike others in the industry, Microsoft for startups founders hub doesn't require startups to be investor backed, or third party validated to participate. Founders hub is truly open to all too. What do you get if you join them, you speed up your development with free access to GitHub and Microsoft Cloud computing resources and the ability to unlock more credits. Over time, help your startup innovate founders hub is partnering with innovative companies like open AI, a global leader in AI research and development to provide exclusive benefits and discounts through Microsoft for startups founders hub, becoming a founder is no longer about who you know, you'll have access to their mentorship network, giving you a pool of hundreds of mentors across a range of disciplines and areas like idea validation, fundraising, management, and coaching, sales and marketing as well as specific technical stress points, you'll be able to book a one on one meeting with the mentors, many of whom are former founders themselves. Make your idea a reality today with a critical support you'll get from founders hub, to join the program, just visit talk python.fm/founders hub, all one word, links in your show notes. Thank you to Microsoft for supporting the show. Amazing, okay, so if people want to follow along with the process, they can check out PEP 101. Let's keep it here. He also talked about the Python Bill bar that people can check out. But I think maybe we want to jump into our first feature. There's, as Reid said, there's a ton of features and things in here. But there's also maybe some top level ones that will be really important for a lot of folks and really want to tell us about your work. You mentioned before the exception groups and exception star, this is

00:22:18 kind of a major new feature that we added. And the idea is that sometimes you have a situation where you did several things, and maybe more than one of them raised an exception. And now you need to report that there was more than one error in whatever you did. And what you did, could have been a bunch of asynchronous tasks, which is that that was the use case that motivated this whole thing. But they're also kind of situations where just to iterate over a few things and and repeat them and accumulate exceptions, and you want to kind of report all of them. And the PEP lists a bunch of examples of where this can happen. So so that people typically what they do is they'll take a list of exceptions, wrap it in another exception multi arrows, some other kind of wrapper and throw that and then you have to catch it. And then you have to iterate over the list and look at the exceptions, but you don't have a message to handle the exceptions. Like you have to accept, like catch these but not catch exceptions,

00:23:15 right? Because when you accept you might have like, except socket error, or you might have an accept, like file not found type of thing. But if those both happen, neither of those would run in Python 310, right, because it's some kind of weird wrapper. And it's not a socket exception. And it's not a file exception, but it kind of contains both. And so in a sense, you write both run, I don't know.

00:23:36 And then if you catch the wrapper, even you do something with some of the exceptions, you better not forget to raise the rest because you're not handling them. So yeah, there are a lot of problems when you try to work around this. And like what happened with trail so Trailhead was the error would raise this wrapper, and it was it had to do a lot of complicated acrobatics, just to have some error handling. So the motivation was, yeah, we have task groups in Python 311, which are kind of like true nurseries, kind of structured. Collection of asynchronous tasks. And task groups were on the cards, they started, like uresa ribbon of who was kind of maintaining a sinker on the beginning, he wrote a lot of bison, Kyle, he wanted to add task groups since 2017 2018, something like that. And what was holding it up was error handling and there was no good way to handle errors. So now we have accept star which is with generalizes accept and works with groups. So you can say accept star socket error and then it will just extract all the socket errors from the group and give you those and automatically erase everything else. That's basically the idea.

00:24:45 This is pretty interesting. We have try do your thing and then accept star you know one error type except star another error type except star a set of errors potentially. So what happens if I'm in this situation and say The first error type, and maybe something from the third error, errors, catch clause is thrown in one of these tasks group of exception groups, each exception

00:25:08 in the group will be handled, but most one of the clauses so the first clause that matches its type will consume it. And each clause executes once. So if there are more than one arrows of that type, then what gets kind of bound in the Accept stuff error as E, what gets bound to is a group of arrows. So you get all the arrows in a group, execute that clause and then move on to the next clause with whatever is not 100. Yet

00:25:36 interesting. So it might run two of the clauses in squares in traditional exception handling, it goes from top to bottom, and it looks for an inheritance type of match. And the first one that matches that's it. But in this case with the star, you could get multiple ones. I guess the star to me, when I look at this, the star is reminiscent of args star where you have

00:25:57 unpacking? Yeah, yeah, exactly. It's not exactly unpacking. But it was, it was kind of the intention to make it look.

00:26:05 Nice. Yeah, this looks like a really cool feature. You talked about the task groups and trio and those things. So when I saw that concurrent errors, obviously come to mind, because if I try to both write something to a database and call a web service asynchronously, and I start both of those, and they both crash or you know, multiple ones crash, which error Do you want the database error? Or do you want the API or you probably want to know about both of them, right? So that that's a real natural reason to bring these together. But maybe you will also list out some of the other reasons that you might run into this, maybe give people some other ideas. So

00:26:38 that example of in the socket module, we have the create connection function, and that function, I was showing it in the stream, it iterates over all the configurations that you could try to connect with them. And depending on what's going on on the other side, hopefully one of the works, but if none of them work, you have to report errors. And what we do in python three times we just raise the last exception. So you don't know what happened. Really, you want to know why the past attempt failed, you don't even know how many attempts were made to connect and how many configurations did we try? So that was a long standing open problem, kind of can we do better than just recall the last row and we closed it we just added.

00:27:18 Another place that comes to mind is maybe all familiar with some of these retry libraries, like retry, but yeah, I think there's others as well, where you put a decorator on to some function, you say, try this multiple times. And if it fails, do like some sort of exponential back off, because maybe the server is overloaded, right? Those types of things would be really great. Like if it retries all the times it's supposed to and it fails, it'd be good to get all the errors, not just the last one or the first one or whatever it decided it was going to give you. Yeah, so kind of thing. Yeah. Nice. Okay. Well, congratulations on getting that feature out. That's That's great. All right, what do we got next here. I think also related to this, I wanted to talk about this PEP 678.

00:28:00 That's a very small and simple feature that what Zack had, DODDS wrote this PEP, he was trying out exception group sales, here was the first kind of user even before the PR was merged, he was trying it out, he was trying to integrate it with hypothesis library. So there, you write a test. And the library executed many times with different inputs, and you get failures in some of the inputs, and you want to report all of them. So Zach had an exception group, kind of an exception wrapper, kind of like material, multi error, he had his own version that he built in his library. And he could associate each exception he attached to it, which inputs generated this this error, which is very important, you need to tell people what the input was and what happened with it. And he couldn't do that with in a convenient way with exception groups. So we added this, for to base exception, this is not a feature, it's any exception. You can add strings, you've got add note, give it a string, and you can call it as many times as you want, and then add notes to the exceptions. And they will appear in the choice but in the default choice back that the interpreter prints. So that's all it is. It's a very simple feature. But it was received surprisingly well. People kind of like that you can enrich an exception after you catch it. So you have the information that you know the error message and the tidy you decide that when you raise the exception, but then sometimes when you catch it, there's some more information, some context, like what was I trying to do when this error happened?

00:29:30 Sure, yeah, cuz often you'll see except some type of exception type, you'll deal with what you can, but you can't really handle it there. So you got to raise it again. And this is a place to add more information without completely wrapping it right. Right.

00:29:43 Exactly. A lot of people have to chain it say this raised from that. So there will be situations where maybe you won't need to do that. Yeah, I'd love

00:29:52 to see that go away. I sort of template libraries and stuff in the web all the time. I see like, there's all these different errors and you got to went through a bunch of stuff to figure out what happened. Yeah. But

00:30:02 also think about for instance, like, I think this is super useful actually for end users even like, think about that you're doing some curious query to the database, right? And then I don't know, it may fail for 6 million reasons. And then you want to add, what were you asking for, right? So you add your, your query, or your user or whatever, because probably the exception that the postgres thingy that is underneath is not going to contain your actual thing. So this actually may save you hours, right? Because in many enterprise environments, you don't even have ACL access to that, sorry, too broad. So you don't, you can just go there and see what's going on. So it will be super cool that you say, if something fails, you know, I was trying to do this with this day down, like with this thing. So like, if it fails, now you can know what's going on. And you don't even need to log in, which is, I think it's quite good. Yeah, it's

00:30:53 a great idea. Or if you know, here's probably why this happened. As a library developer, you like, look, this is the error. But here's the note, this is probably because you didn't initialize the connection before you call this. So make sure you know, like that. Another error area where I see this can be useful is I want to raise like, the example you have in the docks is type error. But it could also be value error, or some other built in low level type, you know, like, really, this is just I want to raise that error. But it doesn't have a place for me to put additional information. And so I want to kind of enrich that with more and so not just catch, add the data and then raise it again. But actually, I want to use a Bayes error type that doesn't let me put more details in it and then just raise that right. That would also work.

00:31:36 I think. So I mean, I think the intention was, there was some discussions about using nodes in the interpreter, and I pushed back on it. So this is owned by the application, the interpretation, the notes, you know, because people can wipe out the notes, they can change the order, they can do what they want. It's, it's the applications, at least to ask the question, and you put whatever context you want to put,

00:31:57 is there only one note, when I say Add note, does that set the note? Or can I have a list of notes? It's a list

00:32:03 of notes. Okay. Got it. Yeah. And you can wipe it out if you want. You can it's, it's just a list to attach to the exception in the way he wants it.

00:32:11 Really? Yeah. Okay. Yeah, it's a great, it's a really great feature. I mean, it's, I'm sure it was way less work than except star, but it's also gonna be really valuable. I think it's very simple. But it's Yeah, Brent and Mark, you guys got any thought about this before we move on to the next?

00:32:25 I think it's really cool. I like it. Great work here. Indeed.

00:32:28 I think it is. Well, actually, I'm really excited about it. I want to talk about let's let's talk about faster Python for a little bit. So Mark, I had you and Guido on back. Wow, almost to the day a year ago. We're off by November 1 2021. So quite not not that long ago. Let's talk a little bit about the work that you're doing there. I guess the headline is that Python 311 is 10 to 6010 to 50% Faster than previous sort of on a reasonable range of applications. That story?

00:33:02 Yeah, it's somewhere between minus a few percent and plus 100. But it's very, it's a huge amount. I mean, sure. I mean, if you've got some application, there's basically spent all his time in NumPy, or something like that, you're not really going to speed up at all. But if it's pure Python, you'd expect it to be good. 40 50% faster. So but it depends, right? That's

00:33:22 a good point. Because a lot of people do make Python faster by writing C or rust or other languages. And at that point, like it's out of your hands, right? Yeah. So when

00:33:32 we're looking, hopefully, for 312, to start looking at the sort of interface between Python and C code, so we should speed up code even, there's quite a lot of C code words sped up the time spent in the C code in doing actual work in the C code. But there's still quite a lot of sort of marshaling of data that happens and hopefully will streamline that, but it's the existence of C extensions is sort of, in some ways, limits or opportunity to speed things up. But it's also absent, you know, why Python is so popular in the first place. So one of the main reasons so definitely need to acknowledge it. Yeah,

00:34:02 absolutely. So Brent, I'll definitely have you talk about the specialising interpreters. But Mark, maybe give us a rundown of some of things from your plan that made it in in here. I know, some more aimed for 310. But they didn't make it until here, right.

00:34:15 Yeah. So the whole thing, all that original plan I put up, there was more of a just to get this discussion, get discussion going sort of thing. And so basically, yes, more or less a year off. So if you just shift everything one forward, I mean, there was a lot of discussion on speeding up the interpreter in the first iteration, and then looking more sort of data structures. And the second thing is, is much more jumbled than that. We're doing sort of bit of everything. So obviously, what I was planning on, you know, expecting a smaller team, so things are being a bit shuffled. So yeah, there's specializing interpreter, obviously, that's kind of key. There's also quite a lot of stuff we've done with data structures. I mean, we shrunk the Python objects. I mean, the Python objects, you know, there's been shrinking for years. I mean, I've got some numbers here. So I can to seven and three, two. Unlike an object with just four attributes would take 352 Bytes on a 64 bit machine. And for three level we've got it down 212 And for 312 96 Well, before we get too excited, there's only 32 in C++. So you know, we've got a better way to

00:35:14 go. Yeah, but you know, it's going in the right direction. And you sure some people out there listening just be like, Okay, well, it's half the size, roughly, and it's gonna be less than that. So, yay, we can use less memory. But maybe you could talk a little bit about how that affects things like L one, l, two L, three cache hits and other sorts of, it's more important than just I need less RAM. Right?

00:35:37 Yeah. So there's two, two things that happen. There's yeah, there's things are faster, because the hardware is just happier. If you pack everything together, it's in a high level cache. So you're not getting these sort of long pauses as you hit the memory. And the other thing is just the data structures are, because there's less of them, there's less indirection. So for example, loaded attributes, that we've got it down for basically an old or older versions of Python, there were sort of effectively five memory reads, and they were dependent memory, you'd have to read one before the next one, and so on,

00:36:07 go to the class, find it, go to the object, find its dictionary pointer that's in the dictionary and then go to that, right. Like it's,

00:36:13 yeah, that's very much that. And it's down to more or less to now. So that's, obviously there's still interpretive overhead on that. So that this, it's not quite that much faster, but it's getting there. So yeah, this is the data structures and end of the frames the Python frames, every time you call a Python function, we used to just allocate a heap object for the frame, and all this stuff would go in there. And now they're all basically an urban contiguous sort of block of memory. So it's just bumping a pointer rather than allocating which is also faster. And frames are just smaller anyway, because of the the zero cost exceptions, which I think we both we mentioned on the release thing, but yeah, this is well,

00:36:53 let's tell people about zero cost exceptions. Okay, well, zero, to pay for errors if you're not raising errors, right? Yeah,

00:37:00 that's the idea. And that's why they're called zero cost. zero cost is in quotes in this and there's a reason for that is it's just, that's the name it has got. They're definitely not zero cost. The idea is they're they're pretty low cost if you don't have an exception, but they tend to even more expensive if you do get an exception, because you have to do more lookup. The important thing here is that just there was lots of runtime information we need to maintain and we don't now so that, again, shrinks the frames and just makes calls faster, because calls in Python are notoriously slow. So that's one thing. We've sped up significantly.

00:37:31 Yeah. So the idea was, in previous releases of Python, if you just enter a try block, even if it was successful, there was a little bit of overhead to set up the mechanisms of potentially handling the errors and information you needed. Right. Yeah. And that this wasn't just the overhead, you defer more of that? Right. Yeah,

00:37:48 I mean, it's actually not so much that overhead is the just the space, you had to put that data in, had to be allocated every time you made a call, in case there was an exception. And then we had to there was massively over allocated to the amount of space anyone ever needed. So just that was the big sort of advantage. Nice.

00:38:04 Yeah, this is fantastic. You don't want to discourage people from putting proper early error handling in their code. Yeah. What do you think I see your name on this feature here and GitHub? What are your thoughts on it?

00:38:17 Yeah, I think I think it's cool. I mean, I was kind of, you know, it was a nice touch that Mark implemented between when I wrote the prototype for exception groups and when the PEP was approved. So that kind of well, and good well, it was good. It was I got intimately acquainted with zero cost exceptions to that exercise.

00:38:40 Oh, well, it's suitable for for for some people.

00:38:44 It is Mike a lot about that. No, I think it's a cool feature. And I mean, I followed up on that we now have, after we remove that, we still had a I was talking about this on Monday, we had a jump over the exception Hunter, and then I sold to aquarium and this is a jump it's not zero, you have to jump up with exception Hunter, if there's no exception. So now we have we did we identify exception handlers as called blocks. And before we lay out the code of the function, we put all the code blocks in the end. So now if there's no exception, there isn't even an exception handler to jump over. That will be in 312.

00:39:21 So excellent. You made zero cost even faster. So now it's

00:39:27 smaller. Yeah.

00:39:28 It's asymptotically. Approaching there.

00:39:31 Yeah. So but it's kind of nice that we have this notion of cold blocks and hot blocks. And we can maybe do other things with it kind of nice that all the all the happy. But the fast code is kind of in the beginning of the functions, bytecode block, and, you know, in terms of caches, and all that you don't have to I think we bring a few benefits beyond just not having to jump. Yeah,

00:39:54 this is excellent. It's a really great feature and pretty straightforward. All right, Brent, tell us about the specializing adaptive intern For sure, that's a big deal. You and I spoke about that about six weeks ago, I think,

00:40:04 yeah, basically, the headline is the bytecode changes while it's running to adapt to your code, which is really neat. So it's kind of finding places where we can do the same thing. But using less work, right, like cheating a little bit. But cheating in a way that is not visible at all. A good example is something like a global load, or a load from the built in. So if I'm looking at like the len function that requires two dictionary lookups, every time I want to look at the len function anywhere, I first need to check the global namespace, and that's going to be a failed lookup, then I need to check the built ins dictionary, and that's going to be a successful lookup. So every time I want to use Len, or range or list or any of those built ins, that's the cost that I have to pay. But people don't change the global namespace that often. And people change the built ins namespace even less often, or at least they shouldn't be acting it very often.

00:40:55 I'm gonna make false true and true false. Let's see what that does.

00:41:00 And so you can make these observations where it's like, okay, well, if the set of keys in the global namespace hasn't changed, since last time this bytecode instruction ran, then I know that that lookups gonna fail, because if it failed last time, and the keys are the same, then it's going to fail this time as well. So we can just skip that. And same for the built ins dictionary, you know, if we know that the keys in that dictionary haven't changed, that actually means that the internal layout of the dictionary is the same. And we don't even need to look up Len in the built in dictionary, we can reach directly to the last location where it was before and give you that instead. And so that's cool, obviously, in a lot of code as like a an older code as a kind of a micro optimization. Whenever someone was using a built in and like a very hot Python loop, sometimes you'd see them like do this kind of quark trick where they make it a local variable by saying like land equals land, or something like that as part of the functions arguments, so that you turn it into a fast local load. And what we've essentially done is, you know, made ugly acts like that. Totally unnecessary.

00:42:02 Yeah. Which is really that behind the scenes transparently. Yeah,

00:42:05 exactly. And so that's, that's just, you know, one example, we've done tons of specializations for all sorts of things ranging from calls to attribute lookups to attribute stores, etc. So, yeah, it's a it's a really, really powerful thing. What was it? 569? Yeah. Mark Rody.

00:42:23 There's 656590. Yes, there. Yeah,

00:42:26 yeah. So this, this interpreter is Mark's baby, he could you could tell us much more.

00:42:30 Tonight. Yeah, it's gonna give you a chance to give a shout out about specialist.

00:42:33 Yeah, yeah. So this is why I was on your show a couple of weeks ago. So looking at bytecode disassembly is it's not fun. And so one thing that's kind of cool is, you know, if you upgrade to Python 311, we were in your code, you saw it got, you know, 1020 30% Faster, you might be wondering, like, Okay, where did it get faster, like, what is faster about my code. And so specialist is basically a package that I made, it's PIP installable, it only works on 311. And basically, if you run your code using specialist instead of Python, so you just type specialist, my project.py, or whatever, it will open a web browser and show you your code, but color highlighted to show you where the interpreter was able to specialize your code where it wasn't. And that's really neat. So you can see like, oh, actually, you know, these are the attribute loads that go faster. These are the places where my global loads are being cached.

00:43:23 Yes, awesome. Yeah, this is a really cool project. And it has some proactive features, not just informational aspects, I think anyway, you know, if you run a profiler, it'll show you where your code spending time, but it doesn't mean you should go change everything. To make it faster, you should look at like, oh, this loop, or this one function is like the thing that maybe we should think about slightly changing the algorithm or the way we do a loop or something. And it's a little bit similar here, because the specializing adaptive interpreter only specializes some things like it doesn't specialize floats, interacting with it's or those types of things, and or I think division as well. And so there's certain ways you might be able to slightly change inside of a really hot loop, you know, make something a float ahead of time, if you know, it's going to be involved in floating point operations, right.

00:44:10 Yeah, the idea is that this is show us how we can fix things so that you don't need to mess with your code.

00:44:16 I see. So this is in the future. Okay. Yeah. Awesome. Yeah, I would not necessarily

00:44:21 encourage people to start tuning individual bytecode instructions in their code due to our implementation details.

00:44:27 Otherwise, you will end call them and see

00:44:29 y'all. I gotta take all those decimal points back out of my code. No, just kidding. Yeah, I

00:44:33 want to get every single bytecode instruction green. Some things will never specialize. And that's just an artifact of programs. But you know, if we can specialize enough and we typically do, you know, one line maybe 20 bytecode instructions, if you know four of them get specialized successfully and two of them don't generally that will still be.

00:44:51 Did you know what it used to do for April Fool? Like usual do a pipe this plugin that soju the percentage of specialized instructions in your code And people can fix the percentage so they can say find my test suite. If my goal is not specific.

00:45:07 If you do specialize it is

00:45:11 dropping people Yeah, it's like a coverage thing, ya know, kind of thing about this. So follow can tell you more about this, but his cool new tracebacks the whole reason specialist is able to do these, these cool, you know, column level highlighting of your source code is because we do have that fine grained position information under the hood. So it's we kind of just piggybacked on that feature in order to give you that, but I was kind of thinking another another thing, another April Fool's project could be, you know, column level coverage information. So to get to 100%. coverage, you have to cover every single column, suddenly, I feel like people might take that too seriously,

00:45:47 even though why it's based on his wife's business goal.

00:45:51 You think you're intense by having branch coverage turned on? Just what do you have column coverage

00:45:54 turn? Yeah, you can only cover two white spaces per line. So you got to call that a lot. All right. I think that's a perfect segue over to one of the most tangible contributions from Pablo here. Maybe tell us about this new finegrained error locations and trace backs. This is fantastic. This will say people will be in in debuggers or rewriting their code with tons of print statements to figure out what's going on. I think, yeah, I

00:46:18 think guru Maitreya we put all the effort into this. So this is a man I don't even remember my PEP. So I don't know it's PEP something something and it has a horrendous mix. Seven and let's see phase seven thinks

00:46:33 error locations and trace backs. Yeah,

00:46:35 the wars name. Even I think I was talking with Mark in the Python core developers brain. And he was saying like, what it means like fine grain like, like, you know, like this very fine grain like, so I think we are renaming the PEP to fancy tracebacks. I think there's much better. Anyway, so this is a project I work together with about two hunters, Gaya and Amara scar. So kudos to them as well, because they participated equally on this. And the idea is that we were like, we started this project to make, you know, to improve the error messages in the interpreter and the general experience, not only for, you know, people, because when people talk about this, they normally refer to people starting to learn Python, but like, to be honest, most of these things also affect people that are expert, like, I always say that when I implemented the suggestions, I was the first one benefiting from them, because like, I make a lot of typos. And you know, like, this is all you mean, this. So the year that we have is that most of the time, the lack of, you know, the interpreter, so you can have the position when the error happens, but it's quite limited, because most of the time, you people tend to have due to Python, flexible syntax, a huge amount of like complexity in a single line. In the pub, that is one of the examples like you access a range of keys in a dictionary, and some of them doesn't work or is not there, or is known, or something like that, right? And then it fails. Or sometimes you have like several function calls or several additions. And you know, it's quite difficult. And most of the time fixing these things involve going into other via, like PDB, and then trying to inspect every single object and say, Okay, this dictionary doesn't have this key at this level. And like, you know, that sucks. Like, it's not because like the loggers are cool, but like, it's cooler not to use them right. And you know, we thought what can we do here and we we arrived to this idea actually also to mention everyone in both this was originally inspired by some kind of prototype that car from the pipe it made very long ago, when he saw like a kind of minimal version of this and then I said Okay, can we do this and what we do now is that we propagate because the parser are super cool back parser knows a solution of all the tokens and things like that. So we are propagating those that information through the interpreter. And we store this information now in cold objects. So a side effect of this PEP activities are called objects are slightly bigger, although, you know, because call objects don't tend to be a huge percentage of your application, it doesn't really matter that much. Maybe big files are bigger Well, you know, you have a lot of disk space I'm sure and the idea is that you know, we store this information in code objects. So when you raise an exception we say well, what is the instruction that is right this exception and then once we know which is instruction that raise exception then we go and say okay, what is the position information that generated this instruction? Because we propagated we know and then we can say okay, here is kind of like the like the lines, the columns that this instruction spans, so that kind of allows us to underline the specific location but we will get farther sorry.

00:49:41 I was gonna say this is super valuable example we have in the PEP is you have a dictionary and say bracket of key A and then the thing that comes back is another dictionary. So you say bracket B and then another dictionary bracket C and then bracket D, and if the on 310. The errors just like if one of those misses is none say None type is Object Object is not scriptable or maybe you know, does not contain that key or some weird thing like that. But is it A, B, C or D? You have no idea you're in a debugger, printing them out separately or something. But now it just goes, Nope, it's the C one. That's it's the third as subscript one. And that's just just jump right to it. Yeah. Okay. Yeah.

00:50:19 So this error, non type is no subscript, or is kind of like, Thanks for the info. Like, it's like, you know, water is what? Okay, thanks. Very useful. So then we one is going to rain anyway, we did like the first version of this. And then we realized, realize that there was some kind of like, you know, it was cool, like most people really like it. But like, especially, for instance, with the example with a dictionary that has many dictionaries inside, there were some confusion because like, you know, along the lines, the whole thing, and then, you know, the order of operations. And you know, also with complex mathematical expressions, like if you do a plus b plus c, and the last operation fails, it needs to underlie a plus b plus c, because what happened is that it first added a plus b, and that gives you something that, then you add a to z. And what happened is that the last operation failed, but that includes a plus b, so you need to underline the whole thing. If you know, they're their operations, and I just underline a plus b plus c, you know, that will what will fail is the last one, because that's the last one now this execute, but is it still confusion? Because, you know, specifically also with a dictionary, people were saying, Yeah, okay, but like, you're on the line, I have three keys here, which is the one that failed. I mean, you know, you can learn by experience, or is the last one, but it's kind of like it was not a great experience. So we went a step farther. So where we do is that once we know the kind of range in the line that suits the problem, we rebars, that chunk of expression, and then we know Okay, so we know now that this is compression has this ASD, and then we analyze the ESD. And then we say, okay, is this ASD something that we can further improve their message, like, for instance, is this ASD I want to have good access in a dictionary or I want to attribute access or I want to have function calls or or maybe binary operations. And if it's the case, then we use a specialized, like, you know, underline, I don't know, tilde squiggle. You know, the dictionary ones have these different one that marks which key axis it was not the same thing for binary operations and things like that. So we do this extra step at the end that you know, doesn't have extra work, but it tries to improve even upon the kind of under just underline the line just so we can offer even more rich information. And I'm quite happy. I'm very pleased about this. i Sorry, Mark, but I think it's the best feature of relearned is probably the second thing when when I said this, but it's true. Totally, totally true. 100% true. So I'm very excited about this. I literally use this every day I today was deploying Python 3.1 This week, sorry, I was deploying python three, bloomer and something wrong wrong. And literally these things say my day deceived, say me to just logging into some forsaken machine and understanding what's going on. While about that, so super cool, may happy I hope that everybody that uses this and is happy reached out to us and say I am happy because like normally people reach out to us when they are not happy. And they say you should reach out to us and say nice, I this, you know, I did this cool thing, you should tweet it

00:53:19 out or something, though, don't don't open issue saying you're happy. It's

00:53:24 just exactly just tweet a couple till there's a few carrots and a smiley face. It

00:53:28 is only to the hobby at bite on the door, which I will take, and I will address awesome is that we improve it a bit further. One of the things that happened is that, you know, like sometimes if the whole line is wrong, because this is simple. You have the review, sorry, for the ones I had before. We have we have here some some we're seeing some output, but doesn't matter. Don't worry, I will describe it. So for instance, you're calling a function, and that's the whole thing that is in the line will use to underline the whole thing. So it will say okay, even if the whole line is failing, so there's no like a part of the line is failing, the whole thing is failing. We used to underline that, and that apparently is still on the PEP. Maybe I should change that because there is no like that anymore. Because I want to just, I mean, come on, like if it's the whole line is failing, underlying the whole line is actually not that useful. And you know, you're you're spending vertical space. So you need to scroll a lot. At the beginning. I say, Yeah, well, this is inconsistent. I don't like it and I push back a bit but like then you know more people say Paulo, you're wrong. And then I say, Okay, I'm wrong, we improve this further. So you say but don't take this as a sign of bias. Don't tell me that I'm wrong, collectively, please. But right. So now if the whole line is underlined, we don't underline it because it doesn't really add any new information right. So, so only if a part of the line is contains the or not the whole line. So this means that we are not going to you know, consume a lot of vertical space for no reason. And the last thing I want to say is that you know, there is some people somewhere in the universe that may care about That extra disk space on their big files, or they just really really hate squiggles. I don't know if that is even physically possible. But you know, they're very different and diverse set of people, you are one of those that is a collection of different ways you can deactivate this feature, there is an environment variable with a super long name. And there is minus x option when you launch the interpreter. So you can say python minus x something something I don't know how it's called, I think it's called Knowledge ranges, follow the weather, incredible naming. And then you said knowledge about ranges to one and it deactivates the feature incredible, like magic is gone. You can reclaim your PAC files and you can even generate the way he fires without this information. If when you're compiling because he files you said this evil Marion bio, but don't do that. listeners don't do that is evil. Don't do that. Just just use it is great. Yeah.

00:55:54 There's another kind of type of errors that I think we're gonna get. It's about edge cases where the compiler doesn't get the line numbers, right? Because all these kinds of fine grain locations, it's all new. And you know, we're still ironing out

00:56:09 from future there is a from future, I think that you put like a bunch of things with different features complaints on a random place.

00:56:15 Yeah, that today. Today, I found that one. But I've been looking at the compiler and line number location information, and it's a bit off here. And then we have received bug reports from other people as well. It doesn't look right, the range here looks too broad. So yeah, we're gonna get we're gonna be ironing that out. So I guess let's meet Well,

00:56:36 yeah, it's really nice when people are using betas and release candidates, though, because we were able to catch a lot of those before the release, because there were a couple people, I forget exactly the name of the project. But they're working on like a code animation tool where it animates the code while it's running. And they were using these new ranges to identify ASD notes and stuff. And so they did this thing, I guess, where they like run their tool in the entire standard library, make sure it's correct. And so we got a bunch of bug reports that basically say, oh, you know, this column information is off with this weird multi line attribute access or something

00:57:06 you recall, I think, you fix an error. That was super weird, because he was like, a method axis, like, you know, my instance, or any of the metal axis has like some like Bower or something like that it was wrong. And if you added some extra letter, it was fine. It was fine. Yeah, it

00:57:23 was like if you split your method access across two lines, if you do like x dot method, or x dot method, or x dot method, on three lines, or something, the way we trace those lines, we always trace the method when we're actually loading the method, even if it's on a different line is like where the actual method load started. And then we were doing some weird math to like figure out where the.is So we would try to put on the same line as the dot. And so we just like subtract one from the length of the name. And so there's all sorts of crazy stuff. And that

00:57:56 came from the guy because we fix that and then it was wrong again, because like we were late miscalculating the name.

00:58:03 Oh, my goodness. Yeah. So all sorts of fun stuff like that.

00:58:07 Yeah. Amazing. Well, yeah, this is definitely one of the highlight features, for sure. And also the performance work, they all join. Alright, we're, we're getting very, very short on time. So I think maybe it's super super lightning round here. Let me just say we also got a Tama lib support built in. We've got the async IO task groups, Allah trio nurseries, we've got a new features for atomic grouping and regular expressions, a self type, a lot of type things have been added. So we got the self type very Patrick generics, literal strings, which is very interesting. And Lucas did a talk about that on the release live stream stuff for type dict and data class transformations. So great stuff. Now let's just really quickly round out what's the python three elevens story for postscript, Pio died? Is there? Do you know any anyone out there?

00:58:57 No. I don't know. I suppose it works.

00:58:59 Web assembly is now at tier two or tier three supported platform right.

00:59:03 So he has been making a lot of improvements to the build process, which you know, is not easy. So kudos to Christina Himes, if you're listening, you're great. I suppose the tie by script can we through highlight this is how many layers is this? So biolay through these can leverage all these improvements? Because I don't know how is the whole layer that we're thing is working. But Biolite has a bunch of patches that you know you need to modify C Python so it builds nicely on web assembly platforms. I don't know the details on that. I just know that some of them are okayish some of them are not okay and quite difficult to maintain. And Geeta has been making a lot of great effort to you know, change here and there. And I like pulled out of micros and if they have some things like that. So see Python kind of builds easier. This probably translates the Biolite I hope, kind of, you know, consume this bill in an easier way with less budget and I suppose that translates into Who buys crip? Like just using the pie thing? But But yeah, I don't think that there is a huge amount of improvements more than, you know, we are working towards official support as Brian was mentioning, we have this new system is super cool

01:00:15 that has like, unrelated fun fact Mike drop in one of the early developers, right is actually managing our team at Microsoft. Oh,

01:00:22 it's funny how this circle comes back around, indeed

01:00:27 how the darn tables.

01:00:30 Right? All right, we are out of time. But super exciting. I wish we had some champagne. And probably we didn't even bring hats to celebrate Python. But I know everyone out there is extremely

01:00:42 cheap. But I have a python three live in DC. Yes. Yeah, that's a great logo

01:00:46 for 311 and stuff in general. But just for the release, it's awesome. All right. Before we get out here, let me just ask you one final question. And then we'll we'll call it a show notable peipsi package something want to give a shout out to we'll go top to bottom in the picture here, Pablo,

01:01:00 Nadal, a package? What am I going to say memory use memory? One only by whom? Memory profiler soldier Memories Collection today

01:01:12 with memory that in the underlying errors? You'll be all good. Exactly. Yeah,

01:01:15 combination.

01:01:19 Why not had some interaction with the author of bytecode version can because that was looking at the things to do in the testing and in the interpreter that are kind of like that. So this is the library thing kind of from Python. Right. bytecode. And it Oh, wow. It's pretty neat. And he's struggling with zero cost exceptions. But that's what

01:01:37 it's like inline assembly. But for Python? Yeah.

01:01:41 So I'm not bison script, you can kind of write that of bytecode and get it to do another. Yeah, just doing stuff. That's

01:01:49 awesome. Brent, how about you?

01:01:50 Well, I'm partial towards specialist. But if I had to choose something else, right speed speed, I really liked the scaling profile profiler. I've been using it a lot of my own projects. And it's, it's awesome. I don't know how its memory profiling compares to memory, I'm sure memory is better. But scaling is really nice for measuring the difference to the performance across both Python and C code groups. Cool, excellent.

01:02:11 Mark was not actually a pi pi package, I was gonna say the SIS module, which is like, pretty much the most fundamental. There's all sorts of fun things in there, you can change the recursion limit and see, you can model it if you're interested in how Python works, it's actually quite a sort of fun thing to play with. So thank you all,

01:02:28 for all the hard work. And I know there are many people who did a ton of work as well who are not on the show here. But you can represent them as well. So thanks all for being here. Final call to action. People want to get started with 311. What do you tell them? Is it ready for them to get going? What do you think that's awesome.

01:02:44 He's also an also now 311 comes with a bunch of wheels for all your packages, because there has been a lot of good work in the people of, you know, third party libraries. And now that you know, people are using the GI Bill, we'll relearn how it was released. We will for NumPy and pandas and our angelfall are things that previously was failing massively, because nobody could compile them on their crappy laptop. But now you don't need that. You can just download them and it works. So just use 311. There's no reason yeah, that's boring. That will be a reasonably you're boring. And you don't want to use a 311. And don't use the same break

01:03:16 anything, not even a package much less get up.

01:03:21 And we need more benchmarks. So well. That's true. Yeah, absolutely. That's how people can help us make things faster. It's more benchmarks. So we have, there's a sort of standard PIs and performance suite, but it's kind of a bunch of toy programs and so on. So if you've got something that might make a nice sort of benchmark, you know, sort of some self contained but sort of realistic program then yeah, that's no. All right. Cool. Well,

01:03:43 thanks again. Great work on it. cam gear lock out in the audience's yay. See I build we'll Yeah, absolutely. Great stuff. And so thanks again, everyone. I'm super excited to start using 311. Myself.

01:03:54 So thank you, Michael, for inviting us.

01:03:55 Yeah, it's great to have you here. Thank you. Thank you. Bye. I hope you enjoyed this crossover talk Python episode. If you did, please consider subscribing to talk Python to me if you don't already do so. On behalf of myself and Brian rock, and thanks for being part of Python bytes. By now

